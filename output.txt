
=== File: ./_quarto.yml ===
project:
  type: website
  resources:
    - "www/*"
  preview:
    port: 3000
    browser: false

format:
  html:
    theme: cosmo
    css: styles.css
    toc: true

website:
  title: "SEM Coding Summer 2025"
  site-url: https://Imbernoulli.github.io/summer/
  description: "清华大学经管学院暑期代码能力提升培训"
  twitter-card: true
  open-graph: true
  reader-mode: true
  page-navigation: true
  repo-branch: master
  repo-url: https://github.com/Imbernoulli/summer
  repo-actions: [issue]
  navbar:
    background: primary
    search: true
    right:
      - icon: github
        href: https://github.com/Imbernoulli/summer
  sidebar:
    style: "floating"

metadata-files:
  - sidebar.yml


=== File: ./.nojeklyll ===


=== File: ./.DS_Store ===

=== File: ./sidebar.yml ===
website:
  sidebar:
    contents:
      - index.qmd
      - section: 计算机工具简介
        contents:
          - Lessons/lesson1.qmd
          - Lessons/lesson2.qmd
          - Lessons/lesson3.qmd
      - section: 计算机实战应用
        contents:
          - Lessons/lesson4.qmd
          - Lessons/lesson5.qmd
          - Lessons/lesson6.qmd
          - Lessons/lesson7.qmd
          - Lessons/lesson11.qmd
          - Lessons/lesson10.qmd
      - section: 计算机前沿展望
        contents:
          - Lessons/lesson8.qmd
          - Lessons/lesson9.qmd
      - section: 课程资源
        contents:
          - Lessons/byl.qmd
      - section: 计算机系相关资源
        contents:
          - text: "技能培训"
            href: https://summer24.net9.org
          - text: "技能文档"
            href: https://docs.net9.org


=== File: ./LICENSE ===
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


=== File: ./styles.css ===
div.description {
  font-style: italic;
}

.quarto-figure>figure {
  margin-top: 0.5rem;
}

figcaption.figure-caption{
  margin-top: 0;
  margin-left: 1rem;
  font-style: italic;
}

.cell-output pre {
    margin-left: 0.8rem;
    margin-top: 0;
    background: none;
    border-left: 2px solid lightsalmon;
    border-top-left-radius: 0;
    border-top-right-radius: 0;
}
  
.cell-output .sourceCode {
    background: none;
    margin-top: 0;
}
  
.cell > .sourceCode {
    margin-bottom: 0;
}
  
main ol ol, main ul ul, main ol ul, main ul ol {
    margin-bottom: 0.5em;
}



=== File: ./MANIFEST.in ===
include settings.ini
include LICENSE
include CONTRIBUTING.md
include README.md
recursive-exclude * __pycache__


=== File: ./index.qmd ===
---
title: "Practical Deep Learning"
image: https://course.fast.ai/www/social.png
description: A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.
creator: "@jeremyphoward"
---

:::{.callout-tip}
## New!

We just launched a new >30 hour video course for more experienced students:

[Practical Deep Learning for Coders part 2: *Deep Learning Foundations to Stable Diffusion*](Lessons/part2.qmd)
:::

::: {layout="[30,70]"}

![](images/pencil_sketch.png)

This free course is designed for people (and bunnies!) with some coding experience who want to learn how to apply deep learning and machine learning to practical problems.<br /><br />Deep learning can do all kinds of amazing things. For instance, all illustrations throughout this website are made with deep learning, using [DALL-E 2](https://openai.com/dall-e-2/).

:::

## Welcome!

Practical Deep Learning for Coders 2022 part 1, recorded at the [University of Queensland](https://www.uq.edu.au/), covers topics such as how to:

::: {layout="[30,70]"}

![](images/imagine.png)

- Build and train deep learning models for computer vision, natural language processing, tabular analysis, and collaborative filtering problems
- Create random forests and regression models
- Deploy models
- Use PyTorch, the world’s fastest growing deep learning software, plus popular libraries like fastai and Hugging Face

:::

There are 9 lessons, and each lesson is around 90 minutes long. The course is based on our [5-star rated book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527), which is [freely available](Resources/book.qmd) online.

You don’t need any special hardware or software — we’ll show you how to use free resources for both building and deploying models. You don’t need any university math either — we’ll teach you the calculus and linear algebra you need during the course.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Real results

以下是一个用Markdown格式编写的复杂数学公式：

$$
\int_{-\infty}^{\infty} \frac{e^{-x^2/2}}{\sqrt{2\pi}} \left( \sum_{n=0}^{\infty} \frac{H_n(x)}{\sqrt{2^n n!}} z^n \right) dx = e^{z^2/2}
$$

这是高斯积分与埃尔米特多项式的一个关系式，其中$H_n(x)$是埃尔米特多项式。

如果您需要更复杂的公式或特定主题的公式，请告诉我。

Our videos have been viewed over 6,000,000 times already! Take a look at the [dozens of testimonials](Resources/testimonials.qmd) about our book and course by alumni, top academics, and industry experts.

::: {layout="[70,30]"}

> *'Deep Learning is for everyone' we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning.*

![](images/people/norvig.jpg)<br>**Peter Norvig**<br>Director of Research, Google

:::

By the end of the second lesson, you will have built and deployed your own deep learning model on data you collect. Many students post their course projects to our forum; you can [view them here](https://forums.fast.ai/t/share-your-work-here/96015). For instance, if there’s an unknown dinosaur in your backyard, maybe you need this [dinosaur classifier](https://notebooksg.jarvislabs.ai/U7mQvWgvra53-YvTogLgJCfNzgJxRJv238Go2bHoAmHBqQFHoOL1ZFeKDG8gYmnO/)!

![](https://user-images.githubusercontent.com/346999/177054810-3e56fcb1-4fb7-418a-b10b-351bab20bf6b.png)

Alumni of our course have gone on to jobs at organizations like **Google Brain**, **OpenAI**, **Adobe**, **Amazon**, and **Tesla**, published research at top conferences such as [NeurIPS](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems), and created startups using skills they learned here. Petro Cuenca, lead developer of the widely-acclaimed [Camera+](https://camera.plus/) app, after completing the course went on to add deep learning features to his product, which was then [featured by Apple](https://twitter.com/pcuenq/status/1540121042596904963) for its “machine learning magic”.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Your teacher

::: {layout="[70,30]"}

I am Jeremy Howard, your guide on this journey. I lead the development of fastai, the software that you'll be using throughout this course. I have been using and teaching machine learning for around 30 years. I was the top-ranked competitor globally in machine learning competitions on Kaggle (the world's largest machine learning community) two years running. Following this success, I became the President and Chief Scientist of Kaggle. Since first using neural networks 25 years ago, I have led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic (chosen by MIT Tech Review as one of the "world's smartest companies"). 

![Jeremy Howard](images/people/jeremy.jpg)

:::

I am the co-founder, along with Dr. Rachel Thomas, of fast.ai, the organization behind this course. At fast.ai we care a lot about teaching. In this course, I start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on… We always teach through examples. We ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Is this course for me?

Previous fast.ai courses have been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Many students have told us about how they've become [multiple gold medal winners](https://forums.fast.ai/t/my-first-gold-medal/54237) of [international machine learning competitions](https://towardsdatascience.com/my-3-year-journey-from-zero-python-to-deep-learning-competition-master-6605c188eec7), [received offers](https://forums.fast.ai/t/how-has-your-journey-been-so-far-learners/6480/2) from top companies, and having [research](https://ui.adsabs.harvard.edu/abs/2020EGUGA..2221465A/abstract) [papers](http://www.ieomsociety.org/ieom2020/papers/37.pdf) [published](https://pubs.rsna.org/doi/abs/10.1148/ryai.2019190113?journalCode=ai). For instance, Isaac Dimitrovsky [told us](https://forums.fast.ai/t/thanks-ra2-dream-challenge-win/76875) that he had "*been playing around with ML for a couple of years without really grokking it... [then] went through the fast.ai part 1 course late last year, and it clicked for me*". He went on to achieve first place in the prestigious international [RA2-DREAM Challenge](https://www.synapse.org/#!Synapse:syn20545111/wiki/594083) competition! He developed a [multistage deep learning method](https://www.synapse.org/#!Synapse:syn21478998/wiki/604432) for scoring radiographic hand and foot joint damage in rheumatoid arthritis, taking advantage of the fastai library.

It doesn't matter if you don't come from a technical or a mathematical background (though it's okay if you do too!); we wrote this course to make deep learning accessible to as many people as possible. The only prerequisite is that you know how to code (a year of experience is enough), preferably in Python, and that you have at least followed a high school math course.

Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. A lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you'll see in this course, those people are wrong. Here's a few things you *absolutely don't need* to do world-class deep learning:

| Myth (don't need) | Truth
|---|---|
| Lots of math | Just high school math is sufficient
| Lots of data | We've seen record-breaking results with <50 items of data
| Lots of expensive computers | You can get what you need for state of the art work for free

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## The software you will be using

In this course, you'll be using [PyTorch](https://pytorch.org/), [fastai](https://docs.fast.ai), Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), and [Gradio](https://gradio.app/).

We've completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. We spent over a thousand hours testing PyTorch before deciding that we would use it for future courses, software development, and research. PyTorch is now the world's fastest-growing deep learning library and is already used for most research papers at top conferences.

PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library one of the most popular libraries for adding this higher-level functionality on top of PyTorch. In this course, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai.

Transformers is a popular library focused on natural language processing (NLP) using *transformers models*. In the course you'll see how to create a cutting-edge transfomers model using this library to detect similar concepts in patent applications.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Why deep learning?

Deep learning has power, flexibility, and simplicity. That's why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. Here's a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:

- **Natural language processing (NLP)** Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept
- **Computer vision** Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles
- **Medicine** Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy
- **Biology** Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions
- **Image generation** Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists
- **Recommendation systems** Web search; product recommendations; home page layout
- **Playing games** Chess, Go, most Atari video games, and many real-time strategy games
- **Robotics** Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up
- **Other applications** Financial and logistical forecasting, text to speech, and much more...

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## What you will learn

After finishing this course you will know:

- How to train models that achieve state-of-the-art results in:
  - Computer vision, including image classification (e.g., classifying pet photos by breed)
  - Natural language processing (NLP), including document classification (e.g., movie review sentiment analysis) and phrase similarity
  - Tabular data with categorical data, continuous data, and mixed data
  - Collaborative filtering (e.g., movie recommendation)
- How to turn your models into web applications, and deploy them
- Why and how deep learning models work, and how to use that knowledge to improve the accuracy, speed, and reliability of your models
- The latest deep learning techniques that really matter in practice
- How to implement stochastic gradient descent and a complete training loop from scratch

Here are some of the techniques covered (don't worry if none of these words mean anything to you yet--you'll learn them all soon): 

- Random forests and gradient boosting
- Affine functions and nonlinearities
- Parameters and activations
- Transfer learning
- Stochastic gradient descent (SGD)
- Data augmentation
- Weight decay
- Image classification
- Entity and word embeddings
- And much more

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## How do I get started?

To watch the videos, click on the *Lessons* section in the navigation sidebar. The videos are all captioned; while watching the video click the "CC" button to turn them on and off. To get a sense of what's covered in a lesson, you might want to skim through some lesson notes taken by one of our students (thanks Daniel!). Here's his [lesson 7 notes](Lessons/Summaries/lesson7.qmd) and [lesson 8 notes](Lessons/Summaries/lesson8.qmd). You can also access all the videos through [this YouTube playlist](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU).

Each video is designed to go with various chapters from the book. The entirety of every chapter of the book is available as an interactive Jupyter Notebook. [Jupyter Notebook](https://jupyter.org/) is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it! Since the most important thing for learning deep learning is writing code and experimenting, it's important that you have a great platform for experimenting with code.

We'll mainly use [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks) and [Paperspace Gradient](https://gradient.run/notebooks) because we've found they work really well for this course, and have good free options. We also will do some parts of the course on your own laptop. (If you don't have a Paperspace account yet, sign up with [this link](https://console.paperspace.com/signup?R=lg6rnx) to get $10 credit -- and we get a credit too.)

We strongly suggest *not* using your own computer for training models in this course, unless you're very experienced with Linux system adminstration and handling GPU drivers, CUDA, and so forth.

If you need help, there's a [wonderful online community](https://forums.fast.ai/c/p1v5/54) ready to help you at forums.fast.ai. Before asking a question on the forums, search carefully to see if your question has been answered before.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::


=== File: ./python.py ===
import os

files_to_process = [""]
dirs_to_process = ["."]

with open("output.txt", 'w', encoding='utf-8') as f:
    # 处理单个文件
    for file_path in files_to_process:
        if os.path.isfile(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as input_f:
                    f.write(f"\n=== File: {file_path} ===\n")
                    content = input_f.read()
                    f.write(content)
                    f.write("\n")
            except Exception as e:
                pass

    # 处理目录
    for d in dirs_to_process:
        for root, dirs, files in os.walk(d):
            for file in files:
                file_path = os.path.join(root, file)
                if any(x in file_path for x in [
                    ".log", ".git", ".sample", ".pyc", ".md",
                    "migration", "__", "moment", "modal"
                ]):
                    continue
                try:
                    with open(file_path, 'r', encoding='utf-8') as input_f:
                        f.write(f"\n=== File: {file_path} ===\n")
                        content = input_f.read()
                        f.write(content)
                        f.write("\n")
                except Exception as e:
                    pass

=== File: ./output.txt ===

=== File: ./_quarto.yml ===
project:
  type: website
  resources:
    - "www/*"
  preview:
    port: 3000
    browser: false

format:
  html:
    theme: cosmo
    css: styles.css
    toc: true

website:
  title: "SEM Coding Summer 2025"
  site-url: https://Imbernoulli.github.io/summer/
  description: "清华大学经管学院暑期代码能力提升培训"
  twitter-card: true
  open-graph: true
  reader-mode: true
  page-navigation: true
  repo-branch: master
  repo-url: https://github.com/Imbernoulli/summer
  repo-actions: [issue]
  navbar:
    background: primary
    search: true
    right:
      - icon: github
        href: https://github.com/Imbernoulli/summer
  sidebar:
    style: "floating"

metadata-files:
  - sidebar.yml


=== File: ./.nojeklyll ===


=== File: ./.DS_Store ===

=== File: ./sidebar.yml ===
website:
  sidebar:
    contents:
      - index.qmd
      - section: 计算机工具简介
        contents:
          - Lessons/lesson1.qmd
          - Lessons/lesson2.qmd
          - Lessons/lesson3.qmd
      - section: 计算机实战应用
        contents:
          - Lessons/lesson4.qmd
          - Lessons/lesson5.qmd
          - Lessons/lesson6.qmd
          - Lessons/lesson7.qmd
          - Lessons/lesson11.qmd
          - Lessons/lesson10.qmd
      - section: 计算机前沿展望
        contents:
          - Lessons/lesson8.qmd
          - Lessons/lesson9.qmd
      - section: 课程资源
        contents:
          - Lessons/byl.qmd
      - section: 计算机系相关资源
        contents:
          - text: "技能培训"
            href: https://summer24.net9.org
          - text: "技能文档"
            href: https://docs.net9.org


=== File: ./LICENSE ===
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


=== File: ./styles.css ===
div.description {
  font-style: italic;
}

.quarto-figure>figure {
  margin-top: 0.5rem;
}

figcaption.figure-caption{
  margin-top: 0;
  margin-left: 1rem;
  font-style: italic;
}

.cell-output pre {
    margin-left: 0.8rem;
    margin-top: 0;
    background: none;
    border-left: 2px solid lightsalmon;
    border-top-left-radius: 0;
    border-top-right-radius: 0;
}
  
.cell-output .sourceCode {
    background: none;
    margin-top: 0;
}
  
.cell > .sourceCode {
    margin-bottom: 0;
}
  
main ol ol, main ul ul, main ol ul, main ul ol {
    margin-bottom: 0.5em;
}



=== File: ./MANIFEST.in ===
include settings.ini
include LICENSE
include CONTRIBUTING.md
include README.md
recursive-exclude * __pycache__


=== File: ./index.qmd ===
---
title: "Practical Deep Learning"
image: https://course.fast.ai/www/social.png
description: A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.
creator: "@jeremyphoward"
---

:::{.callout-tip}
## New!

We just launched a new >30 hour video course for more experienced students:

[Practical Deep Learning for Coders part 2: *Deep Learning Foundations to Stable Diffusion*](Lessons/part2.qmd)
:::

::: {layout="[30,70]"}

![](images/pencil_sketch.png)

This free course is designed for people (and bunnies!) with some coding experience who want to learn how to apply deep learning and machine learning to practical problems.<br /><br />Deep learning can do all kinds of amazing things. For instance, all illustrations throughout this website are made with deep learning, using [DALL-E 2](https://openai.com/dall-e-2/).

:::

## Welcome!

Practical Deep Learning for Coders 2022 part 1, recorded at the [University of Queensland](https://www.uq.edu.au/), covers topics such as how to:

::: {layout="[30,70]"}

![](images/imagine.png)

- Build and train deep learning models for computer vision, natural language processing, tabular analysis, and collaborative filtering problems
- Create random forests and regression models
- Deploy models
- Use PyTorch, the world’s fastest growing deep learning software, plus popular libraries like fastai and Hugging Face

:::

There are 9 lessons, and each lesson is around 90 minutes long. The course is based on our [5-star rated book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527), which is [freely available](Resources/book.qmd) online.

You don’t need any special hardware or software — we’ll show you how to use free resources for both building and deploying models. You don’t need any university math either — we’ll teach you the calculus and linear algebra you need during the course.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Real results

以下是一个用Markdown格式编写的复杂数学公式：

$$
\int_{-\infty}^{\infty} \frac{e^{-x^2/2}}{\sqrt{2\pi}} \left( \sum_{n=0}^{\infty} \frac{H_n(x)}{\sqrt{2^n n!}} z^n \right) dx = e^{z^2/2}
$$

这是高斯积分与埃尔米特多项式的一个关系式，其中$H_n(x)$是埃尔米特多项式。

如果您需要更复杂的公式或特定主题的公式，请告诉我。

Our videos have been viewed over 6,000,000 times already! Take a look at the [dozens of testimonials](Resources/testimonials.qmd) about our book and course by alumni, top academics, and industry experts.

::: {layout="[70,30]"}

> *'Deep Learning is for everyone' we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning.*

![](images/people/norvig.jpg)<br>**Peter Norvig**<br>Director of Research, Google

:::

By the end of the second lesson, you will have built and deployed your own deep learning model on data you collect. Many students post their course projects to our forum; you can [view them here](https://forums.fast.ai/t/share-your-work-here/96015). For instance, if there’s an unknown dinosaur in your backyard, maybe you need this [dinosaur classifier](https://notebooksg.jarvislabs.ai/U7mQvWgvra53-YvTogLgJCfNzgJxRJv238Go2bHoAmHBqQFHoOL1ZFeKDG8gYmnO/)!

![](https://user-images.githubusercontent.com/346999/177054810-3e56fcb1-4fb7-418a-b10b-351bab20bf6b.png)

Alumni of our course have gone on to jobs at organizations like **Google Brain**, **OpenAI**, **Adobe**, **Amazon**, and **Tesla**, published research at top conferences such as [NeurIPS](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems), and created startups using skills they learned here. Petro Cuenca, lead developer of the widely-acclaimed [Camera+](https://camera.plus/) app, after completing the course went on to add deep learning features to his product, which was then [featured by Apple](https://twitter.com/pcuenq/status/1540121042596904963) for its “machine learning magic”.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Your teacher

::: {layout="[70,30]"}

I am Jeremy Howard, your guide on this journey. I lead the development of fastai, the software that you'll be using throughout this course. I have been using and teaching machine learning for around 30 years. I was the top-ranked competitor globally in machine learning competitions on Kaggle (the world's largest machine learning community) two years running. Following this success, I became the President and Chief Scientist of Kaggle. Since first using neural networks 25 years ago, I have led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic (chosen by MIT Tech Review as one of the "world's smartest companies"). 

![Jeremy Howard](images/people/jeremy.jpg)

:::

I am the co-founder, along with Dr. Rachel Thomas, of fast.ai, the organization behind this course. At fast.ai we care a lot about teaching. In this course, I start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on… We always teach through examples. We ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Is this course for me?

Previous fast.ai courses have been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Many students have told us about how they've become [multiple gold medal winners](https://forums.fast.ai/t/my-first-gold-medal/54237) of [international machine learning competitions](https://towardsdatascience.com/my-3-year-journey-from-zero-python-to-deep-learning-competition-master-6605c188eec7), [received offers](https://forums.fast.ai/t/how-has-your-journey-been-so-far-learners/6480/2) from top companies, and having [research](https://ui.adsabs.harvard.edu/abs/2020EGUGA..2221465A/abstract) [papers](http://www.ieomsociety.org/ieom2020/papers/37.pdf) [published](https://pubs.rsna.org/doi/abs/10.1148/ryai.2019190113?journalCode=ai). For instance, Isaac Dimitrovsky [told us](https://forums.fast.ai/t/thanks-ra2-dream-challenge-win/76875) that he had "*been playing around with ML for a couple of years without really grokking it... [then] went through the fast.ai part 1 course late last year, and it clicked for me*". He went on to achieve first place in the prestigious international [RA2-DREAM Challenge](https://www.synapse.org/#!Synapse:syn20545111/wiki/594083) competition! He developed a [multistage deep learning method](https://www.synapse.org/#!Synapse:syn21478998/wiki/604432) for scoring radiographic hand and foot joint damage in rheumatoid arthritis, taking advantage of the fastai library.

It doesn't matter if you don't come from a technical or a mathematical background (though it's okay if you do too!); we wrote this course to make deep learning accessible to as many people as possible. The only prerequisite is that you know how to code (a year of experience is enough), preferably in Python, and that you have at least followed a high school math course.

Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. A lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you'll see in this course, those people are wrong. Here's a few things you *absolutely don't need* to do world-class deep learning:

| Myth (don't need) | Truth
|---|---|
| Lots of math | Just high school math is sufficient
| Lots of data | We've seen record-breaking results with <50 items of data
| Lots of expensive computers | You can get what you need for state of the art work for free

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## The software you will be using

In this course, you'll be using [PyTorch](https://pytorch.org/), [fastai](https://docs.fast.ai), Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), and [Gradio](https://gradio.app/).

We've completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. We spent over a thousand hours testing PyTorch before deciding that we would use it for future courses, software development, and research. PyTorch is now the world's fastest-growing deep learning library and is already used for most research papers at top conferences.

PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library one of the most popular libraries for adding this higher-level functionality on top of PyTorch. In this course, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai.

Transformers is a popular library focused on natural language processing (NLP) using *transformers models*. In the course you'll see how to create a cutting-edge transfomers model using this library to detect similar concepts in patent applications.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## Why deep learning?

Deep learning has power, flexibility, and simplicity. That's why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. Here's a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:

- **Natural language processing (NLP)** Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept
- **Computer vision** Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles
- **Medicine** Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy
- **Biology** Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions
- **Image generation** Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists
- **Recommendation systems** Web search; product recommendations; home page layout
- **Playing games** Chess, Go, most Atari video games, and many real-time strategy games
- **Robotics** Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up
- **Other applications** Financial and logistical forecasting, text to speech, and much more...

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## What you will learn

After finishing this course you will know:

- How to train models that achieve state-of-the-art results in:
  - Computer vision, including image classification (e.g., classifying pet photos by breed)
  - Natural language processing (NLP), including document classification (e.g., movie review sentiment analysis) and phrase similarity
  - Tabular data with categorical data, continuous data, and mixed data
  - Collaborative filtering (e.g., movie recommendation)
- How to turn your models into web applications, and deploy them
- Why and how deep learning models work, and how to use that knowledge to improve the accuracy, speed, and reliability of your models
- The latest deep learning techniques that really matter in practice
- How to implement stochastic gradient descent and a complete training loop from scratch

Here are some of the techniques covered (don't worry if none of these words mean anything to you yet--you'll learn them all soon): 

- Random forests and gradient boosting
- Affine functions and nonlinearities
- Parameters and activations
- Transfer learning
- Stochastic gradient descent (SGD)
- Data augmentation
- Weight decay
- Image classification
- Entity and word embeddings
- And much more

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::

## How do I get started?

To watch the videos, click on the *Lessons* section in the navigation sidebar. The videos are all captioned; while watching the video click the "CC" button to turn them on and off. To get a sense of what's covered in a lesson, you might want to skim through some lesson notes taken by one of our students (thanks Daniel!). Here's his [lesson 7 notes](Lessons/Summaries/lesson7.qmd) and [lesson 8 notes](Lessons/Summaries/lesson8.qmd). You can also access all the videos through [this YouTube playlist](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU).

Each video is designed to go with various chapters from the book. The entirety of every chapter of the book is available as an interactive Jupyter Notebook. [Jupyter Notebook](https://jupyter.org/) is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it! Since the most important thing for learning deep learning is writing code and experimenting, it's important that you have a great platform for experimenting with code.

We'll mainly use [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks) and [Paperspace Gradient](https://gradient.run/notebooks) because we've found they work really well for this course, and have good free options. We also will do some parts of the course on your own laptop. (If you don't have a Paperspace account yet, sign up with [this link](https://console.paperspace.com/signup?R=lg6rnx) to get $10 credit -- and we get a credit too.)

We strongly suggest *not* using your own computer for training models in this course, unless you're very experienced with Linux system adminstration and handling GPU drivers, CUDA, and so forth.

If you need help, there's a [wonderful online community](https://forums.fast.ai/c/p1v5/54) ready to help you at forums.fast.ai. Before asking a question on the forums, search carefully to see if your question has been answered before.

:::{.callout-tip}
## Get started

Start watching [lesson 1](Lessons/lesson1.qmd) now!
:::


=== File: ./images/oil_teddy.png ===

=== File: ./images/imagine.png ===

=== File: ./images/bear_sunnies.png ===

=== File: ./images/book.png ===

=== File: ./images/scales.png ===

=== File: ./images/bear_sunnies2.png ===

=== File: ./images/myths.png ===

=== File: ./images/forest.png ===

=== File: ./images/bunny_net.png ===

=== File: ./images/pencil_sketch.png ===

=== File: ./images/excited_code2.png ===

=== File: ./images/excited_code3.png ===

=== File: ./images/course2022p2l1.jpg ===

=== File: ./images/cute_bunny.png ===

=== File: ./images/teddy_net.png ===

=== File: ./images/people/sravya.jpg ===

=== File: ./images/people/shetty.jpg ===

=== File: ./images/people/economist.png ===

=== File: ./images/people/helena.png ===

=== File: ./images/people/rt-head.jpg ===

=== File: ./images/people/erikb.jpg ===

=== File: ./images/people/sara.png ===

=== File: ./images/people/nichol.jpg ===

=== File: ./images/people/dsakva.png ===

=== File: ./images/people/mattob.jpeg ===

=== File: ./images/people/taro.png ===

=== File: ./images/people/robink.jpeg ===

=== File: ./images/people/chrisk.jpeg ===

=== File: ./images/people/jh-head.jpg ===

=== File: ./images/people/dario.png ===

=== File: ./images/people/norvig.jpg ===

=== File: ./images/people/jeremy.jpg ===

=== File: ./images/people/harvard_shield.png ===

=== File: ./images/people/mit.jpeg ===

=== File: ./images/people/yannet.jpg ===

=== File: ./Resources/kaggle.qmd ===
---
title: Kaggle
---

[Kaggle](https://www.kaggle.com) is the world's largest data science community. One of Kaggle's features is "Notebooks", which is "*a cloud computational environment that enables reproducible and collaborative analysis*". In particular, Kaggle provides access to GPUs for free. Every lesson provides direct links to notebooks on Kaggle that are ready for you to start using. Click "**Copy & Edit**" at the top right of any notebook to start working with it.

**In order to use a GPU on Kaggle, your account must be phone verified. You can enable this on [your account page](https://www.kaggle.com/me/account) (after you've signed up and are logged in) under "Phone Verification".**

Here's some information from Kaggle's notebook page:

> Jupyter notebooks consist of a sequence of cells, where each cell is formatted in either Markdown (for writing text) or in a programming language of your choice (for writing code). To start a notebook, click on “Create Notebook”, and select “Notebook”. This will open the Notebooks editing interface.
>
>"[Comprehensive data exploration with Python](https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python/notebook)" is a great example of a Python Jupyter Notebook-type.

Instead of Kaggle, you can also use Paperspace Gradient. We've found they work really well for this course, and have good free options. If you don't have a Paperspace account yet, sign up with [this link](https://console.paperspace.com/signup?R=lg6rnx) to get $10 credit -- and we get a credit too. Gradient is a little harder to use because the notebooks are not ready-to-run, but it's more powerful because you get a full Linux environment to work in. It's a good option for folks who are comfortable working with git and the command line. To access the course notebooks on Paperspace, you will need to clone [this GitHub repo](https://github.com/fastai/course22).


=== File: ./Resources/book.qmd ===
---
title: The book
---

::: {layout="[30,70]"}

![](../images/book.png)

[Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) is the book that forms the basis for this course. We recommend reading the book as you complete the course. There's a few ways to read the book -- you can buy it as a paper book or Kindle ebook, or you can [read it for free online](https://github.com/fastai/fastbook). The whole book is written as Jupyter notebooks, so you can also execute all the code in the book yourself.<br><br>To go to the interactive Jupyter version of any chapter, click any of the chapter links in the [Colab](#Colab) section immediately below. If you just want to read the book, without interacting with the code, jump to the [nbviewer](#nbviewer) section.

:::

## Colab

[Google Colab](https://colab.research.google.com/) is a free (with paid subscription option) platform for running Jupyter Notebooks in the cloud. You can open any chapter of the book in Colab by clicking on one of these links:

- [Chapter 1, Intro](https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb)
- [Chapter 2, Production](https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb)
- [Chapter 3, Ethics](https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb)
- [Chapter 4, MNIST Basics](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb)
- [Chapter 5, Pet Breeds](https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb)
- [Chapter 6, Multi-Category](https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb)
- [Chapter 7, Sizing and TTA](https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb)
- [Chapter 8, Collab](https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb)
- [Chapter 9, Tabular](https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb)
- [Chapter 10, NLP](https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb)
- [Chapter 11, Mid-Level API](https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb)
- [Chapter 12, NLP Deep-Dive](https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb)
- [Chapter 13, Convolutions](https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb)
- [Chapter 14, Resnet](https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb)
- [Chapter 15, Arch Details](https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb)
- [Chapter 16, Optimizers and Callbacks](https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb)
- [Chapter 17, Foundations](https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb)
- [Chapter 18, GradCAM](https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb)
- [Chapter 19, Learner](https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb)
- [Chapter 20, Conclusion](https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb)

## nbviewer

[nbviewer](https://nbviewer.org/) is a free platform for reading Jupyter Notebooks. You can open any chapter of the book in nbviewer by clicking on one of these links:

- [Chapter 1, Intro](https://nbviewer.org/github/fastai/fastbook/blob/master/01_intro.ipynb)
- [Chapter 2, Production](https://nbviewer.org/github/fastai/fastbook/blob/master/02_production.ipynb)
- [Chapter 3, Ethics](https://nbviewer.org/github/fastai/fastbook/blob/master/03_ethics.ipynb)
- [Chapter 4, MNIST Basics](https://nbviewer.org/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb)
- [Chapter 5, Pet Breeds](https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb)
- [Chapter 6, Multi-Category](https://nbviewer.org/github/fastai/fastbook/blob/master/06_multicat.ipynb)
- [Chapter 7, Sizing and TTA](https://nbviewer.org/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb)
- [Chapter 8, Collab](https://nbviewer.org/github/fastai/fastbook/blob/master/08_collab.ipynb)
- [Chapter 9, Tabular](https://nbviewer.org/github/fastai/fastbook/blob/master/09_tabular.ipynb)
- [Chapter 10, NLP](https://nbviewer.org/github/fastai/fastbook/blob/master/10_nlp.ipynb)
- [Chapter 11, Mid-Level API](https://nbviewer.org/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb)
- [Chapter 12, NLP Deep-Dive](https://nbviewer.org/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb)
- [Chapter 13, Convolutions](https://nbviewer.org/github/fastai/fastbook/blob/master/13_convolutions.ipynb)
- [Chapter 14, Resnet](https://nbviewer.org/github/fastai/fastbook/blob/master/14_resnet.ipynb)
- [Chapter 15, Arch Details](https://nbviewer.org/github/fastai/fastbook/blob/master/15_arch_details.ipynb)
- [Chapter 16, Optimizers and Callbacks](https://nbviewer.org/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb)
- [Chapter 17, Foundations](https://nbviewer.org/github/fastai/fastbook/blob/master/17_foundations.ipynb)
- [Chapter 18, GradCAM](https://nbviewer.org/github/fastai/fastbook/blob/master/18_CAM.ipynb)
- [Chapter 19, Learner](https://nbviewer.org/github/fastai/fastbook/blob/master/19_learner.ipynb)
- [Chapter 20, Conclusion](https://nbviewer.org/github/fastai/fastbook/blob/master/20_conclusion.ipynb)



=== File: ./Resources/forums.qmd ===
---
title: Forums
---

If you need help, there's a wonderful online community ready to help you at forums.fast.ai. Before asking a question on the forums, search carefully to see if your question has been answered before. (The forum system won't let you post until you've spent a few minutes on the site reading existing topics.)

Every lesson has a dedicated forum thread---so that's the place to look first to see if your question has already been answered:

- [Lesson 1](https://forums.fast.ai/t/lesson-1-official-topic/95287)
- [Lesson 2](https://forums.fast.ai/t/lesson-2-official-topic/96033)
- [Lesson 3](https://forums.fast.ai/t/lesson-3-official-topic/96254)
- [Lesson 4](https://forums.fast.ai/t/lesson-4-official-topic/96441)
- [Lesson 5](https://forums.fast.ai/t/lesson-5-official-topic/96491/12)
- [Lesson 6](https://forums.fast.ai/t/lesson-6-official-topic/96972)
- [Lesson 7](https://forums.fast.ai/t/lesson-7-official-topic/97076)
- [Lesson 8](https://forums.fast.ai/t/lesson-8-official-topic/97159)

If you're hitting an error with your code, click the magnifying glass on the top-right of any forums page to access Search, and try searching for a few words from the error message.

## Forum etiquette

1.  If you like a post, it's better to "like" it with the :heart: rather than commenting.  It saves traffic in the Forums and makes it easier for everyone to find posts.  
2.  **Please be mindful of looking to see if a topic exists before starting a new thread.**
3. Do a quick **search** of the forums to see if your question is already under discussion.  
4. **Do not @ people if you are not referencing them for a specific reason** that requires the attention of that forum member. Be especially mindful in mentioning Jeremy.
5. You can use this thread to chat about pretty anything (except politics/religion/stuff that might start a flame war!): [General chat](https://forums.fast.ai/t/general-course-chat/95293). For other posts, please keep them to stuff that's at least somewhat related to the course and its content.


=== File: ./Resources/testimonials.qmd ===
---
title: Testimonials
---

## Praise for the book

- "'Deep Learning is for everyone' we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning." -- **Peter Norvig**, Director of Research, Google
- "As artificial intelligence has moved into the era of deep learning, it behooves all of us to learn as much as possible about how it works. Deep Learning for Coders provides a terrific way to initiate that, even for the uninitiated, achieving the feat of simplifying what most of us would consider highly complex" -- **Eric Topol**, Author of *Deep Medicine*; Professor: Scripps Research
- "If you are looking for a guide that starts at the ground floor and takes you to the cutting edge of research, this is the book for you. Don't let those PhDs have all the fun---you too can use deep learning to solve practical problems." -- **Hal Varian**, Emeritus Professor, UC Berkeley; Chief Economist, Google
- "Jeremy and Sylvain take you on an interactive--in the most literal sense as each line of code can be run in a notebook--journey through the loss valleys and performance peaks of deep learning. Peppered with thoughtful anecdotes and practical intuitions from years of developing and teaching machine learning, the book strikes the rare balance of communicating deeply technical concepts in a conversational and light-hearted way. In a faithful translation of fast.ai's award-winning online teaching philosophy, the book provides you with state-of-the-art practical tools and the real-world examples to put them to use. Whether you're a beginner or a veteran, this book will fast-track your deep learning journey and take you to new heights--and depths." -- **Sebastian Ruder**, Research Scientist, Deepmind
- "Jeremy Howard and Sylvain Gugger have authored a bravura of a book that successfully bridges the AI domain with the rest of the world. This work is a singularly substantive and insightful yet absolutely relatable primer on deep learning for anyone who is interested in this domain: a lodestar book amongst many in this genre." -- **Anthony Chang**, Chief Intelligence and Innovation Officer, Children's Hospital of Orange County
- "How can I 'get' deep learning without getting bogged down? How can I quickly learn the concepts, craft, and tricks-of-the-trade using examples and code? Right here. Don't miss the new locus classicus for hands-on deep learning" -- **Oren Etzioni**, Professor: University of Washington, CEO: Allen Institute for AI
- "This book is a rare gem- the product of carefully crafted and highly effective teaching, iterated and refined over several years resulting in thousands of happy students. I'm one of them. fast.ai changed my life in a wonderful way, and I'm convinced that they can do the same for you." -- **Jason Antic**, Creator of *DeOldify*
- "Deep Learning for Coders is an incredible resource. The book wastes no time and teaches how to use Deep Learning effectively in the first few chapters. It then covers the inner workings of ML models and frameworks in a thorough but accessible fashion, which will allow you to understand and build upon them. I wish there was a book like this when I started learning ML, it is an instant classic!" -- **Emmanuel Ameisen**, Author of *Building Machine Learning Powered Applications*
- "Gugger and Howard have created an ideal resource for anyone who has ever done even a little bit of coding. This book, and the fast.ai courses that go with it, simply and practically demystify deep learning using a hands on approach, with pre-written code that you can explore and re-use. No more slogging through theorems and proofs about abstract concepts. In Chapter 1 you will build your first deep learning model, and by the end of the book you will know how to read and understand the Methods section of any deep learning paper." -- **Curtis Langlotz**, Director, Center for Artificial Intelligence in Medicine and Imaging, Stanford University
- "This book demystifies the blackest of black boxes: Deep Learning. It enables quick code experimentations with a complete python notebook. It also dives into the ethical implication of Artificial Intelligence, and shows how to avoid it from becoming dystopian." -- **Guillaume Chaslot **, Fellow, Mozilla
- "As a pianist turned OpenAI researcher, I'm often asked for advice on getting into Deep Learning, and I always point to fastai. This book manages the seemingly impossible - it's a friendly guide to a complicated subject, and yet it's full of cutting-edge gems that even advanced practitioners will love." -- **Christine Payne**, Researcher, OpenAI; Creator of *Musenet* and *Jukebox*
- "An extremely hands-on, accessible book to help anyone quickly get started on their deep learning project. It's a very clear, easy to follow and honest guide to practical deep learning. Helpful for beginners to executives/managers alike. The guide I wished I had years ago!" -- **Carol Reiley**, Founding President and Chair, Drive.ai
- "Jeremy and Sylvain's expertise in deep learning, their practical approach to ML, and their many valuable open-source contributions have made then key figures in the PyTorch community. This book, which continues the work that they and the fast.ai community are doing to make ML more accessible, will greatly benefit the entire field of AI." -- **Jerome Pesenti**, Vice President of AI, Facebook
- "Deep Learning is one of the most important technologies now, responsible for many amazing recent advances in AI. It used to be only for PhDs, but no longer! This book, based on a very popular fast.ai course, makes DL accessible to anyone with programming experience. This book teaches the "whole game", with excellent hands-on examples and a companion interactive site. And PhDs will also learn a lot." -- **Gregory Piatetsky-Shapiro**, President, KDnuggets
- "An extension of the fast.ai course that I have consistently recommended for years, this book by Jeremy and Sylvain, two of the best Deep Learning experts today, will take you from beginner to qualified practitioner in a matter of months. Finally, something positive has come out of 2020!" -- **Louis Monier**, Founder of Altavista; former Head of Airbnb AI Lab
- "We recommend this book! Deep Learning for Coders with fastai and PyTorch uses advanced frameworks to move quickly through concrete, real-world artificial intelligence or automation tasks. This leaves time to cover usually neglected topics, like safely taking models to production and a much-needed chapter on data ethics." -- **John Mount and Nina Zumel**, Authors of *Practical Data Science with R*
- "Deep Learning for Coders is much more than a book, as it is accompanied by fastai, a robust community and powerful machine learning framework built on pytorch.  State of the art methods are provided out of the box with no compromises, including tricks to make one competitive with top industrial research labs with only a fraction of the compute.  The philosophies with respect to education and learning espoused in this book and companion courses have given me the tools to accelerate my personal growth on many dimensions.   Through fastai and this book, I have also learned valuable practices for software engineering, testing, iterative development, and ethical frameworks.  Jeremy is an awe-inspiring individual who is not only among the top data scientists in the world but an impressive mental athlete who has mastered a wide variety of fields, and you get a glimpse into his mind in this book.  Finally, Jeremy and Sylvian are exceptional in that they teach with empathy at all times, which translates into the most approachable book you can buy on deep learning today." -- **Hamel Husain**, Machine Learning Engineer: GitHub; Product Lead: CodeSearchNet
- "This book is "for Coders" and does not require a PhD. Now, I do have a PhD and I am no coder, so why have I been asked to review this book? Well, to tell you how friggin awesome it really is! Within a couple of pages from Chapter 1 you'll figure out how to get a state-of-the-art network able to classify cat vs. dogs in 4 lines of code and less than 1 minute of computation. Then you land Chapter 2, which takes you from model to production, showing how you can serve a webapp in no time, without any HTML or JavaScript, without owning a server. I think of this book as an onion. A complete package that works using the best possible settings. Then, if some alterations are required, you can peel the outer layer. More tweaks? You can keep discarding shells. Even more? You can go as deep as using bare PyTorch. You'll have three independent voices accompanying you around your journey along this 500 page book, providing you guidance and individual perspective." -- **Alfredo Canziani**, Professor of Computer Science, NYU
- "Deep Learning for Coders with fastai and Pytorch is an approachable conversationally-driven book that uses the whole game approach to teaching deep learning concepts. The book focuses on getting your hands dirty right out of the gate with real examples and bringing the reader along with reference concepts only as needed. A practitioner may approach the world of deep learning in this book through hands-on examples in the first half, but will find themselves naturally introduced to deeper concepts as they traverse the back half of the book with no pernicious myths left unturned." -- **Josh Patterson**, Patterson Consulting
- "When your model is not performing as well as you had hoped (almost always), read this book!  It provides a great combination of Jeremy's practical experience and Sylvain theoretical knowledge, and makes the art of deep learning accessible." -- **Ron Kohavi**, VP and Technical Fellow at Airbnb, and co-author of *Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing*
- "Jeremy, Sylvain and Rachel are the absolute masters of creating accessible tools and building community around AI. This is yet another installment of the fast.ai team creating an amazing resource that will help onboard the next hundred thousand aspiring AI researchers globally. Congrats!!!" -- **Joe Spisak**, PyTorch Product Manager, Facebook
- "I am very impressed with your teaching materials. You take great care in presenting difficult topics to a varied audience. Thanks so much for sharing this." -- **Brian Lovell**, Professor of AI, University of Queensland

:::{.callout-tip}
## Get started

Start watching [lesson 1](../Lessons/lesson1.qmd) now!
:::

## In the news

::: {layout="[30,70]"}

![*The Economist*](../images/people/economist.png){fig-align="left"}

"This month fast.ai, an education non-profit based in San Francisco, kicked off the third year of its course in deep learning. Since its inception it has attracted more than 100,000 students, scattered around the globe from India to Nigeria. The course and others like it come with a simple proposition: there is no need to spend years obtaining a phd in order to practise deep learning. Creating software that learns can be taught as a craft, not as a high intellectual pursuit to be undertaken only in an ivory tower. Fast.ai’s course can be completed in just seven weeks.<br/>
<br/>
Demystifying the subject, to make it accessible to anyone who wants to learn how to build ai software, is the aim of Jeremy Howard, who founded fast.ai with Rachel Thomas, a mathematician. He says school mathematics is sufficient. “No. Greek. Letters,” Mr Howard intones, thumping the table for punctuation.<br/>
<br/>
It is working."<br/>
[New schemes teach the masses to build AI](https://www.economist.com/business/2018/10/27/new-schemes-teach-the-masses-to-build-ai)

:::

---

::: {layout="[30,70]"}

![*Harvard Business Review*](../images/people/harvard_shield.png){fig-align="left"}

"fast.ai... can actually get smart, motivated students to the point of being able to create industrial-grade ML deployments."<br/>
[The Business of Artificial Intelligence](https://hbr.org/2017/07/the-business-of-artificial-intelligence)

:::

---

::: {layout="[30,70]"}

![*MIT Tech Review*](../images/people/mit.jpeg){fig-align="left"}

"Students from Fast.ai, a small organization that runs free machine-learning courses online, just created an AI algorithm that outperforms code from Google’s researchers, according to an important benchmark."<br/>
[A small team of student AI coders beats Google’s machine-learning code](https://www.technologyreview.com/2018/08/10/141098/small-team-of-ai-coders-beats-googles-code/)

:::

---

:::{.callout-tip}
## Get started

Start watching [lesson 1](../Lessons/lesson1.qmd) now!
:::

## Praise for the course

::: {layout="[30,70]"}

![**Christopher Kelly**<br/>*CEO- Nourish, Balance, Thrive*](../images/people/chrisk.jpeg)

I’ve tried (and if I’m honest) failed to scale the steep deep learning curve many times. I’ve bought several books and spent entire weekends watching presentations and workshops on YouTube. My biggest fear was that my financial and time investment in this course would end up in the MOOC graveyard with all the others. Jeremy Howard (the instructor) is amongst the best teachers I’ve known. I realise with hindsight it was the equations that were preventing me from becoming a deep learning practitioner. Jeremy brought me up to speed with the state-of-the-art, and within two weeks I was in the top half of the leaderboard for three Kaggle competitions.
<br /><br />
Many of the ideas in computer science are described using a language that makes things sound more complicated than they are. Jeremy even explained some concepts with spreadsheets, which did wonders to reassure me that I did understand what was going on inside my deep learning algorithm. Sometimes half the battle of learning a new skillset is setting up the development environment. Before starting this course, I wasted hours figuring out how to configure a productive environment. I was surprised and delighted by how easy Jeremy made setting up my deep learning environment in the cloud.
<br /><br />
I’d recommend this MOOC to anyone looking to get started in the exciting field of deep learning. I could hardly believe my luck when I discovered that the guy I just seen give a TED Talk was teaching!


:::

---

::: {layout="[30,70]"}

![**Sravya Tirukkovalur**<br/>*Vice President, Apache Sentry*](../images/people/sravya.jpg)

If you are looking to venture into the Deep learning field, look no further and take this course. It is very hands-on and adopts a top-down approach, which means everyone irrespective of varying knowledge can get started with implementing Deep learning models immediately. Another major factor why this course is very appealing is its emphasis on social relevance. That is, how can we use this awesome technology to serve the world better?

:::

---

::: {layout="[30,70]"}

![**Matt O'Brien**<br/>*Data Scientist, UCSF Neurology*](../images/people/mattob.jpeg)

This course filled a gap I couldn't find anywhere else&mdash;there really is no other source where I could learn from a 'code first' perspective. This means you can prod, poke, and cajole these networks in different ways, and see how they respond. You can quickly feel an intuitive perspective growing as you explore.

:::

---

::: {layout="[30,70]"}

![**Helena Sarin**<br/>*AI Artist*](../images/people/helena.png)

fast.ai - it’s truly amazing how many of alumni are now well known players in AI industry, first learning AI coding hands on, through generosity and deep expertise of Rachel and Jeremy 🙏🥰

:::

---

::: {layout="[30,70]"}

![**Yannet Interian**<br/>*Assistant Professor of Analytics, University of San Francisco*](../images/people/yannet.jpg)

I teach machine learning in a master’s degree program. After this course, I cannot ignore the new developments in deep learning&mdash;I will devote one third of my machine learning course to the subject. Also, I now have the tools to apply deep learning models to real world problems. Some of the best features of this course are the well-documented ipython notebooks containing the tricks needed to be a proficient deep learning practitioner. Overall, I was very impressed with this course.

:::

---

::: {layout="[30,70]"}

![**Dennis Sakva**<br/>*Energy sector analyst at Dragon Capital*](../images/people/dsakva.png)

Jeremy, your class is absolutely fantastic. I've been pitching it to all my ML friends. The best description of CNNs and RNNs out there. Your Excel spreadsheet on embeddings was an 'aha' moment for me. You're an amazing educator. Thanks!

:::

---

::: {layout="[30,70]"}

![**Sara Hooker**<br/>*Researcher, Google Brain*](../images/people/sara.png)

This is a fantastic hands on learning experience. Like many data professionals outside of academia I found deep learning to be intimidating and opaque. This class changed that and empowered me to make deep learning part of the toolkit I use at Udemy. While there are a lot of resources available online about the theoretical underpinnings of deep learning this is the only course I have found that guides students through the implementation of fundamental deep learning frameworks. 
<br /> <br />
There were three things that stood out to me that made this class special: 1) you will start coding right away and see the power of neural networks in lesson one, 2) Jeremy spends a lot of the course demystifying the subject, and in the process empowers anyone to get started in the field of deep learning, 3) many ‘tricks’ on how to optimize your architecture are passed down in rapid sequence. Save yourself a lot of time by watching this course, it will take you many more hours of trial and error to learn the same content by yourself.

:::

---

::: {layout="[30,70]"}

![**Janardhan Shetty**<br/>*Senior Big Data Engineer at Salesforce*](../images/people/shetty.jpg)

Sometimes I feared whether I would be able to solve any deep learning problems, as all the research papers I read were very mathy beyond reach of simple intuitive terms. But Jeremy and Rachel (Course Professors) believe in the theory of 'Simple is Powerful', by virtue of which anyone who takes this course will be able to confidently understand the simple techniques behind the 'magic' Deep Learning.

:::

---

::: {layout="[30,70]"}

![**Dario Fanucchi**<br/>*Co-founder and CTO at Isazi Consulting*](../images/people/dario.png)

Running a company is extremely time intensive, so I was a weary of taking on the commitment of the course. It was definitely worth it, though. It smashed my preconceptions about the technological obstructions to doing deep learning, and showed again and again examples where just a small subset of the training data and just a few epochs of training on standard GPU hardware could get most of the way towards a really good model

:::

---

::: {layout="[30,70]"}

![**Taro-Shigenori Chiba**<br/>*Organizer of the SF Deep Learning Study Group*](../images/people/taro.png)

It can take years to develop the necessary skills and knowledge for Deep Learning, especially without the support of mentors and peers. Not only did Jeremy teach us the most valuable methods and practices, he provided us with an invaluable community and environment. The course exceeded my expectations and showed me first hand how both Deep Learning and ourselves could change the world for better.

:::

---

::: {layout="[30,70]"}

![**Robin Kraft (@robinkraft)**<br/>*Product Manager at Planet Labs (Satellites)*](../images/people/robink.jpeg)

It was very empowering to be able to start training a model within minutes downloading the Jupyter notebooks. Jeremy and Rachel were excellent instructors and the content was high quality and enlightening. It was very cool to be able to read blogposts about the latest Deep Learning research and actually be able to understand it. I was surprised to be able to match academic results from just 2 years ago with pretty simple architectures.

:::

---

::: {layout="[30,70]"}

![**Nichol Bradford**<br/>*Executive Director of Transformative Tech Lab at Sofia University*](../images/people/nichol.jpg)

I'm a CEO, not a coder, so the idea that I'd be able to create a GPU deep learning server in the cloud meant learning a lot of new things&mdash;but... I did it!  Jeremy Howard is an incredible instructor and is able to make what might seem like a difficult subject completely accessible. In addition, he believes in deep learning for wide audiences so has developed a practical, experience based class.  I really enjoyed the classes, and used the videos to watch them all twice. The course covered cutting edge topics, and I now feel comfortable with deep learning concepts and can engage effectively in technical discussions with my data science team.

:::

:::{.callout-tip}
## Get started

Start watching [lesson 1](../Lessons/lesson1.qmd) now!
:::


=== File: ./_tools/testimonials.ipynb ===
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49312760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9914135",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"\"\"  - text: |\n",
    "      I’ve tried (and if I’m honest) failed to scale the steep deep learning curve many times. I’ve bought several books and spent entire weekends watching presentations and workshops on YouTube. My biggest fear was that my financial and time investment in this course would end up in the MOOC graveyard with all the others. Jeremy Howard (the instructor) is amongst the best teachers I’ve known. I realise with hindsight it was the equations that were preventing me from becoming a deep learning practitioner. Jeremy brought me up to speed with the state-of-the-art, and within two weeks I was in the top half of the leaderboard for three Kaggle competitions.\n",
    "      <br /><br />\n",
    "      Many of the ideas in computer science are described using a language that makes things sound more complicated than they are. Jeremy even explained some concepts with spreadsheets, which did wonders to reassure me that I did understand what was going on inside my deep learning algorithm. Sometimes half the battle of learning a new skillset is setting up the development environment. Before starting this course, I wasted hours figuring out how to configure a productive environment. I was surprised and delighted by how easy Jeremy made setting up my deep learning environment in the cloud.\n",
    "      <br /><br />\n",
    "      I’d recommend this MOOC to anyone looking to get started in the exciting field of deep learning. I could hardly believe my luck when I discovered that the guy I just seen give a TED Talk was teaching!\n",
    "    author: Christopher Kelly\n",
    "    position: CEO- Nourish, Balance, Thrive\n",
    "    avatar: images/people/chrisk.jpeg\n",
    "  - text: \"If you are looking to venture into the Deep learning field, look no further and take this course. It is very hands-on and adopts a top-down approach, which means everyone irrespective of varying knowledge can get started with implementing Deep learning models immediately. Another major factor why this course is very appealing is its emphasis on social relevance. That is, how can we use this awesome technology to serve the world better?\"\n",
    "    author: Sravya Tirukkovalur\n",
    "    position: Vice President, Apache Sentry\n",
    "    avatar: images/people/sravya.jpg\n",
    "  - text: \"This course filled a gap I couldn't find anywhere else&mdash;there really is no other source where I could learn from a 'code first' perspective. This means you can prod, poke, and cajole these networks in different ways, and see how they respond. You can quickly feel an intuitive perspective growing as you explore.\"\n",
    "    author: Matt O'Brien\n",
    "    position: Data Scientist, UCSF Neurology\n",
    "    avatar: images/people/mattob.jpeg\n",
    "  - text: \"I teach machine learning in a master’s degree program. After this course, I cannot ignore the new developments in deep learning&mdash;I will devote one third of my machine learning course to the subject. Also, I now have the tools to apply deep learning models to real world problems. Some of the best features of this course are the well-documented ipython notebooks containing the tricks needed to be a proficient deep learning practitioner. Overall, I was very impressed with this course.\"\n",
    "    author: Yannet Interian\n",
    "    position: Assistant Professor of Analytics, University of San Francisco\n",
    "    avatar: images/people/yannet.jpg\n",
    "  - text: \"Jeremy, your class is absolutely fantastic. I've been pitching it to all my ML friends. The best description of CNNs and RNNs out there. Your Excel spreadsheet on embeddings was an 'aha' moment for me. You're an amazing educator. Thanks!\"\n",
    "    author: Dennis Sakva\n",
    "    position: Energy sector analyst at Dragon Capital\n",
    "    avatar: images/people/dsakva.png\n",
    "  - text: |\n",
    "      This is a fantastic hands on learning experience. Like many data professionals outside of academia I found deep learning to be intimidating and opaque. This class changed that and empowered me to make deep learning part of the toolkit I use at Udemy. While there are a lot of resources available online about the theoretical underpinnings of deep learning this is the only course I have found that guides students through the implementation of fundamental deep learning frameworks. \n",
    "      <br /> <br />\n",
    "      There were three things that stood out to me that made this class special: 1) you will start coding right away and see the power of neural networks in lesson one, 2) Jeremy spends a lot of the course demystifying the subject, and in the process empowers anyone to get started in the field of deep learning, 3) many ‘tricks’ on how to optimize your architecture are passed down in rapid sequence. Save yourself a lot of time by watching this course, it will take you many more hours of trial and error to learn the same content by yourself.\n",
    "    author: Sara Hooker\n",
    "    position: Data Scientist, Udemy\n",
    "    avatar: images/people/sara.png\n",
    "  - text: \"Sometimes I feared whether I would be able to solve any deep learning problems, as all the research papers I read were very mathy beyond reach of simple intuitive terms. But Jeremy and Rachel (Course Professors) believe in the theory of 'Simple is Powerful', by virtue of which anyone who takes this course will be able to confidently understand the simple techniques behind the 'magic' Deep Learning.\"\n",
    "    author: Janardhan Shetty\n",
    "    position: Senior Big Data Engineer at Salesforce\n",
    "    avatar: images/people/shetty.jpg\n",
    "  - text: \"Running a company is extremely time intensive, so I was a weary of taking on the commitment of the course. It was definitely worth it, though. It smashed my preconceptions about the technological obstructions to doing deep learning, and showed again and again examples where just a small subset of the training data and just a few epochs of training on standard GPU hardware could get most of the way towards a really good model\"\n",
    "    author: Dario Fanucchi\n",
    "    position: Co-founder and CTO at Isazi Consulting\n",
    "    avatar: images/people/dario.png\n",
    "  - text: \"It can take years to develop the necessary skills and knowledge for Deep Learning, especially without the support of mentors and peers. Not only did Jeremy teach us the most valuable methods and practices, he provided us with an invaluable community and environment. The course exceeded my expectations and showed me first hand how both Deep Learning and ourselves could change the world for better.\"\n",
    "    author: Taro-Shigenori Chiba\n",
    "    position: Organizer of the SF Deep Learning Study Group\n",
    "    avatar: images/people/taro.png\n",
    "  - text: \"It was very empowering to be able to start training a model within minutes downloading the Jupyter notebooks. Jeremy and Rachel were excellent instructors and the content was high quality and enlightening. It was very cool to be able to read blogposts about the latest Deep Learning research and actually be able to understand it. I was surprised to be able to match academic results from just 2 years ago with pretty simple architectures.\"\n",
    "    author: Robin Kraft (@robinkraft)\n",
    "    position: Product Manager at Planet Labs (Satellites)\n",
    "    avatar: images/people/robink.jpeg\n",
    "  - text: \"I'm a CEO, not a coder, so the idea that I'd be able to create a GPU deep learning server in the cloud meant learning a lot of new things&mdash;but with all the help available on the wiki and from the instructors and community on the forum I did it!  Jeremy Howard is an incredible instructor and is able to make what might seem like a difficult subject completely accessible. In addition, he believes in deep learning for wide audiences so has developed a practical, experience based class.  I really enjoyed the classes, and used the videos to watch them all twice. The course covered cutting edge topics, and I now feel comfortable with deep learning concepts and can engage effectively in technical discussions with my data science team.\"\n",
    "    author: Nichol Bradford\n",
    "    position: Executive Director of Transformative Tech Lab at Sofia University\n",
    "    avatar: images/people/nichol.jpg\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae0dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e917bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ym = yaml.safe_load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0fe3a23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Christopher Kelly**<br/>*CEO- Nourish, Balance, Thrive*](../images/people/chrisk.jpeg)\n",
      "\n",
      "I’ve tried (and if I’m honest) failed to scale the steep deep learning curve many times. I’ve bought several books and spent entire weekends watching presentations and workshops on YouTube. My biggest fear was that my financial and time investment in this course would end up in the MOOC graveyard with all the others. Jeremy Howard (the instructor) is amongst the best teachers I’ve known. I realise with hindsight it was the equations that were preventing me from becoming a deep learning practitioner. Jeremy brought me up to speed with the state-of-the-art, and within two weeks I was in the top half of the leaderboard for three Kaggle competitions.\n",
      "<br /><br />\n",
      "Many of the ideas in computer science are described using a language that makes things sound more complicated than they are. Jeremy even explained some concepts with spreadsheets, which did wonders to reassure me that I did understand what was going on inside my deep learning algorithm. Sometimes half the battle of learning a new skillset is setting up the development environment. Before starting this course, I wasted hours figuring out how to configure a productive environment. I was surprised and delighted by how easy Jeremy made setting up my deep learning environment in the cloud.\n",
      "<br /><br />\n",
      "I’d recommend this MOOC to anyone looking to get started in the exciting field of deep learning. I could hardly believe my luck when I discovered that the guy I just seen give a TED Talk was teaching!\n",
      "\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Sravya Tirukkovalur**<br/>*Vice President, Apache Sentry*](../images/people/sravya.jpg)\n",
      "\n",
      "If you are looking to venture into the Deep learning field, look no further and take this course. It is very hands-on and adopts a top-down approach, which means everyone irrespective of varying knowledge can get started with implementing Deep learning models immediately. Another major factor why this course is very appealing is its emphasis on social relevance. That is, how can we use this awesome technology to serve the world better?\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Matt O'Brien**<br/>*Data Scientist, UCSF Neurology*](../images/people/mattob.jpeg)\n",
      "\n",
      "This course filled a gap I couldn't find anywhere else&mdash;there really is no other source where I could learn from a 'code first' perspective. This means you can prod, poke, and cajole these networks in different ways, and see how they respond. You can quickly feel an intuitive perspective growing as you explore.\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Yannet Interian**<br/>*Assistant Professor of Analytics, University of San Francisco*](../images/people/yannet.jpg)\n",
      "\n",
      "I teach machine learning in a master’s degree program. After this course, I cannot ignore the new developments in deep learning&mdash;I will devote one third of my machine learning course to the subject. Also, I now have the tools to apply deep learning models to real world problems. Some of the best features of this course are the well-documented ipython notebooks containing the tricks needed to be a proficient deep learning practitioner. Overall, I was very impressed with this course.\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Dennis Sakva**<br/>*Energy sector analyst at Dragon Capital*](../images/people/dsakva.png)\n",
      "\n",
      "Jeremy, your class is absolutely fantastic. I've been pitching it to all my ML friends. The best description of CNNs and RNNs out there. Your Excel spreadsheet on embeddings was an 'aha' moment for me. You're an amazing educator. Thanks!\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Sara Hooker**<br/>*Data Scientist, Udemy*](../images/people/sara.png)\n",
      "\n",
      "This is a fantastic hands on learning experience. Like many data professionals outside of academia I found deep learning to be intimidating and opaque. This class changed that and empowered me to make deep learning part of the toolkit I use at Udemy. While there are a lot of resources available online about the theoretical underpinnings of deep learning this is the only course I have found that guides students through the implementation of fundamental deep learning frameworks. \n",
      "<br /> <br />\n",
      "There were three things that stood out to me that made this class special: 1) you will start coding right away and see the power of neural networks in lesson one, 2) Jeremy spends a lot of the course demystifying the subject, and in the process empowers anyone to get started in the field of deep learning, 3) many ‘tricks’ on how to optimize your architecture are passed down in rapid sequence. Save yourself a lot of time by watching this course, it will take you many more hours of trial and error to learn the same content by yourself.\n",
      "\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Janardhan Shetty**<br/>*Senior Big Data Engineer at Salesforce*](../images/people/shetty.jpg)\n",
      "\n",
      "Sometimes I feared whether I would be able to solve any deep learning problems, as all the research papers I read were very mathy beyond reach of simple intuitive terms. But Jeremy and Rachel (Course Professors) believe in the theory of 'Simple is Powerful', by virtue of which anyone who takes this course will be able to confidently understand the simple techniques behind the 'magic' Deep Learning.\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Dario Fanucchi**<br/>*Co-founder and CTO at Isazi Consulting*](../images/people/dario.png)\n",
      "\n",
      "Running a company is extremely time intensive, so I was a weary of taking on the commitment of the course. It was definitely worth it, though. It smashed my preconceptions about the technological obstructions to doing deep learning, and showed again and again examples where just a small subset of the training data and just a few epochs of training on standard GPU hardware could get most of the way towards a really good model\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Taro-Shigenori Chiba**<br/>*Organizer of the SF Deep Learning Study Group*](../images/people/taro.png)\n",
      "\n",
      "It can take years to develop the necessary skills and knowledge for Deep Learning, especially without the support of mentors and peers. Not only did Jeremy teach us the most valuable methods and practices, he provided us with an invaluable community and environment. The course exceeded my expectations and showed me first hand how both Deep Learning and ourselves could change the world for better.\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Robin Kraft (@robinkraft)**<br/>*Product Manager at Planet Labs (Satellites)*](../images/people/robink.jpeg)\n",
      "\n",
      "It was very empowering to be able to start training a model within minutes downloading the Jupyter notebooks. Jeremy and Rachel were excellent instructors and the content was high quality and enlightening. It was very cool to be able to read blogposts about the latest Deep Learning research and actually be able to understand it. I was surprised to be able to match academic results from just 2 years ago with pretty simple architectures.\n",
      "\n",
      ":::\n",
      "\n",
      "::: {layout=\"[30,70]\"}\n",
      "\n",
      "![**Nichol Bradford**<br/>*Executive Director of Transformative Tech Lab at Sofia University*](../images/people/nichol.jpg)\n",
      "\n",
      "I'm a CEO, not a coder, so the idea that I'd be able to create a GPU deep learning server in the cloud meant learning a lot of new things&mdash;but with all the help available on the wiki and from the instructors and community on the forum I did it!  Jeremy Howard is an incredible instructor and is able to make what might seem like a difficult subject completely accessible. In addition, he believes in deep learning for wide audiences so has developed a practical, experience based class.  I really enjoyed the classes, and used the videos to watch them all twice. The course covered cutting edge topics, and I now feel comfortable with deep learning concepts and can engage effectively in technical discussions with my data science team.\n",
      "\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in ym:\n",
    "    print(f\"\"\"::: {{layout=\"[30,70]\"}}\n",
    "\n",
    "![**{l['author']}**<br/>*{l['position']}*](../{l['avatar']})\n",
    "\n",
    "{l['text']}\n",
    "\n",
    ":::\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d6528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


=== File: ./www/social.png ===

=== File: ./Lessons/lesson11.qmd ===
---
title: "5: Python开源项目复线与应用"
---

TBD

=== File: ./Lessons/lesson10.qmd ===
---
title: "6: 基于大语言模型的自动化流程构建"
---

TBD

=== File: ./Lessons/byl.qmd ===
---
title: "博弈论与机制设计"
---

::: titlepage
**博弈论与机制设计**

吕博涵整理

[lyubh.cn](lyubh.cn){.uri}

2025-07-11
:::

# 博弈论基础

## 什么是博弈？

一个博弈主要由三个要素构成 ：

1.  **参与人 (Players)**：博弈的决策主体。

2.  **规则
    (Rules)**：规定了参与人可以采取的行动、行动的顺序以及他们拥有的信息。

3.  **收益/支付
    (Payoffs)**：当所有参与人做出决策后，每个参与人得到的效用或回报。

## 正规形式博弈 (Normal Form Game)

正规形式博弈，也称为策略形式博弈，通常用于描述参与人同时做出决策（或者等同于同时做出）的博弈。

特点：非合作 (Non-cooperative)，即参与人不能进行有约束力的承诺或协议。

::: example
**例子 1** (囚徒困境 (Prisoner's Dilemma)).
*两名囚徒被分开审讯，他们可以选择合作（C: 告诉同伙/保持沉默）或背叛（D:
告诉警察）。效用表示为 (囚徒1的效用,
囚徒2的效用)。负数越大表示惩罚越重。*

*假设效用如下 (注释中的C,D含义与笔记相反，这里采用笔记中的C:坦白,
D:抵赖，但为了与经典囚徒困境含义一致，我们设C=抵赖，D=坦白)：*

笔记中的原始效用 (C:坦白, D:抵赖)： 

*$$\begin{array}{c|cc}
    \text{参与人1 / 参与人2} & \text{C (坦白)} & \text{D (抵赖)} \\
    \hline
    \text{C (坦白)} & -3,-3 & -1,-4 \\
    \text{D (抵赖)} & -4,-1 & 0,0 \\
\end{array}$$*

*在这个例子中，\"坦白\" 是每个参与人的占优策略。\"均选择坦白\" (-3,-3)
是纳什均衡。*

如果我们将 C 定义为"合作/沉默"，D
定义为"背叛/坦白"，一个更经典的支付矩阵可能是： 

*$$\begin{array}{c|cc}
    \text{参与人1 / 参与人2} & \text{C (合作/沉默)} & \text{D (背叛/坦白)} \\
    \hline
    \text{C (合作/沉默)} & -1,-1 & -10,0 \\
    \text{D (背叛/坦白)} & 0,-10 & -5,-5 \\
\end{array}$$*

*在这个经典版本中，(D,D) 即 (-5,-5) 是纳什均衡，尽管 (C,C) 即 (-1,-1)
对双方都更好。*

*笔记中的 (-3,-3) 对应经典版 (D,D)，(0,0) 对应经典版 (C,C)。*
:::

### 策略的优劣势

-   **严格占优策略 (Strictly Dominant Strategy)**：一个策略 $\hat{a}_i$
    对参与人 $i$
    来说是严格占优策略，如果无论其他参与人选择什么策略，选择 $\hat{a}_i$
    的收益总是严格大于选择任何其他策略 $a_i \neq \hat{a}_i$ 的收益。

    Formal: $\Pi^i(\hat{a}_i, a_{-i}) > \Pi^i(a_i, a_{-i})$ for all
    $a_i \neq \hat{a}_i$ and for all $a_{-i} \in A_{-i}$.

-   **弱占优策略 (Weakly Dominant Strategy)**：一个策略 $\hat{a}_i$
    对参与人 $i$ 来说是弱占优策略，如果无论其他参与人选择什么策略，选择
    $\hat{a}_i$ 的收益总是大于或等于选择任何其他策略
    $a_i \neq \hat{a}_i$
    的收益，并且至少在一种其他参与人的策略组合下，选择 $\hat{a}_i$
    的收益严格更高。

    Formal: $\Pi^i(\hat{a}_i, a_{-i}) \ge \Pi^i(a_i, a_{-i})$ for all
    $a_i \neq \hat{a}_i$ and for all $a_{-i} \in A_{-i}$, and
    $\exists a_{-i}' \in A_{-i}$ such that
    $\Pi^i(\hat{a}_i, a_{-i}') > \Pi^i(a_i, a_{-i}')$.

-   **严格劣策略 (Strictly Dominated Strategy)**：一个策略 $a_i'$
    是严格劣策略，如果存在另一个策略 $a_i$ 使得无论其他参与人选择什么，
    $a_i$ 的收益总是严格大于 $a_i'$ 的收益。

-   **弱劣策略 (Weakly Dominated Strategy)**：一个策略 $a_i'$
    是弱劣策略，如果存在另一个策略 $a_i$ 使得无论其他参与人选择什么，
    $a_i$ 的收益总是大于或等于 $a_i'$
    的收益，并且至少在一种其他参与人的策略组合下，$a_i$ 的收益严格更高。

**重要结论**：在寻找纳什均衡时，可以忽略严格劣策略。弱劣策略的剔除需要更谨慎。

### 纳什均衡 (Nash Equilibrium)

::: definition
**定义 2** (纳什均衡). *在一个 $n$ 人正规形式博弈中，策略组合
$(a_1^*, a_2^*, \dots, a_n^*)$ 是一个纳什均衡，如果对于每一个参与人
$i \in \{1, 2, \dots, n\}$，策略 $a_i^*$ 是在给定其他参与人选择
$a_{-i}^* = (a_1^*, \dots, a_{i-1}^*, a_{i+1}^*, \dots, a_n^*)$
的情况下的最优反应 (best
response)。也就是说，没有任何一个参与人有动机单方面改变自己的策略。*

*Formal: 对于任意 $i \in \{1, 2, \dots, n\}$，以及任意
$a_i \in A_i$，都有：$\Pi^i(a_i^*, a_{-i}^*) \ge \Pi^i(a_i, a_{-i}^*)$，此时称
$a_i^*$ 是对 $a_{-i}^*$ 的最优反应。*
:::

纳什均衡不一定是唯一的。

::: example
**例子 3** (协调博弈 (Coordination Game)). *$$\begin{array}{c|cc}
     & \text{A} & \text{B} \\
    \hline
    \text{A} & 2,2 & 0,0 \\
    \text{B} & 0,0 & 1,1 \\
\end{array}$$ 在这个博弈中，(A,A) 和 (B,B) 都是纯策略纳什均衡。*
:::

## 混合策略 (Mixed Strategies)

当纯策略纳什均衡不存在，或者参与人希望让对手难以预测自己的行为时，会使用混合策略。混合策略是参与人在其可用纯策略集合上选择的一个概率分布。

::: example
**例子 4** (剪刀石头布 (Rock-Paper-Scissors)). *效用矩阵如下 (赢+1,
输-1, 平0)： $$\begin{array}{c|ccc}
    \text{1 / 2} & \text{石头(R)} & \text{布(P)} & \text{剪刀(S)} \\
    \hline
    \text{石头(R)} & 0,0 & -1,1 & 1,-1 \\
    \text{布(P)} & 1,-1 & 0,0 & -1,1 \\
    \text{剪刀(S)} & -1,1 & 1,-1 & 0,0 \\
\end{array}$$ 这个博弈没有纯策略纳什均衡。
唯一的混合策略纳什均衡是双方都以 $(1/3, 1/3, 1/3)$
的概率随机选择石头、布、剪刀。此时，双方的期望支付都是0。*
:::

**混合策略的条件**：当一个参与人愿意混合使用某些纯策略时，这些纯策略在给定对手策略的情况下，必须能给该参与人带来相同的期望效用。否则，他会选择期望效用最高的那个纯策略。

::: example
**例子 5** (混合策略求解). *考虑以下博弈，参与人1选择A或B的概率为 $p$ 和
$1-p$；参与人2选择A或B的概率为 $q$ 和 $1-q$。 $$\begin{array}{c|cc}
    \text{1 / 2} & \text{A ($q$)} & \text{B ($1-q$)} \\
    \hline
    \text{A ($p$)} & 1,1 & 0,0 \\
    \text{B ($1-p$)} & 0,0 & 2,2 \\
\end{array}$$ 对于参与人1： 期望效用
$\Pi^1(A) = q \cdot 1 + (1-q) \cdot 0 = q$ 期望效用
$\Pi^1(B) = q \cdot 0 + (1-q) \cdot 2 = 2-2q$ 若要混合A和B，则
$\Pi^1(A) = \Pi^1(B) \implies q = 2-2q \implies 3q=2 \implies q=2/3$。
同理，对于参与人2可解得 $p=2/3$。 因此，一个混合策略纳什均衡是双方都以
$2/3$ 的概率选择A，以 $1/3$ 的概率选择B。 ((A,A), (B,B)
也是纯策略纳什均衡)。*

*笔记中 的另一个例子： $$\begin{array}{c|ccc}
    \text{1 / 2} & \text{D} & \text{E} & \text{F} \\
    \hline
    \text{A} & 1,1 & 0,0 & 0,0 \\
    \text{B} & 0,0 & 2,2 & 0,0 \\
    \text{C} & 0,0 & 0,0 & 3,3 \\
\end{array}$$ 纯策略纳什均衡有 (A,D), (B,E), (C,F)。
考虑参与人1混合A和B，参与人2混合D和E。假设参与人1选择A的概率为$p_A$，B为$p_B$
($p_C=0$)；参与人2选择D的概率为$q_D$，E为$q_E$ ($q_F=0$)。
$\Pi^1(A) = q_D \cdot 1 + q_E \cdot 0 = q_D$
$\Pi^1(B) = q_D \cdot 0 + q_E \cdot 2 = 2q_E$ 若混合A,B，则
$q_D = 2q_E$。因为 $q_D+q_E=1$，所以 $q_D=2/3, q_E=1/3$。
同理，对参与人2： $\Pi^2(D) = p_A \cdot 1 + p_B \cdot 0 = p_A$
$\Pi^2(E) = p_A \cdot 0 + p_B \cdot 2 = 2p_B$ 若混合D,E，则
$p_A = 2p_B$。因为 $p_A+p_B=1$，所以 $p_A=2/3, p_B=1/3$。
此时，参与人1选择A的期望效用为 $2/3$，选择B的期望效用也为
$2/3$。选择C的期望效用为0。由于 $2/3 \ge 0$，参与人1不会切换到C。
参与人2同理。 所以一个混合策略纳什均衡是
$a_1 = (2/3, 1/3, 0)$，$a_2 = (2/3, 1/3, 0)$。*
:::

## 扩展形式博弈 (Extensive Form Game / Game Tree)

扩展形式博弈用博弈树来表示，它明确了参与人的行动顺序、每个决策节点的信息以及最终的支付。

::: example
**例子 6** (抢劫者博弈). *参与人1 (受害者) 可以选择给钱 (-100) 或不给钱
(0)。 如果给钱，博弈结束，支付为 (-100, +100)。 如果不给钱，参与人2
(抢劫者) 可以选择走开 (Walk away) 或杀人 (Kill)。 若走开，支付为 (0,0)。
若杀人，支付为 $(-\infty, -1000)$
(这里的-1000对抢劫者来说也是极差的结果，表示被捕等)。 博弈树表示：*

-   *节点1 (参与人1决策):*

    -   *动作: 给\$100 $\rightarrow$ 支付: $(-100, +100)$*

    -   *动作: 给\$0 $\rightarrow$ 进入节点2 (参与人2决策)*

        -   *动作: 走开 $\rightarrow$ 支付: $(0,0)$*

        -   *动作: 杀人 $\rightarrow$ 支付: $(-\infty, -1000)$*
:::

### 子博弈 (Subgame)

::: definition
**定义 7** (子博弈).
*一个扩展形式博弈的子博弈是原博弈的一部分，它满足：*

1.  *它从一个单独的决策节点开始。*

2.  *它包含该节点之后的所有决策节点和终端节点。*

3.  *如果一个决策节点属于子博弈，那么该节点所在信息集中的所有其他节点也必须属于该子博弈
    (即不能打断信息集)。*
:::

**信息集 (Information
Set)**：如果一个参与人在某个决策节点做决策时，不知道自己确切处于该信息集中的哪一个具体节点，那么这些节点构成一个信息集。

### 子博弈完美纳什均衡 (Subgame Perfect Nash Equilibrium - SPE)

::: definition
**定义 8** (SPE).
*一个策略组合是子博弈完美纳什均衡，如果它在整个博弈中构成纳什均衡，并且在每一个子博弈中也都构成纳什均衡。*
:::

SPE通过**逆向归纳法 (Backward Induction)**
求解，从最后一个子博弈开始分析，逐步向前推导。SPE排除了包含"不可信威胁
(incredible threat)"的纳什均衡。

在抢劫者博弈中 ： 子博弈：从参与人2的决策节点开始。

-   在该子博弈中，参与人2比较"走开"(0) 和"杀人"(-1000)
    的效用。理性选择是"走开"。

现在回到参与人1的决策：

-   如果给\$100，效用为 -100。

-   如果不给\$0，预料到参与人2会"走开"，则效用为 0。

参与人1比较 -100 和 0，选择不给\$0。
因此，SPE是：参与人1选择"不给\$0"；参与人2的策略是"如果参与人1不给钱，则选择走开"。支付为
(0,0)。 笔记中提到的一个非SPE的纳什均衡是：$a_1$=给\$100, $a_2$=杀人
(如果参与人1不给钱)。这是纳什均衡，因为给定$a_2$，参与人1给钱是最佳选择；给定$a_1$，参与人2无论做什么都无所谓，因为其决策节点不会到达。但"杀人"是不可信威胁。

::: example
**例子 9** (蜈蚣博弈 (Centipede Game)). *两个参与人轮流选择"继续
(K)"或"停止
(S)"。每次"继续"都会使总金额增加，但将决策权交给对方；选择"停止"则拿走当前总金额的较大部分。
例如：*

-   *1开始: (S) $\rightarrow$ (1,1) ; (K) $\rightarrow$ 轮到2*

-   *2决策: (S) $\rightarrow$ (0,3) ; (K) $\rightarrow$ 轮到1*

-   *1决策: (S) $\rightarrow$ (2,2) ; (K) $\rightarrow$ 轮到2*

-   *2决策: (S) $\rightarrow$ (1,4) ; (K) $\rightarrow$ \...*

-   *\...*

-   *假设最后一步是100轮后，2决策: (S) $\rightarrow$ (99,99) ; (K)
    $\rightarrow$ (98,101)*

-   *101轮后，1决策: (S) $\rightarrow$ (100,100)*

*通过逆向归纳法，SPE是参与人1在第一个决策节点就选择"停止 (S)"，获得支付
(1,1)。*
:::

::: example
**例子 10** (鲁宾斯坦交替出价谈判模型 (Rubinstein Alternating Offer
Bargaining Game)).
*两个参与人轮流提议如何分割一块大小为1的"馅饼"。馅饼会随时间"融化"，即存在时间折扣因子
$\delta \in [0,1]$。*

-   *Period 1 (参与人1提议): 提议 $x_1$ 给自己，则 $1-x_1$
    给参与人2。参与人2选择接受或拒绝。*

    -   *接受: 支付 $(x_1, 1-x_1)$*

    -   *拒绝: 进入Period 2，总馅饼大小变为 $\delta$*

-   *Period 2 (参与人2提议): 提议 $x_2$ 给自己，则 $\delta-x_2$
    给参与人1。(注意这里 $x_2$ 是从当前总额 $\delta$ 中分的)
    更常见的表示是，参与人2提议自己分得 $y_2 \in [0,1]$
    的比例，则其收益为 $y_2 \delta$，参与人1收益为
    $(1-y_2)\delta$。我们采用笔记中的方式。 参与人2提议自己分得份额
    $s_2$，则参与人1得到 $\delta-s_2$。参与人1接受或拒绝。*

    -   *接受: 支付 $(\delta-s_2, s_2)$*

    -   *拒绝: 进入Period 3，总馅饼大小变为 $\delta^2$*

-   *Period 3 (参与人1提议): \...*

*笔记中的 $x_i$ 指的是当期提议者给自己留下的份额。 假设
$T$期结束，如果到 $T$ 期还未达成协议，则双方支付为0。*
:::

逆向归纳 (以3期为例，笔记中 $x_i$ 为提议者提议给自己的份额)：

-   Period 3 (参与人1提议): 参与人2会接受任何 $1-x_3 \ge 0$
    (因为拒绝则得到0)。所以参与人1会提议 $x_3=1$ (即自己拿走所有剩余的
    $\delta^2$)。参与人2得到0。此时对参与人1的价值是 $\delta^2$。

-   Period 2 (参与人2提议，总额为 $\delta$):
    参与人1会接受如果自己得到的份额
    $x_1' \ge \delta \cdot (\text{参与人1在第3期得到的份额的折现值})$。
    笔记中符号为：参与人2提议自己拿 $x_2$，给参与人1的份额是
    $\delta-x_2$ (注意这里不是比例，而是绝对值)。参与人1接受条件是
    $\delta-x_2 \ge \delta \cdot \delta^2 = \delta^3$
    (这是参与人1在第三期得到的$\delta^2$再折现到第二期)。不对，应该是参与人1在第3期提议时，他会得到整个当时的饼
    $\delta^2$。所以，参与人2在第2期提议时，要使得参与人1接受，必须给参与人1的份额至少等于参与人1在第3期能得到的份额的折现值，即
    $\delta \cdot (\text{Player 1's payoff if P3 is reached})$. 按笔记
    $x_3=1$ (指P1拿走P3的全部饼 $\delta^2$)。 P2: P2提议自己拿 $x_2$
    (实际是比例，这里符号有点乱)。P2能给P1的最少是 $x_3 \cdot \delta$
    (这是符号错误，应为P1在P3能拿到的$\delta^2$的价值折现到P2)。
    我们按标准模型和笔记结果来： P3: P1提议，P1得 $\delta^2$, P2得 $0$.
    P2: P2提议。P1的拒绝选项是在P3得到 $\delta^2$ (折现到P2是
    $\delta \cdot \delta^2 = \delta^3$ -- 这种理解是错误的)。
    正确理解：如果P2拒绝P1在P3的提议，P2得到0。所以P1在P3提议
    $(x_3=1, 1-x_3=0)$，P1得 $\delta^2$, P2得 $0$。 P2:
    P2提议。P1知道如果拒绝，P1在P3会得到
    $\delta^2$。所以P1在P2接受的条件是自己得到的份额
    $\ge \delta \cdot (\text{P1在P3的份额}) = \delta \cdot \delta^2 = \delta^3$。
    这是错误的。 P1在P3得到的是当时的全部 $\delta^2$
    (未折现的，因为那是P3的支付)。P2提议时，P1比较的是P2给他的 和
    他在P3能得到的(折现到P2)。所以P1要求
    $\text{share from P2} \ge \delta \cdot \delta^2$。

按笔记的 $x_i$ 定义 (当期提议者提议留给自己的份额)： Period 3 (P1提议,
总额 $\delta^2$): P1提议 $x_3=\delta^2$, P2得0。P1接受。 Period 2
(P2提议, 总额 $\delta$): P1若拒绝，则在P3得到
$\delta \cdot \delta^2 = \delta^3$
(折现到P1时刻)。不对，P1在P3得到的是$\delta^2$，折现到P2时刻是
$\delta \cdot (\text{P1在P3得到的份额}) = \delta \cdot \delta^2$。这还是不对。

让我们严格按照笔记的最终公式和逻辑 ： $x_i$ 是第 $i$
期提议者提议给自己的份额 (相对于当前总额)。 Period 3 (P1提议, 总价值
$\delta^2$): P2会接受任何 $1-x_3 \ge 0$ (指分配比例)。P1提议 $x_3=1$
(P1拿全部)。P1得 $\delta^2 \cdot 1 = \delta^2$。P2得 $0$。 Period 2
(P2提议, 总价值 $\delta$): P1若拒绝，P1在P3得到的份额是 $\delta^2$
(这是P3的价值)。P1在P2时，P3的价值折现为 $\delta \cdot \delta^2$
(P1在P3得到的份额 $\times$ P3的价值)。 P1在P2接受条件是：
$1-x_2 \ge \delta x_3^*$ (其中 $x_3^*$
是P1在P3能确保得到的份额，这里是1)。 即
$1-x_2 \ge \delta \cdot 1 \implies (1-x_2)\delta \ge \delta^2 \cdot 1$
(这是P1在P2得到的实际价值)。 P2会使得P1刚好接受，即
$(1-x_2)\delta = \delta^2 \cdot 1 \implies 1-x_2 = \delta \implies x_2 = 1-\delta$
(P2提议自己拿 $1-\delta$ 的比例)。 P2的收益是
$(1-\delta)\delta = \delta-\delta^2$。P1的收益是
$\delta \cdot \delta = \delta^2$。 Period 1 (P1提议, 总价值 $1$):
P2若拒绝，P2在P2能得到的份额是 $(1-\delta)\delta = \delta-\delta^2$。
P1提议自己拿 $x_1$。P2接受条件是
$1-x_1 \ge \delta \cdot (\text{P2在P2得到的份额的比例 } x_2^*) = \delta (1-\delta)$
(这是P2在P2得到的实际价值 $\delta-\delta^2$ 折现到P1)。 即
$1-x_1 \ge \delta(1-\delta) = \delta-\delta^2$。 P1会使得P2刚好接受，即
$x_1 = 1 - (\delta-\delta^2) = 1-\delta+\delta^2$。 P1的收益是
$1-\delta+\delta^2$。P2的收益是 $\delta-\delta^2$。

当 $T \to +\infty$:
$x_1 = 1 - \delta + \delta^2 - \delta^3 + \dots = \frac{1}{1+\delta}$
$1-x_1 = \frac{\delta}{1+\delta}$ 参与人1 (先出价者) 得到
$\frac{1}{1+\delta}$，参与人2得到
$\frac{\delta}{1+\delta}$。存在先动优势。

**稳态均衡 (Stationary Equilibrium)**:
假设参与人在相似情境下做出的决策相同。即 $x_1 = x_3 = x_5 = \dots = x^*$
(P1的提议)。$x_2 = x_4 = \dots = \hat{x}$ (P2的提议)。 P1提议
$x^*$。P2接受条件: $1-x^* \ge \delta \hat{x}$
(P2在下一期自己提议能得到的折现值)。P1会给P2刚好使其接受:
$1-x^* = \delta \hat{x}$ (1) P2提议 $\hat{x}$。P1接受条件:
$1-\hat{x} \ge \delta x^*$
(P1在下一期自己提议能得到的折现值)。P2会给P1刚好使其接受:
$1-\hat{x} = \delta x^*$ (2) 联立 (1) (2): $\hat{x} = 1-\delta x^*$
$1-x^* = \delta (1-\delta x^*) = \delta - \delta^2 x^*$
$1-\delta = x^*(1-\delta^2) = x^*(1-\delta)(1+\delta)$
$x^* = \frac{1}{1+\delta}$
$\hat{x} = 1 - \frac{\delta}{1+\delta} = \frac{1}{1+\delta}$
所以，P1在P1提议自己拿 $\frac{1}{1+\delta}$，P2接受。P2会得到
$1-x^* = \frac{\delta}{1+\delta}$。

# 重复博弈 (Repeated Games)

当同一个策略型博弈 (称为阶段博弈 stage game)
被参与人重复进行多次时，就构成了重复博弈。参与人可以根据博弈的历史来选择当前阶段的行动。

## 有限重复博弈

如果阶段博弈有唯一的纳什均衡，那么有限次重复该博弈的唯一子博弈完美纳什均衡
(SPE) 是在每个阶段都进行该纳什均衡。

::: example
**例子 11** (有限重复囚徒困境). *阶段博弈 (H=合作/沉默,
L=背叛/坦白，这里L是支配策略): $$\begin{array}{c|cc}
     & \text{H} & \text{L} \\
    \hline
    \text{H} & 3,3 & -1,4 \\
    \text{L} & 4,-1 & 0,0 \\ % 笔记中此处为0,0，经典为1,1或类似值
\end{array}$$ 阶段博弈的唯一纳什均衡是 (L,L)。 如果重复2期： Period 2:
无论Period 1发生什么，Period 2本身是一个独立的博弈，唯一纳什均衡是
(L,L)。 Period 1: 参与人预料到Period 2的结果是 (L,L)，所以在Period
1也会选择 (L,L)。 结论：对于N期重复，唯一的SPE是每期都选择 (L,L)。*
:::

::: example
**例子 12** (有限重复协调博弈). *阶段博弈: $$\begin{array}{c|cc}
     & \text{A} & \text{B} \\
    \hline
    \text{A} & 1,1 & 0,0 \\
    \text{B} & 0,0 & 2,2 \\
\end{array}$$ 阶段博弈有多个纳什均衡: (A,A), (B,B) 和一个混合策略均衡。
如果重复2期 : 可以构建这样的SPE： 策略1:*

-   *Period 1: 选择B。*

-   *Period 2: 如果Period
    1是(B,B)，则选择(B,B)；否则（如出现(B,A)或(A,B)或(A,A)），则选择(A,A)
    (或者某个惩罚性的均衡)。*

*如果双方都遵循这个策略，那么在Period 1选择B，得到(2,2)；Period
2继续(B,B)，得到(2,2)。总效用较高。 如果一方在Period
1偏离（比如从B到A，而对方选B），则Period 1得到(0,0)，Period
2会进入(A,A)得到(1,1)。
这种依赖历史的策略可以支持非阶段博弈纳什均衡的合作结果。 笔记中提到策略
:*

-   *Period 2: 若Period 1是(A,A)或(A,B)或(B,A)，则Period
    2选(A,A)。若Period 1是(B,B)，则Period 2选(B,B)。*

-   *此时Period 1: 若选(B,B)，则总收益是 $2+2=4$。若选(A,A)，则总收益是
    $1+1=2$。
    因此，(B,B)在所有时期是一个SPE。同理，(A,A)在所有时期也是一个SPE。*
:::

## 无限重复博弈

当博弈无限期重复，或者参与人不知道博弈何时结束（以一定概率持续），逆向归纳法不再适用。
引入**折扣因子 (Discount Factor)** $\delta \in [0,1]$。$\delta$
表示未来收益相对于当前收益的价值。如果每期收益为 $u_t$，则总贴现收益为
$\sum_{t=0}^{\infty} \delta^t u_t$。

::: example
**例子 13** (无限重复囚徒困境). *阶段博弈同上，纳什均衡 (L,L) 支付
(0,0)。合作 (H,H) 支付 (3,3)。 **冷酷触发策略 (Grim Trigger
Strategy)**:*

1.  *在第一期选择 H (合作)。*

2.  *在之后的每一期：如果历史上所有时期双方都选择了 H，则继续选择 H。*

3.  *否则 (即一旦有任何一方选择了 L)，则永远选择 L (背叛)。*

*分析该策略是否构成SPE (双方都采用此策略)：*

-   ***一直合作 (H)**: 收益
    $= 3 + 3\delta + 3\delta^2 + \dots = \frac{3}{1-\delta}$。*

-   ***在第一期背叛 (L)** (假设对方合作): 收益
    $= 4 + 0\delta + 0\delta^2 + \dots = 4$
    (因为对方从第二期开始会永远选L)。*

-   ***在第 $k$ 期背叛 (L)** (假设对方合作到 $k-1$ 期): 收益
    $= 3 + 3\delta + \dots + 3\delta^{k-1} + 4\delta^k + 0\delta^{k+1} + \dots$*

*为了使"一直合作"成为纳什均衡，需要满足不偏离的条件，即一直合作的收益
$\ge$ 任何时候偏离的收益。 最强的偏离动机在第一期：
$\frac{3}{1-\delta} \ge 4 \implies 3 \ge 4(1-\delta) \implies 3 \ge 4 - 4\delta \implies 4\delta \ge 1 \implies \delta \ge \frac{1}{4}$。
如果折扣因子足够大 ($\delta \ge 1/4$)，那么 (H,H)
可以作为无限重复博弈的一个SPE的结果。*
:::

### Folk Theorem (无名氏定理)

无名氏定理（有多个版本）指出，在无限重复博弈中，如果参与人有足够的耐心
(即
$\delta \to 1$)，那么任何满足以下两个条件的平均支付都可以作为SPE的结果：

1.  **可行性 (Feasible)**:
    该支付向量必须是阶段博弈所有可能纯策略结果（包括通过协调随机化设备达成的）的凸组合
    (convex hull)。即在下图中的可行支付区域内。

2.  **个体理性 (Individually Rational)**:
    每个参与人的平均支付必须至少等于其在阶段博弈中的最小最大支付 (minmax
    payoff)。 最小最大支付
    $\underline{u}_i = \min_{a_{-i}} \max_{a_i} \Pi^i(a_i, a_{-i})$。这是参与人
    $i$ 在最坏情况下（其他参与人试图最小化 $i$
    的支付）能够保证自己得到的最低支付。

图示 :

-   可行支付集 (Feasible Payoffs): 由阶段博弈结果 (0,0), (-1,4), (4,-1),
    (3,3) 构成的凸包。

-   个体理性支付集 (Individually Rational Payoffs):
    $u_i \ge \underline{u}_i$。在本例囚徒困境中，每个参与人的最小最大支付是0（当对方选择L时，自己选择L能得到0；若对方选择H，自己选择L能得到4，所以最差是0）。所以个体理性区域是
    $u_1 \ge 0, u_2 \ge 0$。

Folk定理表明，当 $\delta$
足够接近1时，可行且个体理性的任何支付组合（例如
(3,3)）都可以通过某种SPE来实现。

# 转换成本 (Switching Costs)

当参与人在不同策略或行动之间转换时，需要付出的成本。这种成本会影响动态博弈和重复博弈的结果。
假设博弈进行 $N$ 个时期，每个时期长度为 $\Delta = 1/N$。转换成本为
$\epsilon$。

## 协调博弈中的转换成本

阶段博弈: $$\begin{array}{c|cc}
     & \text{A} & \text{B} \\
    \hline
    \text{A} & 0,0 & 0,0 \\
    \text{B} & 0,0 & 2,2 \\
\end{array}$$ (笔记中的例子是 (A,A)=(1,1), (A,B)=(0,0), (B,A)=(0,0),
(B,B)=(2,2)，这里用的是讲义Page1的 (A,A)=(0,0)版本)
考虑最后几个时期。如果 $\epsilon$ 足够大，例如 $2\Delta > \epsilon$
(或者笔记中 $\epsilon > 2\Delta$？应为转换的收益 $2\Delta$ 小于转换成本
$\epsilon$ 时不转换)。 笔记的逻辑 :

-   当 $t_1 \approx \frac{\epsilon}{2\Delta}$ (表示离结束还有 $t_1$
    个时期，其中 $2\Delta$ 是选择B相对于A的每期额外收益)。 当剩余时期数
    $k$ 使得 $k \cdot (b-a) < \epsilon$ (其中 $b,a$
    为选B,A的收益)，则不会从A转到B。

-   笔记中分析: 当 $t\Delta > \epsilon$ 时，若选A则会变至B。 Unique SPE
    outcome 是 (B,B) 在所有时期。

更一般的协调博弈形式 : $$\begin{array}{c|cc}
     & \text{A} & \text{B} \\
    \hline
    \text{A} & a,a & d,c \\
    \text{B} & c,d & b,b \\
\end{array}$$ 假设 $b>a>c, b>d$。 (B,B) 成为 unique SPE outcome 的条件是
$b-d > a-c$ (B风险占优A，risk dominate)。

## 囚徒困境中的转换成本

阶段博弈 (C=合作, D=背叛): $$\begin{array}{c|cc}
     & \text{C} & \text{D} \\
    \hline
    \text{C} & 3,3 & -1,4 \\
    \text{D} & 4,-1 & 1,1 \\ % 笔记中为1,1
\end{array}$$

-   Incentive to cheat (从C到D，当对方C): $4-3=1$。

-   Incentive to punish
    (从C到D，当对方从C变为D，自己也从C变为D来惩罚或止损): $1-(-1)=2$
    (如果对方变D，我从C变D，收益从-1变1)。

笔记分析 : 如果 $t_2 \approx \frac{\epsilon}{\Delta}$
(转换收益1相较于成本$\epsilon$)，$t_1 \approx \frac{\epsilon}{2\Delta}$。
SPE #1: Play D all the period. SPE #2: 考虑(C,C)能否维持。 (C,D)
$\rightarrow$ (D,D) (D,C) $\rightarrow$ (D,D) (D,D) $\rightarrow$ (D,D)
(C,C) $\rightarrow$ (C,C) 如果转换成本足够影响决策，(C,C)也可能成为SPE。

另一个囚徒困境例子 : $$\begin{array}{c|cc}
     & \text{C} & \text{D} \\
    \hline
    \text{C} & 3,3 & 0,5 \\
    \text{D} & 5,0 & 1,1 \\
\end{array}$$

-   Incentive to cheat: $5-3=2$.

-   Incentive to punish: $1-0=1$.

这里，欺骗的诱惑 (2) 大于惩罚的收益 (1)。 $t_1: (C,C) \rightarrow (D,C)$
(如果一方单方面从C转D) $t_2: C \rightarrow D$ Unique SPE 是
(D,D)\...(D,D)。

一般形式 : $$\begin{array}{c|cc}
     & \text{C} & \text{D} \\
    \hline
    \text{C} & a,a & d,c \\
    \text{D} & c,d & b,b \\
\end{array}$$ 假设 $c>a>b>d$ (经典囚徒困境)。 Incentive to cheat: $c-a$.
Incentive to punish (如果对方从C到D，自己从C到D，收益从d到b): $b-d$.

-   If $c-a > b-d$: unique SPE is (D,D),\...,(D,D).

-   If $c-a < b-d$: (C,C),\...,(C,C) can be one of the SPEs.

## 无限期转换成本

折扣因子 $\delta = e^{-r\Delta} \approx 1-\Delta$ (当 $r=1$,
$\Delta \to 0$)。 参与人 $i$ 的总贴现效用:
$$U_i = \sum_{t=0}^{\infty} \delta^t u_i(a^t) - \sum_{t=0}^{\infty} \delta^t \epsilon \cdot I_i(a^{t+1} \neq a^t)$$
其中 $I_i(a^{t+1} \neq a^t)$ 是一个指示函数，当 $t+1$ 期的行动与 $t$
期不同时为1，否则为0。 **个体理性支付 (Rational Payoff)**
$\underline{V_i}$: 参与人 $i$ 的纯策略最大最小支付 (pure-strategy
maxmin)。这是在没有转换成本的情况下，参与人 $i$ 能确保得到的最低支付。

# 不完全信息博弈 (Incomplete Information Games)

## 基本概念

### 不对称信息 (Asymmetric Information)

在许多现实的经济互动中，参与者拥有的信息往往是不对称的。不完全信息博弈指的是至少有一方参与者不完全了解其他参与者某些特征（例如，支付函数、类型、策略空间等）的博弈。不对称信息是其中的一种情况，即不同的参与者拥有不同的私有信息
。

一个经典的例子是保险市场中的逆向选择（adverse selection）问题
。例如，健康保险公司可能不了解每个客户的真实健康状况，而客户对此则有更充分的信息。高风险客户更倾向于购买保险，这可能导致保险公司面临亏损。

### 贝叶斯均衡 (Bayesian Equilibrium)

在不完全信息博弈中，我们需要一种新的均衡概念来描述参与者的理性行为。贝叶斯纳什均衡（Bayesian
Nash Equilibrium,
BNE）是纳什均衡在不完全信息博弈中的扩展。在贝叶斯纳什均衡中，每个参与者（根据其私有信息，即"类型"）选择一个最大化其期望效用的策略，同时考虑到其他参与者类型的概率分布以及其他参与者的策略。

一个策略组合构成贝叶斯纳什均衡，如果没有任何一个类型的参与者可以通过单方面改变其策略来提高其期望效用，给定其他参与者的策略和关于其他参与者类型的信念。

## 示例分析

### 例1：不确定博弈的选择

考虑一个博弈，其中参与者1（Player 1）不知道正在进行的是博弈1（Game
1）还是博弈2（Game 2）。他只知道博弈1发生的概率为 $q$，博弈2发生的概率为
$1-q$ 。参与者2（Player 2）则完全知晓当前进行的是哪个博弈 。

博弈的支付矩阵如下（(P1收益, P2收益)）：

::: center
   博弈1 (Game 1) - 概率 $q$          
--------------------------- ------- -------
       P1 $\setminus$ P2         C       D
               A               (3,1)   (0,0)
               B               (0,1)   (3,0)

   博弈2 (Game 2) - 概率 $1-q$          
----------------------------- ------- -------
        P1 $\setminus$ P2          C       D
                A                (3,1)   (0,2)
                B                (0,1)   (3,2)
:::

假设参与者2的策略是：在博弈1中选择C，在博弈2中选择D 。
现在我们分析参与者1的最优策略。 如果参与者1选择行动A，他的期望收益为：
$$\Pi_1(A) = q \cdot (\text{A在博弈1中对C的收益}) + (1-q) \cdot (\text{A在博弈2中对D的收益})$$
$$\Pi_1(A) = q \cdot 3 + (1-q) \cdot 0 = 3q \quad \text{}$$
如果参与者1选择行动B，他的期望收益为：
$$\Pi_1(B) = q \cdot (\text{B在博弈1中对C的收益}) + (1-q) \cdot (\text{B在博弈2中对D的收益})$$
$$\Pi_1(B) = q \cdot 0 + (1-q) \cdot 3 = 3 - 3q \quad \text{}$$

参与者1会选择使其期望收益最大化的行动：

-   如果 $\Pi_1(A) \ge \Pi_1(B)$，即
    $3q \ge 3-3q \implies 6q \ge 3 \implies q \ge \frac{1}{2}$，则参与者1选择A
    ($a_1=A$) 。

-   如果 $\Pi_1(A) \le \Pi_1(B)$，即
    $3q \le 3-3q \implies 6q \le 3 \implies q \le \frac{1}{2}$，则参与者1选择B
    ($a_1=B$) 。

当参与者1根据这个规则选择其行动，而参与者2的策略（在博弈1中选C，在博弈2中选D）是对参与者1（可能的）行动的最佳回应时（需要验证这一点），我们就可能找到了一个贝叶斯均衡
。

参与者2的策略 $a_2(\text{type})$
是一种依赖于其类型（即他所知道的博弈是Game 1还是Game
2）的策略，称为类型条件策略 (type contingent strategy) 。
具体来说，参与者2的类型 contingent strategy 是：

-   如果是博弈1 ($t_2=1$)，选择 $a_2(1)=C$ 。

-   如果是博弈2 ($t_2=2$)，选择 $a_2(2)=D$ 。

一个贝叶斯均衡要求每个参与者的类型条件策略都是对其他参与者策略的最佳回应
。

### 例2：不同博弈的纯策略均衡

考虑另一组博弈支付矩阵 ：

::: center
   博弈1 (Game 1) - 概率 $q$          
--------------------------- ------- -------
       P1 $\setminus$ P2         C       D
               A               (3,1)   (0,0)
               B               (0,0)   (3,1)

   博弈2 (Game 2) - 概率 $1-q$          
----------------------------- ------- -------
        P1 $\setminus$ P2          C       D
                A                (3,1)   (0,2)
                B                (0,2)   (3,1)
:::

假设与前例相同，参与者1不知道是哪个博弈，参与者2知道 。

我们来寻找纯策略贝叶斯均衡：

1.  假设参与者1选择 $a_1=A$ 。
    那么参与者2在博弈1中的最佳回应是选择C（收益1 vs 0），即 $a_2(1)=C$
    。 参与者2在博弈2中的最佳回应是选择D（收益2 vs 1），即 $a_2(2)=D$ 。
    给定参与者2的策略 $(a_2(1)=C, a_2(2)=D)$，参与者1选择A的前提是
    $\Pi_1(A) \ge \Pi_1(B)$：
    $\Pi_1(A | (C,D)) = q \cdot 3 + (1-q) \cdot 0 = 3q$
    $\Pi_1(B | (C,D)) = q \cdot 0 + (1-q) \cdot 3 = 3(1-q)$
    所以，$3q \ge 3(1-q) \implies 3q \ge 3-3q \implies 6q \ge 3 \implies q \ge \frac{1}{2}$
    。 因此，如果 $q \ge \frac{1}{2}$，则 $(a_1=A; a_2(1)=C, a_2(2)=D)$
    是一个纯策略贝叶斯均衡。

2.  假设参与者1选择 $a_1=B$ 。
    那么参与者2在博弈1中的最佳回应是选择D（收益1 vs 0），即 $a_2(1)=D$
    。 参与者2在博弈2中的最佳回应是选择C（收益2 vs 1），即 $a_2(2)=C$ 。
    给定参与者2的策略 $(a_2(1)=D, a_2(2)=C)$，参与者1选择B的前提是
    $\Pi_1(B) \ge \Pi_1(A)$：
    $\Pi_1(B | (D,C)) = q \cdot 3 + (1-q) \cdot 0 = 3q$
    $\Pi_1(A | (D,C)) = q \cdot 0 + (1-q) \cdot 3 = 3(1-q)$
    所以，$3q \ge 3(1-q) \implies q \ge \frac{1}{2}$ 。 因此，如果
    $q \ge \frac{1}{2}$，则 $(a_1=B; a_2(1)=D, a_2(2)=C)$
    是另一个纯策略贝叶斯均衡。

笔记中提到，当 $q < \frac{1}{2}$ 时没有纯策略均衡
。这是因为在上述分析中，如果
$q < \frac{1}{2}$，则参与者1会偏离假设的策略。

# 拍卖理论 (Auction Theory)

拍卖是将物品或服务出售给出价最高者（或在某些情况下，出价最低者）的一种市场机制。拍卖与明码标价、协商定价等是并列的定价方式
。

## 拍卖类型简介

有多种不同规则的拍卖形式，常见的有：

-   **英式拍卖 (English Auction)**: 也称为公开增价拍卖（open bid
    auctions）。价格从低到高逐渐上升，竞标者不断出更高的价格，直到没有人愿意出更高价格为止，最后出价最高者以其出价赢得拍卖品
    。

-   **荷兰式拍卖 (Dutch Auction)**:
    价格从高到低逐渐下降，第一个表示接受当前价格的竞标者赢得拍卖品并支付该价格
    。

-   **第一价格密封投标拍卖 (First-Price Sealed-Bid Auction)**:
    每个竞标者独立地、秘密地提交一个报价（bid），报价最高者赢得拍卖品，并支付其自己报出的价格
    。

-   **第二价格密封投标拍卖 (Second-Price Sealed-Bid Auction)**:
    也称为维克里拍卖（Vickrey
    Auction）。每个竞标者独立地、秘密地提交一个报价，报价最高者赢得拍卖品，但仅需支付所有报价中第二高的价格
    。

-   **全支付拍卖 (All-Pay Auction)**:
    所有竞标者都支付他们报出的价格，无论他们是否赢得拍卖品。出价最高者获得拍卖品
    。

## 第二价格密封投标拍卖 (Second-Price Sealed-Bid Auction)

这是一种重要的拍卖形式，因为它具有一些优良的性质。

假设有两个竞标者参与拍卖。每个竞标者 $i$ 对拍卖品有一个私有价值（private
value）$v_i$，即该物品对其的真实价值 。竞标者 $i$ 对自己的 $v_i$
清楚，但不知道其他竞标者的私有价值。我们假设 $v_i$
是从某个概率分布中抽取的。竞标者 $i$ 提交的报价为 $b_i$ 。

### 弱占优策略：$b_i = v_i$

在第二价格密封投标拍卖中，对于每个竞标者 $i$ 来说，诚实报价（即报价
$b_i$ 等于其私有价值 $v_i$）是一个弱占优策略 (weakly dominant strategy)
。这意味着，无论其他竞标者如何报价，竞标者 $i$ 选择 $b_i=v_i$
所带来的收益至少不比选择任何其他报价 $b_i \neq v_i$
差，并且在某些情况下会更好。

**证明思路** (以竞标者1为例，其价值为 $v_1$，报价为
$b_1$，另一竞标者的最高报价为 $b_{max_2}$) ： 我们需要比较 $b_1=v_1$ 和
$b_1 \neq v_1$ 的情况。设 $b_2$
为竞标者2的报价（如果是多个其他竞标者，则 $b_2$
代表其他竞标者中的最高报价）。竞标者1的盈余（surplus）是
$v_1 - \text{支付价格}$（如果赢得拍卖）或0（如果未赢得）。

1.  **情况1: $b_1 > v_1$ (过高报价)**

    -   若 $b_2 \ge b_1$：竞标者1未赢得。如果他报 $v_1$ 且
        $b_2 \ge v_1$，结果相同。如果他报 $v_1$ 且
        $b_1 > b_2 \ge v_1$，报 $v_1$ 也未赢得（或赢得并支付 $b_2$，但
        $v_1-b_2 \le 0$）。

    -   若 $v_1 < b_2 < b_1$：竞标者1赢得并支付 $b_2$。盈余为
        $v_1 - b_2 < 0$。如果他报 $v_1$，他将不会赢得（因为
        $b_2 > v_1$），盈余为0。这种情况下，报 $b_1 > v_1$ 更差 。

    -   若 $b_2 \le v_1$ (且 $b_2 < b_1$): 竞标者1赢得并支付
        $b_2$。盈余为 $v_1 - b_2 \ge 0$。如果他报
        $v_1$，他也会赢得并支付 $b_2$（因为 $v_1 \ge b_2$），盈余相同。

2.  **情况2: $b_1 < v_1$ (过低报价)**

    -   若 $b_2 \le b_1$：竞标者1赢得并支付 $b_2$。盈余为
        $v_1 - b_2 > 0$。如果他报 $v_1$，他也会赢得并支付 $b_2$（因为
        $v_1 > b_1 \ge b_2$），盈余相同。

    -   若 $b_1 < b_2 < v_1$：竞标者1未赢得，盈余为0。如果他报
        $v_1$，他将会赢得并支付 $b_2$。盈余为
        $v_1 - b_2 > 0$。这种情况下，报 $b_1 < v_1$ 更差 。

    -   若 $b_2 \ge v_1$ (且 $b_2 > b_1$): 竞标者1未赢得。如果他报
        $v_1$，他也不会赢得（因为 $b_2 \ge v_1$），盈余相同。

总结来说，与诚实报价 $b_1=v_1$ 相比：

-   报 $b_1 > v_1$：在某些情况下（当 $v_1 < b_2 < b_1$
    时），会导致负盈余，而诚实报价会得到0盈余。在其他情况下，结果与诚实报价相同或不劣于诚实报价。

-   报 $b_1 < v_1$：在某些情况下（当 $b_1 < b_2 < v_1$
    时），会错失获得正盈余的机会，而诚实报价可以获得正盈余。在其他情况下，结果与诚实报价相同或不劣于诚实报价。

因此，$b_i=v_i$ 是一个弱占优策略。

## 推广至N个竞标者

上述结论可以推广到N个竞标者的情况。对于任何竞标者 $i$，将其余 $N-1$
个竞标者中的最高报价视为 $b_{max_{-i}}$ (相当于前述证明中的
$b_2$)，同样的逻辑仍然适用。因此，$b_i=v_i$ 仍然是弱占优策略 。

### 贝叶斯均衡

在第二价格拍卖中，由于诚实报价 $b_i=v_i$ 是每个参与者的弱占优策略，因此
$(b_1=v_1, b_2=v_2, \dots, b_N=v_N)$ 构成一个贝叶斯纳什均衡
。这是一种类型条件策略，其中每个竞标者的报价直接反映其私有价值。

### 示例

假设有两个竞标者，其私有价值分别为 $v_1=10, v_2=12$ 。
根据贝叶斯均衡策略： $b_1 = v_1 = 10$ $b_2 = v_2 = 12$ 竞标者2的报价最高
($b_2=12$)，因此他赢得拍卖品。他需要支付的价格是第二高的报价，即
$b_1=10$。竞标者2的盈余是 $v_2 - b_1 = 12 - 10 = 2$。竞标者1的盈余是0。
如果 $b_1=10, b_2 > 10$（例如 $b_2=12$），竞标者2赢得并支付 \$10 。 如果
$b_1<10$（例如 $b_1=8$）且 $b_2 \ge 10$（例如
$b_2=12$），竞标者2赢得并支付 $b_1=8$ 。 (注：笔记原文中
$b_1<1, b_2 \ge 1$, bidder 2 pays $b_1$。这部分应为如果 $b_2$
是最高价，他支付第二高价。如果 $b_1$ 是第二高价且 $b_1 < b_2$，他支付
$b_1$。) 更精确地说，如果 $b_i$ 是最高出价， $b_j$ 是严格次高出价，则
$i$ 获胜并支付
$b_j$。如果多个出价者出相同的最高价，通常有某种打破僵局的规则。

### 与英式拍卖的等价性

对于卖方而言，英式拍卖和第二价格密封投标拍卖在一定条件下（例如，私有价值模型）通常能产生相同的期望收益
。在英式拍卖中，竞标者会持续出价直到价格略高于其第二高竞争对手愿意支付的价格（或者说，直到价格达到其自身价值，而倒数第二个退出的竞标者其价值决定了最终价格）。

## 第一价格密封投标拍卖 (First-Price Sealed-Bid Auction)

在这种拍卖中，出价最高的竞标者赢得拍卖品，并支付其自己报出的价格。与第二价格拍卖不同，这里不存在诚实报价的占优策略。竞标者面临一个权衡：出价越高，赢得拍卖的概率越大，但一旦赢得，利润（价值 -
价格）就越小。

### 模型设定 (双人，价值服从\[0,1\]均匀分布)

假设有两个竞标者（$i=1,2$）。每个竞标者的私有价值 $v_i$ 是独立同分布
(IID) 的，服从区间 $[0,1]$ 上的均匀分布 ($v_i \sim U[0,1]$)
。每个竞标者只知道自己的 $v_i$。
我们寻找对称的贝叶斯纳什均衡，其中每个竞标者都使用相同的报价函数（bidding
function）$b_i = B(v_i)$，该函数将其私有价值 $v_i$ 映射到一个报价 $b_i$
。我们通常假设 $B(v)$ 是一个严格递增函数 ($B'(v)>0$) 。

对于竞标者1，他假设竞标者2使用报价函数 $B(v_2)$。竞标者1选择其报价 $b_1$
以最大化其期望盈余 $\Pi_1(b_1, v_1)$：
$$\Pi_1(b_1, v_1) = P(\text{竞标者1赢得拍卖}) \cdot (v_1 - b_1)$$
竞标者1赢得拍卖的条件是 $b_1 > B(v_2)$。由于 $B(\cdot)$
单调递增，这等价于 $B^{-1}(b_1) > v_2$ 。 因为
$v_2 \sim U[0,1]$，$P(v_2 < B^{-1}(b_1)) = B^{-1}(b_1)$ （假设
$B^{-1}(b_1)$ 仍在 $[0,1]$ 区间内）。 所以，期望盈余为：
$$\Pi_1(b_1, v_1) = B^{-1}(b_1) \cdot (v_1 - b_1) \quad \text{}$$
竞标者1选择 $b_1$ 来最大化上式。为了找到最优的 $b_1$，我们对 $b_1$
求导并令其等于0。
根据反函数求导法则，$\frac{d B^{-1}(y)}{dy} = \frac{1}{B'(B^{-1}(y))}$
。
$$\frac{\partial \Pi_1}{\partial b_1} = \frac{d B^{-1}(b_1)}{d b_1} (v_1 - b_1) + B^{-1}(b_1) (-1) = 0$$
$$\frac{1}{B'(B^{-1}(b_1))} (v_1 - b_1) - B^{-1}(b_1) = 0 \quad \text{}$$
在对称均衡中，$b_1 = B(v_1)$，所以 $B^{-1}(b_1) = v_1$。将此代入上式：
$$\frac{1}{B'(v_1)} (v_1 - B(v_1)) - v_1 = 0 \quad \text{}$$
$$v_1 - B(v_1) - v_1 B'(v_1) = 0$$
$$v_1 B'(v_1) + B(v_1) = v_1 \quad \text{}$$ 这是一个一阶线性常微分方程
(ODE)。注意到左边是 $(v_1 B(v_1))'$ 的展开式：
$$(v_1 B(v_1))' = v_1 \quad \text{}$$ 两边对 $v_1$ 积分：
$$v_1 B(v_1) = \int v_1 dv_1 = \frac{1}{2} v_1^2 + C \quad \text{}$$
其中 $C$ 是积分常数。 为了确定
$C$，我们考虑边界条件。如果一个竞标者的价值为0
($v_1=0$)，他最优的出价应该是0 ($B(0)=0$)。代入上式：
$0 \cdot B(0) = \frac{1}{2} \cdot 0^2 + C \implies C = 0$ 。
所以，均衡报价函数为： $$B(v_1) = \frac{1}{2} v_1 \quad \text{}$$
这意味着，在有两个竞标者且其价值服从 $[0,1]$
均匀分布的第一价格拍卖中，均衡策略是报出自己价值的一半。

### 模型设定 (N人，一般分布)

现在考虑有 $N$ 个竞标者。每个竞标者的私有价值 $v_i$ 独立同分布于区间
$[\underline{v}, \overline{v}]$，其累积分布函数 (CDF) 为
$F(v_i)$，概率密度函数 (PDF) 为 $f(v_i)$ 。
我们仍然寻找对称的贝叶斯纳什均衡报价函数 $b_i = B(v_i)$，并假设
$B'(v)>0$ 。

假设竞标者1认为其他 $N-1$ 个竞标者都使用报价函数
$B(\cdot)$。竞标者1选择自己的报价
$b_1$（或者等价地，选择一个"伪装"的价值 $\tilde{v}_1$ 使得
$b_1 = B(\tilde{v}_1)$）来最大化其期望效用。 竞标者1的期望效用
$\Pi_1(b_1, v_1)$ 或 $\tilde{\Pi}_1(\tilde{v}_1, v_1)$ (用 $\tilde{v}_1$
表示其选择的报价水平对应的价值) 为：
$$\tilde{\Pi}_1(\tilde{v}_1, v_1) = P(\text{竞标者1以 } B(\tilde{v}_1) \text{ 获胜}) \cdot (v_1 - B(\tilde{v}_1)) \quad \text{}$$
竞标者1获胜的条件是他的报价 $B(\tilde{v}_1)$ 高于其他所有 $N-1$
个人的报价。即 $B(\tilde{v}_1) > B(v_j)$ 对所有 $j \neq 1$。由于
$B(\cdot)$ 单调递增，这等价于 $\tilde{v}_1 > v_j$ 对所有 $j \neq 1$。
其他 $N-1$ 个竞标者的价值都小于 $\tilde{v}_1$ 的概率为
$(F(\tilde{v}_1))^{N-1}$。 所以，
$$\tilde{\Pi}_1(\tilde{v}_1, v_1) = (F(\tilde{v}_1))^{N-1} (v_1 - B(\tilde{v}_1)) \quad \text{}$$
为了使 $\tilde{v}_1 = v_1$ 是最优选择（即诚实地使用自己的价值通过
$B(\cdot)$ 函数来报价），我们需要最大化
$\tilde{\Pi}_1(\tilde{v}_1, v_1)$ 关于 $\tilde{v}_1$，然后代入
$\tilde{v}_1=v_1$。 一阶条件
$\frac{\partial \tilde{\Pi}_1(\tilde{v}_1, v_1)}{\partial \tilde{v}_1} = 0$
在 $\tilde{v}_1 = v_1$ 时成立 ：
$$\frac{\partial \tilde{\Pi}_1}{\partial \tilde{v}_1} = \frac{d(F(\tilde{v}_1))^{N-1}}{d\tilde{v}_1} (v_1 - B(\tilde{v}_1)) + (F(\tilde{v}_1))^{N-1} (-B'(\tilde{v}_1))$$
在 $\tilde{v}_1=v_1$ 时：
$$((F(v_1))^{N-1})' (v_1 - B(v_1)) - (F(v_1))^{N-1} B'(v_1) = 0 \quad \text{}$$
这是一个关于 $B(v_1)$ 的微分方程。
$((F(v_1))^{N-1})' = (N-1)(F(v_1))^{N-2}f(v_1)$。 所以，
$(N-1)(F(v_1))^{N-2}f(v_1) (v_1 - B(v_1)) - (F(v_1))^{N-1} B'(v_1) = 0$。

可以证明（通过求解这个微分方程，通常设定一个边界条件如
$B(\underline{v})=\underline{v}$，如果 $\underline{v}$
是可能的最低价值且在此价值下不会有交易发生，或者更一般地，具有价值
$\underline{v}$ 的竞标者期望收益为0），均衡报价函数为：
$$B(v) = v - \frac{\int_{\underline{v}}^{v} (F(t))^{N-1} dt}{(F(v))^{N-1}} \quad \text{}$$
其中 $t$ 是积分哑变量。 这个表达式也等于
$B(v) = \mathbb{E}[Y_1 | Y_1 < v]$, 其中 $Y_1$ 是其他 $N-1$
个竞标者价值中的最高值。

我们可以验证 $B'(v) > 0$。对上述 $B(v)$ 求导 ：
$$(F(v))^{N-1} (v - B(v)) = \int_{\underline{v}}^{v} (F(t))^{N-1} dt$$
两边对 $v$ 求导：
$$\frac{d(F(v))^{N-1}}{dv} (v - B(v)) + (F(v))^{N-1} (1 - B'(v)) = (F(v))^{N-1} \quad \text{}$$
$$(N-1)(F(v))^{N-2}f(v)(v-B(v)) + (F(v))^{N-1}(1-B'(v)) = (F(v))^{N-1}$$
$$(N-1)(F(v))^{N-2}f(v)(v-B(v)) - (F(v))^{N-1}B'(v) = 0$$
这与我们之前设定的一阶条件一致。从 $B(v)$ 的表达式可以推导出 $B'(v)$
通常为正。

### 另一种推导方法 (基于包络定理)

竞标者1的优化问题是选择 $\tilde{v}_1$ 来最大化
$\tilde{\Pi}_1(\tilde{v}_1, v_1) = (F(\tilde{v}_1))^{N-1} (v_1 - B(\tilde{v}_1))$。
在均衡中，竞标者会选择 $\tilde{v}_1 = v_1$。令
$\Pi_1^*(v_1) = \tilde{\Pi}_1(v_1, v_1)$
为竞标者1在均衡中的最优期望效用。 根据包络定理 (Envelope Theorem)：
$$\frac{d \Pi_1^*(v_1)}{d v_1} = \left. \frac{\partial \tilde{\Pi}_1(\tilde{v}_1, v_1)}{\partial v_1} \right|_{\tilde{v}_1=v_1} \quad \text{}$$
$$\frac{\partial \tilde{\Pi}_1(\tilde{v}_1, v_1)}{\partial v_1} = (F(\tilde{v}_1))^{N-1}$$
所以， $$\frac{d \Pi_1^*(v_1)}{d v_1} = (F(v_1))^{N-1} \quad \text{}$$
两边从 $\underline{v}$ 到 $v$ 积分：
$$\Pi_1^*(v) - \Pi_1^*(\underline{v}) = \int_{\underline{v}}^{v} (F(t))^{N-1} dt \quad \text{}$$
通常，拥有最低可能价值 $\underline{v}$ 的竞标者的期望效用为0，即
$\Pi_1^*(\underline{v})=0$（因为他们几乎不可能获胜，或者即使获胜，出价接近其价值，利润也为0）。
那么，$\Pi_1^*(v) = \int_{\underline{v}}^{v} (F(t))^{N-1} dt$。 又因为
$\Pi_1^*(v) = (F(v))^{N-1} (v - B(v))$，所以
$$(F(v))^{N-1} (v - B(v)) = \int_{\underline{v}}^{v} (F(t))^{N-1} dt$$
$$B(v) = v - \frac{\int_{\underline{v}}^{v} (F(t))^{N-1} dt}{(F(v))^{N-1}}$$
这与之前得到的结果一致。

# 保留价格 (Reserve Price)

卖方在拍卖中可以设定一个保留价格（或最低价格，minimum
price）$r$。如果最高出价低于保留价格
$r$，则物品不会售出（或者卖方自己"买回"物品）。

## 第一价格拍卖中的保留价格

在存在保留价格 $r$ 的情况下，竞标者的行为会受到影响。我们假设
$\underline{v} \le r \le \overline{v}$。 如果竞标者的价值
$v_i < r$，他通常不会出价高于
$r$，因为即使赢了也会亏损。一个关键的边界（或初始）条件是，对于一个价值恰好为
$r$ 的竞标者，他的最优出价是 $r$，即 $B(r)=r$ 。这意味着如果他的价值是
$r$，他愿意支付 $r$，并且期望利润为0。

基于 $B(r)=r$ 这个条件，我们可以修改上一节中 $B(v)$ 的推导。
$$(F(v))^{N-1} (v - B(v)) - (F(r))^{N-1} (r - B(r)) = \int_{r}^{v} (F(t))^{N-1} dt \quad \text{}$$
由于 $B(r)=r$，第二项为0。所以，
$$(F(v))^{N-1} (v - B(v)) = \int_{r}^{v} (F(t))^{N-1} dt$$
$$B(v) = v - \frac{\int_{r}^{v} (F(t))^{N-1} dt}{(F(v))^{N-1}} \quad \text{ for } v \ge r \quad \text{}$$
如果 $v < r$，竞标者通常会出价低于 $r$ (或者不出价，或者出价 $r$
但知道不会以低于 $r$ 的价格成交)。 从这个公式可以看出，如果保留价格 $r$
提高，那么对于任意 $v > r$，积分项 $\int_{r}^{v} (F(t))^{N-1} dt$
会减小，从而导致 $B(v)$ 增大
。即保留价格提高会使得合格的竞标者出价更高。

### 卖方期望收益与最优保留价格

卖方的期望收益 $R^1(r)$ 是所有竞标者价值都低于 $r$
时收益为0（或为卖方自己的保留效用），以及至少有一个竞标者价值高于 $r$
时，最高价值者支付其报价 $B(v_{(N)})$ 的期望值。 令 $v_{(N)}$ 为 $N$
个竞标者中的最高价值。其PDF为 $g_{(N)}(v) = N (F(v))^{N-1} f(v)$。
卖方的期望收益为：
$$R^1(r) = \int_{r}^{\overline{v}} B(v) N (F(v))^{N-1} f(v) dv \quad \text{}$$
代入 $B(v) = v - \frac{\int_{r}^{v} (F(t))^{N-1} dt}{(F(v))^{N-1}}$：
$$R^1(r) = \int_{r}^{\overline{v}} \left( v - \frac{\int_{r}^{v} (F(t))^{N-1} dt}{(F(v))^{N-1}} \right) N (F(v))^{N-1} f(v) dv$$
$$R^1(r) = \int_{r}^{\overline{v}} v N (F(v))^{N-1} f(v) dv - \int_{r}^{\overline{v}} \left( \int_{r}^{v} (F(t))^{N-1} dt \right) N f(v) dv \quad \text{}$$
通过对第二项进行分部积分或改变积分次序（Fubini定理），可以得到（如思考过程所示）：
$$\int_{r}^{\overline{v}} \left( \int_{r}^{v} (F(t))^{N-1} dt \right) N f(v) dv = \int_{r}^{\overline{v}} N (1-F(t)) (F(t))^{N-1} dt \quad \text{}$$
所以，
$$R^1(r) = \int_{r}^{\overline{v}} v N (F(v))^{N-1} f(v) dv - \int_{r}^{\overline{v}} N (1-F(v)) (F(v))^{N-1} dv$$
对 $R^1(r)$ 关于 $r$ 求导并令其等于0，可以找到最优保留价格 $r^*$。
$$\frac{dR^1(r)}{dr} = -r N (F(r))^{N-1} f(r) + N (1-F(r)) (F(r))^{N-1} \quad \text{(Leibniz rule and fundamental theorem of calculus)}$$
令 $\frac{dR^1(r)}{dr} = 0$：
$$-r N (F(r))^{N-1} f(r) + N (1-F(r)) (F(r))^{N-1} = 0$$ 假设
$(F(r))^{N-1} > 0$ (即 $r < \overline{v}$ and $F(r)>0$ for $N>1$),
$$-r f(r) + (1-F(r)) = 0$$ $$r - \frac{1-F(r)}{f(r)} = 0 \quad \text{}$$
这就是确定最优保留价格 $r^*$ 的条件，通常被称为Myerson's optimal reserve
price condition（对于满足某些正则性条件的分布）。这里的
$J(v) = v - \frac{1-F(v)}{f(v)}$ 是所谓的"虚拟价值"(virtual
valuation)。最优保留价格使得最低被接受的竞标者的虚拟价值为0。

如果卖方对物品自身有一个保留效用 $v_0$（即如果物品未售出，卖方获得 $v_0$
的效用），那么最优保留价格 $r^*$ 应该满足：
$$r^* - \frac{1-F(r^*)}{f(r^*)} = v_0 \quad \text{}$$
并且，实际设定的保留价格应该是 $\max(r^*, v_0)$。

**示例：垄断者问题** 当 $N=1$
时（只有一个竞标者，相当于垄断卖方直接向一个买方报价），$F(v)=v$（即
$v \sim U[0,1]$），$f(v)=1$。 保留价格 $r$
就是卖方设定的价格。买方只有当其价值 $v \ge r$ 时才会购买。 卖方收益
$R(r) = P(v \ge r) \cdot r = (1-F(r)) \cdot r = (1-r)r$ 。
$\frac{dR}{dr} = 1-2r = 0 \implies r = \frac{1}{2}$ 。 使用公式
$r - \frac{1-F(r)}{f(r)} = 0 \implies r - \frac{1-r}{1} = 0 \implies r - (1-r) = 0 \implies 2r-1=0 \implies r=\frac{1}{2}$。结果一致。
这与标准垄断定价问题中，如果需求函数为 $Q(p) = 1-F(p)$，边际成本为
$c$，则利润 $\Pi = p Q(p) - c Q(p)$。一阶条件 $\frac{d\Pi}{dp}=0$ 给出
$Q(p) + (p-c)Q'(p)=0 \implies Q(p) + (p-c)(-f(p))=0 \implies p-c = \frac{Q(p)}{f(p)} = \frac{1-F(p)}{f(p)}$。如果
$c=0$，则 $p = \frac{1-F(p)}{f(p)}$，即 $p - \frac{1-F(p)}{f(p)} = 0$ 。

## 第二价格拍卖中的保留价格

在第二价格拍卖中引入保留价格 $r$。规则是：最高出价者 $b_{(N)}$
获胜，当且仅当 $b_{(N)} \ge r$。如果获胜，支付价格为
$\max(r, b_{(N-1)})$，其中 $b_{(N-1)}$ 是第二高出价。如果所有出价都低于
$r$，则物品不售出。由于诚实报价 $b_i=v_i$
仍然是（截断的）占优策略（即如果 $v_i < r$
则不报或报一个不会赢的价格，如果 $v_i \ge r$ 则报
$v_i$），所以分析较为直接。

卖方的期望收益 $R^2(r)$ 由两部分构成：

1.  最高价值 $v_{(N)} \ge r$ 且次高价值 $v_{(N-1)} \ge r$：卖方收到
    $v_{(N-1)}$。

2.  最高价值 $v_{(N)} \ge r$ 且次高价值
    $v_{(N-1)} < r$（或者只有一个竞标者价值 $\ge r$）：卖方收到 $r$。

令 $f_k(v)$ 为第 $k$ 高阶统计量的概率密度函数。$f_2(v)$ （笔记中用
$f_2(v)$ 表示次高价值的PDF，更准确的写法是 $g_{(N-1)}(v)$）为：
$$g_{(N-1)}(v) = N(N-1) f(v) (1-F(v)) (F(v))^{N-2} \quad \text{}$$ (这里
$f_k(v)$ 在笔记 中符号 $f_2(v)$
指的是次高阶统计量的PDF，下标2表示第二高。$f_3(v)$ 同理表示第三高。)
卖方的期望收益可以写为 ：
$$R^2(r) = \int_{r}^{\overline{v}} v \cdot g_{(N-1)}(v \text{ is 2nd highest, and highest is also } \ge v) dv + r \cdot P(\text{exactly one bidder has value } \ge r)$$
更规范的表达是：
$$R^2(r) = \int_{r}^{\overline{v}} v \cdot N(N-1)f(v)(1-F(v))(F(v))^{N-2} dv + r \cdot N (F(r))^{N-1} (1-F(r)) \quad \text{}$$
这里 $N (F(r))^{N-1} (1-F(r))$ 是指最高价值高于 $r$，而其余 $N-1$
个价值都低于 $r$ 的概率（应为
$N(1-F(r))(F(r))^{N-1}$，即一个价值高于r，N-1个低于r的概率）。

对 $R^2(r)$ 关于 $r$ 求导并令其等于0：
$$\frac{dR^2(r)}{dr} = -r \cdot N(N-1)f(r)(1-F(r))(F(r))^{N-2} \quad (\text{from first term by Leibniz})$$
$$+ N(1-F(r))(F(r))^{N-1} \quad (\text{from product rule on } r \cdot N(F(r))^{N-1}(1-F(r)))$$
$$+ r N \left[ -(N-1)F(r)^{N-2}f(r)(1-F(r)) - F(r)^{N-1}f(r) \right] \quad (\text{this needs careful re-derivation from note})$$
根据笔记 中给出的 $\frac{dR}{dr}$ 的各项，化简后得到：
$$\frac{dR^2(r)}{dr} = N(1-F(r))(F(r))^{N-1} - r N f(r) (F(r))^{N-1}$$
$(N-1)(1-F(r))F^{N-2}(r)$ 与第一项求导的直接结果相符。中间的
$N(1-F(r))F^{N-1}(r)$ 来自对第二项
$r P(\text{one above r, N-1 below r})$ 中 $P(\cdot)$部分的求导乘以 $r$
的系数。细节推导见"思考过程"。) 令 $\frac{dR^2(r)}{dr} = 0$，且假设
$(F(r))^{N-1}>0$： $$(1-F(r)) - r f(r) = 0$$
$$r - \frac{1-F(r)}{f(r)} = 0 \quad \text{}$$
这与第一价格拍卖得到的最优保留价格条件完全相同
。这意味着在标准模型下，无论采用第一价格还是第二价格拍卖，最优保留价格的设定是相同的。

笔记中还给出了 $R^1(r) = R^2(r)$
的一个证明概要，即在设定了（相同的）最优保留价格 $r$
的情况下，第一价格拍卖和第二价格拍卖给卖方带来的期望收益是相同的
。这个证明通常依赖于分部积分和对期望收益表达式的巧妙变形。 例如，笔记 中
$R^2(r)$ 的表达式通过分部积分：
$$\int_{r}^{\overline{v}} v N(N-1)(1-F(v))F^{N-2}(v)f(v)dv = \int_{r}^{\overline{v}} v N(1-F(v)) d(F^{N-1}(v))$$
$$= \left[ vN(1-F(v))F^{N-1}(v) \right]_{r}^{\overline{v}} - \int_{r}^{\overline{v}} F^{N-1}(v) d(vN(1-F(v))) dv$$
$$= -rN(1-F(r))F^{N-1}(r) - \int_{r}^{\overline{v}} N F^{N-1}(v) [(1-F(v)) - vf(v)] dv$$
将此结果代回 $R^2(r)$ 的表达式，并与 $R^1(r)$
的表达式进行比较，可以证明二者相等。这实际上是收益等价定理在包含最优保留价格时的体现。

## 收益等价定理 (Revenue Equivalence Theorem)

收益等价定理是拍卖理论中的一个核心结论。

该定理指出，在一系列标准假设下，任何能产生相同结果（即总是将物品分配给具有最高价值的竞标者，并且具有最低可能价值的竞标者期望支付为零）的拍卖机制，都会给卖方带来相同的期望收益
。

标准假设通常包括 ：

1.  有 $N$ 个风险中性的竞标者。

2.  每个竞标者 $i$ 的私有价值 $v_i$ 是从同一概率分布（CDF $F(v)$, PDF
    $f(v)$）中独立抽取的，价值区间为 $[\underline{v}, \overline{v}]$。

3.  竞标者只关心自己的期望盈余（价值 - 支付）。

4.  拍卖机制确保物品总是分配给价值最高的竞标者（效率分配）。

5.  价值为 $\underline{v}$ 的竞标者的期望支付为0（或期望效用为0）。

## 竞标者期望效用与期望支付

考虑一个对称的、单调递增的均衡报价（或行为）策略。令 $\Pi(v)$
为一个类型为 $v$ 的竞标者的期望效用。 笔记中给出的一个设定是 ：
$\Pi_1(\tilde{v}_1, v_1) = P(\text{以类型 } \tilde{v}_1 \text{ 获胜}) v_1 - E[\text{支付} | \text{以类型 } \tilde{v}_1 \text{ 获胜并支付}]$
假设
$P(\text{以类型 } \tilde{v}_1 \text{ 获胜}) = (F(\tilde{v}_1))^{N-1}$
(这是当其他 $N-1$ 人价值都低于 $\tilde{v}_1$
时发生的，例如在第一价格或第二价格拍卖中，如果其他人按其价值的某个增函数出价)。
令 $T(\tilde{v}_1)$ 是当竞标者表现得像类型 $\tilde{v}_1$
并获胜时的期望支付。 则
$\Pi_1(\tilde{v}_1, v_1) = (F(\tilde{v}_1))^{N-1} v_1 - T(\tilde{v}_1)$
(这里的 $T(\tilde{v}_1)$ 是总期望支付，而非条件期望支付)。
(注：更标准的包络定理应用是
$\frac{d\Pi(v)}{dv} = P(\text{类型v获胜})$。笔记中的形式略有不同，但旨在得到支付函数。)

根据笔记 ，通过优化 $\tilde{v}_1$ 得到一阶条件，在 $\tilde{v}_1=v_1$
时：
$$\frac{\partial \Pi_1}{\partial \tilde{v}_1} = \frac{d(F(v_1))^{N-1}}{dv_1} v_1 - T'(v_1) = 0$$
(这里假设 $T(\tilde{v}_1)$ 是类型 $\tilde{v}_1$
的总期望支付函数，而不仅仅是获胜时的支付。) 所以，期望支付函数 $T(v_1)$
的导数为： $$T'(v_1) = v_1 \frac{d(F(v_1))^{N-1}}{dv_1} \quad \text{}$$
积分可得 $T(v_1)$：
$$T(v) = T(\underline{v}) + \int_{\underline{v}}^{v} t \frac{d(F(t))^{N-1}}{dt} dt \quad \text{}$$
令 $H(v) = \int_{\underline{v}}^{v} t ( (F(t))^{N-1} )' dt$。则
$T(v) = T(\underline{v}) + H(v)$ 。

## 卖方总期望收益

卖方的总期望收益 $R$ 是所有 $N$ 个竞标者的期望支付之和。由于对称性，
$$R = N \int_{\underline{v}}^{\overline{v}} T(v) f(v) dv \quad \text{}$$
$$R = N \int_{\underline{v}}^{\overline{v}} (T(\underline{v}) + H(v)) f(v) dv$$
$$R = N T(\underline{v}) \cdot \int_{\underline{v}}^{\overline{v}} f(v) dv + N \int_{\underline{v}}^{\overline{v}} H(v) f(v) dv$$
$$R = N T(\underline{v}) + N \int_{\underline{v}}^{\overline{v}} H(v) f(v) dv \quad \text{(因为 } \int f(v)dv = 1 \text{)} \quad \text{}$$

如果对于所有满足上述条件的拍卖机制，最低类型 $\underline{v}$
的竞标者的期望支付 $T(\underline{v})$
都相同（通常为0），那么这些拍卖机制给卖方带来的总期望收益 $R$ 也将相同
。 例如，在第一价格拍卖、第二价格拍卖和全支付拍卖中（在满足效率分配和
$T(\underline{v})=0$ 条件下），卖方的期望收益是相同的
。这就是收益等价定理的核心内容。

# 共同价值拍卖

在共同价值拍卖 (Common Value Auction)
中，拍卖品的实际价值对于所有竞拍者来说是相同的，但在竞拍时，他们并不知道这个确切的价值。每个竞拍者
$i$ 都会收到一个私人信号
$s_i$，该信号提供了关于这个共同价值的一些信息。竞拍者利用他们的信号来估计物品的价值并决定他们的出价。竞拍者面临的挑战是避免"赢家诅咒"
(Winner's
Curse)，即赢得拍卖可能意味着自己的信号过于乐观，从而导致支付过高的价格。

本讲义探讨第一价格和第二价格共同价值拍卖中的对称贝叶斯纳什均衡
(Symmetric Bayesian Nash Equilibrium)。我们假设竞拍者是风险中性
(risk-neutral) 的，并且旨在最大化其期望利润 (expected profit)。

## 第一价格共同价值拍卖

在第一价格密封投标拍卖 (First-Price Sealed-Bid Auction)
中，竞拍者同时提交他们的出价，而不知道其他人的出价。出价最高的竞拍者赢得物品，并支付其自己的出价金额。

### 情况一：两个竞拍者 (N=2)

让我们考虑一个有两个竞拍者的简单情景。

-   **信号 (Signals)**：每个竞拍者 $i \in \{1,2\}$ 收到一个信号
    $s_i$。假设信号是独立同分布 (IID) 的，服从 $[0,1]$ 上的均匀分布
    (Uniform Distribution)，即
    $s_i \sim \text{IID U}[0,1]$。累积分布函数 (CDF) 为
    $F(x)=x$，概率密度函数 (PDF) 为 $f(x)=1$ (对于 $x \in [0,1]$)。

-   **共同价值 (Common Value)**：物品的共同价值为 $V = s_1 + s_2$。

-   **竞标策略 (Bidding
    Strategy)**：假设竞拍者采用对称且严格递增的竞标函数 $B(s_i)$，其中
    $b_i = B(s_i)$ 是拥有信号 $s_i$ 的竞拍者 $i$ 的出价。

**竞拍者1的期望利润**： 假设竞拍者1的真实信号是
$s_1$，但他考虑如同其信号为 $\tilde{s}_1$ 那样出价，即提交出价
$B(\tilde{s}_1)$。如果竞拍者1的出价高于竞拍者2的出价，即
$B(\tilde{s}_1) > B(s_2)$，则竞拍者1获胜。由于 $B(\cdot)$
是递增的，这等价于 $\tilde{s}_1 > s_2$。 竞拍者1的期望利润 $\pi^1$ 为：
$$\pi^1(\tilde{s}_1, s_1) = P(\text{获胜}) \times [E(\text{价值} | s_1, \text{获胜}) - \text{支付价格}]$$
$$\pi^1(\tilde{s}_1, s_1) = P(s_2 < \tilde{s}_1) \times [E(s_1+s_2 | s_1, s_2 < \tilde{s}_1) - B(\tilde{s}_1)]$$
鉴于 $s_2 \sim U[0,1]$:

-   $P(s_2 < \tilde{s}_1) = F(\tilde{s}_1) = \tilde{s}_1$ (假设
    $\tilde{s}_1 \in [0,1]$)。

-   $E(s_2 | s_2 < \tilde{s}_1) = \frac{\tilde{s}_1}{2}$ (这是
    $U[0, \tilde{s}_1]$ 分布的均值)。

所以，期望利润函数为：
$$\pi^1(\tilde{s}_1, s_1) = \tilde{s}_1 \left( s_1 + \frac{\tilde{s}_1}{2} - B(\tilde{s}_1) \right)$$

**推导均衡竞标策略 $B(s_1)$**: 在对称贝叶斯纳什均衡中，每个竞拍者选择
$\tilde{s}_1 = s_1$ 以最大化其利润。我们可以使用包络定理 (Envelope
Theorem)。对于拥有信号 $s_1$ 的竞拍者1，其优化后的利润为
$\pi^*(s_1) = \pi^1(s_1, s_1)$。 根据包络定理：
$$\frac{d\pi^*(s_1)}{ds_1} = \left. \frac{\partial \pi^1(\tilde{s}_1, s_1)}{\partial s_1} \right|_{\tilde{s}_1=s_1}$$
从
$\pi^1(\tilde{s}_1, s_1) = \tilde{s}_1 s_1 + \frac{\tilde{s}_1^2}{2} - \tilde{s}_1 B(\tilde{s}_1)$，我们有
$\frac{\partial \pi^1(\tilde{s}_1, s_1)}{\partial s_1} = \tilde{s}_1$。
因此，当 $\tilde{s}_1=s_1$ 时： $$\frac{d\pi^*(s_1)}{ds_1} = s_1$$ 对
$s_1$ 积分，并假设 $\pi^*(0)=0$ (信号为0的竞拍者期望利润为零)：
$$\pi^*(s_1) = \int_0^{s_1} x dx = \frac{1}{2}s_1^2$$
我们也知道，根据定义，当 $\tilde{s}_1 = s_1$ 时：
$$\pi^*(s_1) = s_1 \left( s_1 + \frac{s_1}{2} - B(s_1) \right) = \frac{3}{2}s_1^2 - s_1 B(s_1)$$
令 $\pi^*(s_1)$ 的两个表达式相等：
$$\frac{3}{2}s_1^2 - s_1 B(s_1) = \frac{1}{2}s_1^2$$
$$s_1^2 = s_1 B(s_1)$$ 对于 $s_1 > 0$，这意味着 $B(s_1) = s_1$。
所以，对称均衡竞标策略是每个竞拍者出价等于其信号值。

*使用一阶条件 (FOC) 的替代推导*： 对 $\pi^1(\tilde{s}_1, s_1)$ 关于
$\tilde{s}_1$ 求导，并令 $\tilde{s}_1=s_1$ 且导数为0。
$$\frac{\partial \pi^1}{\partial \tilde{s}_1} = \left( s_1 + \frac{\tilde{s}_1}{2} - B(\tilde{s}_1) \right) + \tilde{s}_1 \left( \frac{1}{2} - B'(\tilde{s}_1) \right) = s_1 + \tilde{s}_1 - B(\tilde{s}_1) - \tilde{s}_1 B'(\tilde{s}_1)$$
令 $\tilde{s}_1=s_1$ 且 $\frac{\partial \pi^1}{\partial \tilde{s}_1}=0$:
$$2s_1 - B(s_1) - s_1 B'(s_1) = 0$$ 这是一个一阶线性微分方程：
$(s_1 B(s_1))' = 2s_1$。 积分得到
$s_1 B(s_1) = \int 2x dx = s_1^2 + C$。 使用边界条件 $B(0)=0$ (或者当
$s_1 \to 0$ 时 $B(s_1)$ 不为无穷大)，则 $C=0$。
所以，$s_1 B(s_1) = s_1^2$，这意味着对于 $s_1>0$，$B(s_1)=s_1$。

### 情况二：N 个竞拍者

现在，考虑 $N$ 个竞拍者。

-   **信号 (Signals)**：$s_i \in [\underline{s}, \bar{s}]$
    是独立同分布的，其累积分布函数为 $F(\cdot)$，概率密度函数为
    $f(\cdot)$。

-   **共同价值 (Common Value)**：$V = \sum_{i=1}^N s_i$。

-   **竞标策略 (Bidding Strategy)**：对称且递增的 $B(s_i)$。

**竞拍者1的期望利润**： 拥有信号 $s_1$ 的竞拍者1考虑出价
$B(\tilde{s}_1)$。如果 $B(\tilde{s}_1)$
是最高出价，则竞拍者1获胜，这意味着 $\tilde{s}_1$ 大于所有其他 $N-1$
个信号 $s_j$ ($j \neq 1$)。此事件的概率为
$P(\max_{j \neq 1} s_j < \tilde{s}_1) = [F(\tilde{s}_1)]^{N-1}$。
期望利润为：
$$\pi^1(\tilde{s}_1, s_1) = [F(\tilde{s}_1)]^{N-1} \left( E\left[\sum_{k=1}^N s_k \middle| s_1, \max_{j \neq 1} s_j < \tilde{s}_1 \right] - B(\tilde{s}_1) \right)$$
$$E\left[\sum_{k=1}^N s_k \middle| s_1, \max_{j \neq 1} s_j < \tilde{s}_1 \right] = s_1 + (N-1) E[s_j | s_j < \tilde{s}_1]$$
其中
$E[s_j | s_j < \tilde{s}_1] = \int_{\underline{s}}^{\tilde{s}_1} x \frac{f(x)}{F(\tilde{s}_1)} dx$。
所以， $$\begin{aligned}
\pi^1(\tilde{s}_1, s_1) &= [F(\tilde{s}_1)]^{N-1} \left( s_1 + (N-1) \frac{\int_{\underline{s}}^{\tilde{s}_1} x f(x) dx}{F(\tilde{s}_1)} - B(\tilde{s}_1) \right) \\
&= [F(\tilde{s}_1)]^{N-1} s_1 + (N-1)[F(\tilde{s}_1)]^{N-2} \int_{\underline{s}}^{\tilde{s}_1} x f(x) dx - [F(\tilde{s}_1)]^{N-1} B(\tilde{s}_1)
\end{aligned}$$

**推导均衡竞标策略 $B(s_1)$**: 令 $\pi^*(s_1) = \pi^1(s_1, s_1)$
为均衡利润。根据包络定理：
$$\frac{d\pi^*(s_1)}{ds_1} = \left. \frac{\partial \pi^1(\tilde{s}_1, s_1)}{\partial s_1} \right|_{\tilde{s}_1=s_1}$$
从 $\pi^1(\tilde{s}_1, s_1)$
的表达式可知，$\frac{\partial \pi^1}{\partial s_1} = [F(\tilde{s}_1)]^{N-1}$。
所以，当 $\tilde{s}_1=s_1$ 时：
$$\frac{d\pi^*(s_1)}{ds_1} = [F(s_1)]^{N-1}$$ 从 $\underline{s}$ 积分
(假设
$\pi^*(\underline{s})=0$，即具有最低可能信号的竞拍者的期望利润为零)：
$$\pi^*(s_1) = \int_{\underline{s}}^{s_1} [F(x)]^{N-1} dx$$
根据定义，均衡利润也为：
$$\pi^*(s_1) = [F(s_1)]^{N-1} s_1 + (N-1)[F(s_1)]^{N-2} \int_{\underline{s}}^{s_1} x f(x) dx - [F(s_1)]^{N-1} B(s_1)$$
令 $\pi^*(s_1)$ 的两个表达式相等，并解出 $B(s_1)$：
$$[F(s_1)]^{N-1} B(s_1) = [F(s_1)]^{N-1} s_1 + (N-1)[F(s_1)]^{N-2} \int_{\underline{s}}^{s_1} x f(x) dx - \int_{\underline{s}}^{s_1} [F(x)]^{N-1} dx$$
$$B(s_1) = s_1 + (N-1)\frac{\int_{\underline{s}}^{s_1} x f(x) dx}{F(s_1)} - \frac{\int_{\underline{s}}^{s_1} [F(x)]^{N-1} dx}{[F(s_1)]^{N-1}}$$
这是第一价格共同价值拍卖中的一般对称均衡竞标策略。

## 第二价格共同价值拍卖

在第二价格密封投标拍卖 (Second-Price Sealed-Bid
Auction)，也称为维克里拍卖 (Vickrey Auction)
中，竞拍者提交密封出价。出价最高的竞拍者获胜，但支付第二高出价的价格。

### 情况一：两个竞拍者 (N=2)，价值 $V=s_1+s_2$

-   **信号 (Signals)**：$s_i \sim \text{IID U}[0,1]$。

-   **共同价值 (Common Value)**：$V = s_1 + s_2$。

-   **声称的均衡策略 (Claimed Equilibrium Strategy)**：$B(s_i) = 2s_i$。

**验证 $B(s_i) = 2s_i$**： 假设竞拍者2采用策略
$B_2(s_2) = 2s_2$。拥有信号 $s_1$ 的竞拍者1选择出价 $b_1$
以最大化其期望利润。 如果
$b_1 > B_2(s_2) = 2s_2$，则竞拍者1获胜，这意味着 $s_2 < b_1/2$。
如果竞拍者1获胜，他支付 $B_2(s_2) = 2s_2$。他的利润为
$V - 2s_2 = (s_1+s_2) - 2s_2 = s_1-s_2$。 竞拍者1出价 $b_1$ 的期望利润为
(假设 $b_1/2 \le 1$，即 $b_1 \le 2$)：
$$E[\pi_1(b_1)] = \int_0^{b_1/2} (s_1 - s_2) f(s_2) ds_2$$ 由于
$s_2 \sim U[0,1]$，$f(s_2)=1$：
$$E[\pi_1(b_1)] = \int_0^{b_1/2} (s_1 - s_2) ds_2 = \left[ s_1 s_2 - \frac{s_2^2}{2} \right]_0^{b_1/2} = s_1 \frac{b_1}{2} - \frac{(b_1/2)^2}{2} = \frac{s_1 b_1}{2} - \frac{b_1^2}{8}$$
为了最大化关于 $b_1$ 的期望利润，我们取一阶条件：
$$\frac{\partial E[\pi_1(b_1)]}{\partial b_1} = \frac{s_1}{2} - \frac{2b_1}{8} = \frac{s_1}{2} - \frac{b_1}{4}$$
令其为零： $$\frac{s_1}{2} - \frac{b_1}{4} = 0 \implies b_1 = 2s_1$$
因此，如果竞拍者2出价 $2s_2$，竞拍者1的最佳反应是出价 $2s_1$。这证实了
$B(s_i)=2s_i$ 是一个对称贝叶斯纳什均衡策略。

### 情况二：两个竞拍者 (N=2)，一般价值函数 $V(s_1,s_2)$

假设一个一般的对称价值函数 $V(s_1,s_2) = V(s_2,s_1)$，且
$\frac{\partial V}{\partial s_i} > 0$。竞拍者使用对称、递增的竞标函数
$B(s_i)$。

**竞拍者1的期望利润**： 拥有信号 $s_1$ 的竞拍者1考虑提交对应于类型
$\tilde{s}_1$ 的出价，即 $B(\tilde{s}_1)$。竞拍者2出价 $B(s_2)$。 如果
$B(\tilde{s}_1) > B(s_2)$，或 $\tilde{s}_1 > s_2$ (因为 $B(\cdot)$
递增)，则竞拍者1获胜。如果获胜，他支付 $B(s_2)$。 期望利润
$\pi^1(\tilde{s}_1, s_1)$ 为：
$$\pi^1(\tilde{s}_1, s_1) = \int_{\underline{s}}^{\tilde{s}_1} [V(s_1, s_2) - B(s_2)] f(s_2) ds_2$$
注意：$s_2 \le \tilde{s}_1$ 的概率为
$F(\tilde{s}_1)$。该表达式也可以写成：
$$\pi^1(\tilde{s}_1, s_1) = F(\tilde{s}_1) \left( E[V(s_1,s_2)|s_1, s_2 \le \tilde{s}_1] - E[B(s_2)|s_2 \le \tilde{s}_1] \right)$$
展开后即为：
$$\pi^1(\tilde{s}_1, s_1) = \int_{\underline{s}}^{\tilde{s}_1} V(s_1,s_2)f(s_2)ds_2 - \int_{\underline{s}}^{\tilde{s}_1} B(s_2)f(s_2)ds_2$$

**推导均衡竞标策略 $B(s_1)$**: 为了找到最优的 $\tilde{s}_1$，我们对
$\pi^1(\tilde{s}_1, s_1)$ 关于 $\tilde{s}_1$ 求导
(使用莱布尼茨积分法则)：
$$\frac{\partial \pi^1(\tilde{s}_1, s_1)}{\partial \tilde{s}_1} = [V(s_1, \tilde{s}_1) - B(\tilde{s}_1)] f(\tilde{s}_1)$$
在均衡中，竞拍者会选择
$\tilde{s}_1=s_1$，并且此导数必须为零以实现利润最大化：
$$[V(s_1, s_1) - B(s_1)] f(s_1) = 0$$ 假设在相关信号范围内 $f(s_1) > 0$
(即信号的概率密度不为零)，则必须有： $$B(s_1) = V(s_1, s_1)$$
这意味着，在两人第二价格共同价值拍卖中，均衡策略是：每个竞拍者 $i$
的出价等于"假设另一个竞拍者的信号 $s_j$ 与自己的信号 $s_i$
相同时，物品的价值"。

**例子**：如果 $V(s_1,s_2) = s_1+s_2$。那么
$B(s_1) = V(s_1,s_1) = s_1+s_1 = 2s_1$。这与第3.1节中针对特定价值函数推导出的结果一致。

### N 个竞拍者 ($N \ge 2$) 的讨论

规则 $B(s_1) = V(s_1,s_1)$ 特定于 $N=2$ 的情况，其中 $V(s_1,s_1)$
作为具有两个相同参数的价值函数是明确定义的。对于 $N > 2$
个竞拍者，这个规则如何推广？

考虑共同价值 $V = \sum_{i=1}^N s_i$。

-   对于 $N=2$，$B(s_1) = 2s_1$。

-   对于 $N>2$，如果有人简单地将 $V(s_1,s_1)$ 的想法扩展到
    $V(s_1,s_1,\dots,s_1)$ (即假设所有其他 $N-1$ 个人的信号都等于
    $s_1$)，这将意味着 $B(s_1) = Ns_1$。 然而，$Ns_1$ 通常不是此模型中
    $N>2$ 时的均衡竞标策略。原始笔记在与 $N=2$ 时 $2s_1$
    的结果比较时，正确地指出对于 $N>2$，$B(s_1) \neq Ns_1$。

对于对称第二价格共同价值拍卖的一个更一般的结果 (Milgrom and Weber, 1982)
是，拥有信号 $s_i$ 的竞拍者 $i$ 出价：
$$B(s_i) = E[V | s_i, Y_1 = s_i]$$ 其中 $Y_1 = \max_{j \neq i} s_j$
是其他 $N-1$
个竞拍者中的最高信号。出价是物品的期望价值，条件是基于自己的信号 $s_i$
以及 $s_i$ 是其竞争对手中最高信号的值
(这是其出价对获胜或决定支付价格起作用的条件)。

让我们将此应用于 $V = \sum_{k=1}^N s_k$：
$$B(s_i) = E\left[s_i + \sum_{j \neq i} s_j \middle| s_i, \max_{j \neq i} s_j = s_i \right]$$
$$B(s_i) = s_i + E\left[\max_{j \neq i} s_j \middle| \max_{j \neq i} s_j = s_i \right] + E\left[\sum_{k \neq i, k \text{ 不是最高信号者}} s_k \middle| \max_{j \neq i} s_j = s_i \right]$$
给定条件 $\max_{j \neq i} s_j = s_i$，第二项是 $s_i$。第三项是其他 $N-2$
个信号的总和，这些信号都小于 $s_i$ (因为 $Y_1$ 是除了 $s_i$
以外其他信号中的最大值，且 $Y_1 = s_i$) 。
$$B(s_i) = s_i + s_i + (N-2) E[s_k | s_k < s_i] = 2s_i + (N-2) E[s_k | s_k < s_i]$$
如果信号 $s_k \sim \text{IID U}[0,1]$，那么
$E[s_k | s_k < s_i] = s_i/2$。 所以，均衡竞标策略是：
$$B(s_i) = 2s_i + (N-2)\frac{s_i}{2} = 2s_i + \frac{N}{2}s_i - s_i = s_i + \frac{N}{2}s_i = \left(1 + \frac{N}{2}\right)s_i$$
对于 $N=2$，这得到 $B(s_i) = (1+2/2)s_i = 2s_i$，这是一致的。 对于
$N>2$，$(1+N/2)s_i \neq Ns_i$ (例如，对于
$N=3$，$(1+3/2)s_i = 2.5s_i$，而 $Ns_i=3s_i$)。这证实了笔记中关于简单
$Ns_1$ 规则对于 $N>2$ 通常不正确的论断。

# 机制设计：双寡头拍卖

## 机制设计基本概念

机制设计（Mechanism
Design）是经济学和博弈论的一个分支，它研究如何设计交易规则或制度（即"机制"），以在参与者具有私人信息并追求自身利益最大化的情况下，达成特定的社会或经济目标（如效率、收益最大化等）。机制设计者设定博弈的规则，而参与者在这些规则下进行策略性互动。

## 显示原理

显示原理（Revelation
Principle）是机制设计中的一个奠基性成果。它指出，对于任何一个间接机制（参与者可能需要进行复杂的策略性思考和行动）所能达成的任何贝叶斯纳什均衡结果，总能找到一个等价的\*\*直接机制\*\*（Direct
Mechanism）。在直接机制中，所有参与者只需向机制设计者报告他们的私人信息（例如，对物品的真实估价），并且真实地报告其私人信息对每个参与者来说都是最优策略（即构成一个贝叶斯纳什均衡）。这个直接机制能够实现与原间接机制相同的社会选择结果（例如，谁获得物品，支付多少）。

因此，显示原理极大地简化了机制设计问题：我们无需考虑所有可能的复杂机制，只需在所有\*\*激励相容的直接机制\*\*（Direct
Incentive Compatible, DIC Mechanisms）中寻找最优机制即可。

## 环境描述

-   **参与者**：有两个竞标者（Bidder），编号为
    $i=1, 2$。另有一个卖方（Seller）。

-   **物品**：卖方拥有一个不可分割的物品待售。卖方对物品的估值为0。

-   **竞标者估价**：每个竞标者 $i$ 对物品有一个私人估价
    $v_i$。这些估价是独立同分布的（i.i.d.）。

-   **估价分布**：每个竞标者的估价 $v_i$ 从集合 $\{1, 2\}$
    中抽取，具体概率为：

    -   $P(v_i=1) = \frac{1}{3}$

    -   $P(v_i=2) = \frac{2}{3}$

-   **风险态度**：所有竞标者和卖方都是风险中性的（risk-neutral），即他们只关心期望收益/效用。

-   **机制类型**：我们关注直接机制，其中竞标者报告其估价（可能真实也可能虚报）。设
    $\tilde{v}_i$ 为竞标者 $i$ 报告的估价。

## 机制变量

一个直接机制由以下两部分定义：

-   **分配规则 (Allocation Rule)** $P_i(\tilde{v}_1, \tilde{v}_2)$:
    当竞标者1报告 $\tilde{v}_1$，竞标者2报告 $\tilde{v}_2$ 时，竞标者
    $i$ 获得物品的概率。 由于只有一个物品，必须满足
    $P_1(\tilde{v}_1, \tilde{v}_2) + P_2(\tilde{v}_1, \tilde{v}_2) \le 1$。如果和小于1，则表示卖方保留物品。

-   **支付规则 (Payment Rule)** $T_i(\tilde{v}_1, \tilde{v}_2)$:
    当竞标者1报告 $\tilde{v}_1$，竞标者2报告 $\tilde{v}_2$ 时，竞标者
    $i$ 向卖方支付的期望金额。

我们的目标是设计 $(P_i, T_i)$
以最大化卖方的期望收益，同时满足激励相容和个体理性约束。

# 激励相容约束 (Incentive Compatibility - IC)

## 定义与目标

激励相容约束要求，在直接机制中，每个竞标者都有动机真实地报告其私人估价。换言之，假设其他竞标者都真实报告，那么某个竞标者真实报告自己类型所获得的期望效用，应不小于其谎报任何其他类型所能获得的期望效用。

## 竞标者1的IC约束

我们以竞标者1为例。假设竞标者2真实报告其估价
$v_2$。竞标者1的期望效用是其赢得物品的概率乘以其真实估价，减去其期望支付。
令 $U_1(v_1, \tilde{v}_1)$ 表示竞标者1真实估价为 $v_1$ 但报告估价为
$\tilde{v}_1$ 时的期望效用。该期望是针对竞标者2的真实估价 $v_2$
的不同可能性计算的。
$$U_1(v_1, \tilde{v}_1) = E_{v_2} [P_1(\tilde{v}_1, v_2) \cdot v_1 - T_1(\tilde{v}_1, v_2)]$$
激励相容要求对于所有的 $v_1, \tilde{v}_1$:
$U_1(v_1, v_1) \ge U_1(v_1, \tilde{v}_1)$。

具体到本例中，对竞标者1：

1.  **当真实估价 $v_1=1$ 时（向上约束，防止低估价者谎报为高估价者）：**
    报告 $\tilde{v}_1=1$ 的期望效用 $\ge$ 报告 $\tilde{v}_1=2$
    的期望效用。 $$\begin{aligned}
        & E_{v_2}[P_1(1,v_2)\cdot 1 - T_1(1,v_2)] \ge E_{v_2}[P_1(2,v_2)\cdot 1 - T_1(2,v_2)] \\
        \Leftrightarrow \quad & \frac{1}{3}[P_1(1,1)\cdot 1 - T_1(1,1)] + \frac{2}{3}[P_1(1,2)\cdot 1 - T_1(1,2)] \\
        & \ge \frac{1}{3}[P_1(2,1)\cdot 1 - T_1(2,1)] + \frac{2}{3}[P_1(2,2)\cdot 1 - T_1(2,2)] \quad \cdots \quad (IC_{1 \to 2})
        
    \end{aligned}$$ 

2.  **当真实估价 $v_1=2$ 时（向下约束，防止高估价者谎报为低估价者）：**
    报告 $\tilde{v}_1=2$ 的期望效用 $\ge$ 报告 $\tilde{v}_1=1$
    的期望效用。 $$\begin{aligned}
        & E_{v_2}[P_1(2,v_2)\cdot 2 - T_1(2,v_2)] \ge E_{v_2}[P_1(1,v_2)\cdot 2 - T_1(1,v_2)] \\
        \Leftrightarrow \quad & \frac{1}{3}[P_1(2,1)\cdot 2 - T_1(2,1)] + \frac{2}{3}[P_1(2,2)\cdot 2 - T_1(2,2)] \\
        & \ge \frac{1}{3}[P_1(1,1)\cdot 2 - T_1(1,1)] + \frac{2}{3}[P_1(1,2)\cdot 2 - T_1(1,2)] \quad \cdots \quad (IC_{2 \to 1})
        
    \end{aligned}$$ 

由于对称性，对竞标者2也存在类似的IC约束。

## 期中概率与期中支付

为了简化IC约束的表达，我们定义"期中"（interim）概念。期中指的是在竞标者知道自己的私人估价之后，但在知道其他竞标者估价之前。
对于竞标者1，当其报告估价为 $\tilde{v}_1$ 时：

-   **期中获胜概率 $P_1(\tilde{v}_1)$**:
    $$P_1(\tilde{v}_1) = E_{v_2}[P_1(\tilde{v}_1, v_2)] = P(v_2=1)P_1(\tilde{v}_1,1) + P(v_2=2)P_1(\tilde{v}_1,2)$$
    具体地： $$\begin{aligned}
        P_1(1) &= \frac{1}{3}P_1(1,1) + \frac{2}{3}P_1(1,2) \\
        P_1(2) &= \frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2)
        
    \end{aligned}$$

-   **期中期望支付 $T_1(\tilde{v}_1)$**:
    $$T_1(\tilde{v}_1) = E_{v_2}[T_1(\tilde{v}_1, v_2)] = P(v_2=1)T_1(\tilde{v}_1,1) + P(v_2=2)T_1(\tilde{v}_1,2)$$
    具体地： $$\begin{aligned}
        T_1(1) &= \frac{1}{3}T_1(1,1) + \frac{2}{3}T_1(1,2) \\
        T_1(2) &= \frac{1}{3}T_1(2,1) + \frac{2}{3}T_1(2,2)
        
    \end{aligned}$$

## IC约束的简化形式

使用期中量，竞标者1的IC约束可以重写为： $$\begin{aligned}
P_1(1) \cdot 1 - T_1(1) &\ge P_1(2) \cdot 1 - T_1(2) \quad &(IC'_{1 \to 2}) \label{eq:ic1_simple} \\
P_1(2) \cdot 2 - T_1(2) &\ge P_1(1) \cdot 2 - T_1(1) \quad &(IC'_{2 \to 1}) \label{eq:ic2_simple}
\end{aligned}$$

## IC约束的推论

1.  **获胜概率的单调性 (Monotonicity of Allocation Probability)**: 将式
    [\[eq:ic1_simple\]](#eq:ic1_simple){reference-type="eqref"
    reference="eq:ic1_simple"} 和
    [\[eq:ic2_simple\]](#eq:ic2_simple){reference-type="eqref"
    reference="eq:ic2_simple"} 相加，得到：
    $$P_1(1) - T_1(1) + 2P_1(2) - T_1(2) \ge P_1(2) - T_1(2) + 2P_1(1) - T_1(1)$$
    $$P_1(1) + 2P_1(2) \ge P_1(2) + 2P_1(1)$$ $$P_1(2) \ge P_1(1)$$
    这意味着，真实估价越高的竞标者，其（期中）获胜概率也应该越高（或至少不低）。这是一个普遍的结论。

2.  **支付的关系**: 从
    [\[eq:ic1_simple\]](#eq:ic1_simple){reference-type="eqref"
    reference="eq:ic1_simple"} 可得:
    $T_1(2) - T_1(1) \ge P_1(2) - P_1(1)$。 从
    [\[eq:ic2_simple\]](#eq:ic2_simple){reference-type="eqref"
    reference="eq:ic2_simple"} 可得:
    $2(P_1(2) - P_1(1)) \ge T_1(2) - T_1(1)$。 所以，
    $P_1(2) - P_1(1) \le T_1(2) - T_1(1) \le 2(P_1(2) - P_1(1))$。
    在机制设计中，通常假设某些IC约束是紧的（binding），即取等号。特别是高估价类型不愿谎报为低估价类型的约束
    $(IC'_{2 \to 1})$ 常常被假设为紧的，用以确定支付。 如果
    $(IC'_{2 \to 1})$ 取紧：
    $$P_1(2) \cdot 2 - T_1(2) = P_1(1) \cdot 2 - T_1(1)$$
    $$\Rightarrow T_1(2) - T_1(1) = 2(P_1(2) - P_1(1))$$
    这也符合笔记中紫色箭头所指的推导。

# 个体理性约束 (Individual Rationality - IR)

## 定义与目标

个体理性约束（也称参与约束）要求，每个竞标者在真实报告其类型并参与机制时，所获得的期望效用必须是非负的。否则，竞标者可以选择不参与机制，获得0效用。

## IR约束的设定

对于竞标者1，其IR约束为（假设其真实报告类型）： $$\begin{aligned}
P_1(1) \cdot 1 - T_1(1) &\ge 0 \quad &(IR_1) \label{eq:ir1} \\
P_1(2) \cdot 2 - T_1(2) &\ge 0 \quad &(IR_2) \label{eq:ir2}
\end{aligned}$$
通常，为了最大化卖方收益，\*\*最低估价类型的IR约束会取紧\*\*，即刚好等于0。这是因为卖方希望在不违反参与约束的前提下，尽可能多地向竞标者收取费用。如果最低估价类型的IR满足，且IC满足，则更高估价类型的IR通常也会满足。
因此，我们假设 $(IR_1)$ 取紧：
$$P_1(1) \cdot 1 - T_1(1) = 0 \implies T_1(1) = P_1(1)$$

## 支付规则的确定 (结合IC和IR)

我们已经有：

1.  $T_1(1) = P_1(1)$ (来自紧的 $IR_1$)

2.  $T_1(2) - T_1(1) = 2(P_1(2) - P_1(1))$ (来自紧的 $IC'_{2 \to 1}$)

将 (1) 代入 (2)，得到竞标者1报告类型2时的期中支付：
$$T_1(2) = T_1(1) + 2(P_1(2) - P_1(1)) = P_1(1) + 2P_1(2) - 2P_1(1)$$
$$T_1(2) = 2P_1(2) - P_1(1)$$ 这与笔记中对 $T_1(2)$ 的推导
$(P_1(2)-P_1(1))\cdot 2 + P_1(1)$ 结果一致。

# 卖方期望收益最大化

## 卖方期望收益函数

卖方的总期望收益 $R$
是从所有竞标者处获得的期望支付之和。由于对称性，对竞标者2也存在类似的支付规则:
$T_2(1) = P_2(1)$ 和 $T_2(2) = 2P_2(2) - P_2(1)$。
竞标者1对卖方收益的期望贡献为 $E[T_1]$ (期望是对 $v_1$ 的类型取的)：
$$\begin{aligned}
E[T_1] &= P(v_1=1)T_1(1) + P(v_1=2)T_1(2) \\
&= \frac{1}{3} T_1(1) + \frac{2}{3} T_1(2) \\
&= \frac{1}{3} P_1(1) + \frac{2}{3} (2P_1(2) - P_1(1)) \\
&= \frac{1}{3} P_1(1) + \frac{4}{3} P_1(2) - \frac{2}{3} P_1(1) \\
&= \frac{4}{3} P_1(2) - \frac{1}{3} P_1(1)
\end{aligned}$$ 卖方的总期望收益为 $R = E[T_1] + E[T_2]$：
$$R = \left(\frac{4}{3} P_1(2) - \frac{1}{3} P_1(1)\right) + \left(\frac{4}{3} P_2(2) - \frac{1}{3} P_2(1)\right)$$

## 收益函数的展开与重组

现在，我们将期中获胜概率 $P_i(\cdot)$ 用基于报告类型组合
$(\tilde{v}_1, \tilde{v}_2)$ 的分配规则 $P_i(\tilde{v}_1, \tilde{v}_2)$
展开。 回忆： $P_1(1) = \frac{1}{3}P_1(1,1) + \frac{2}{3}P_1(1,2)$
$P_1(2) = \frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2)$
$P_2(1) = \frac{1}{3}P_2(1,1) + \frac{2}{3}P_2(2,1)$
(竞标者2报告1，对竞标者1的类型取期望)
$P_2(2) = \frac{1}{3}P_2(1,2) + \frac{2}{3}P_2(2,2)$
(竞标者2报告2，对竞标者1的类型取期望)

代入 $R$ 的表达式： $$\begin{aligned}
R = & \frac{4}{3} \left(\frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2)\right) - \frac{1}{3} \left(\frac{1}{3}P_1(1,1) + \frac{2}{3}P_1(1,2)\right) \\
& + \frac{4}{3} \left(\frac{1}{3}P_2(1,2) + \frac{2}{3}P_2(2,2)\right) - \frac{1}{3} \left(\frac{1}{3}P_2(1,1) + \frac{2}{3}P_2(2,1)\right)
\end{aligned}$$ 整理各项： $$\begin{aligned}
R = & \frac{4}{9}P_1(2,1) + \frac{8}{9}P_1(2,2) - \frac{1}{9}P_1(1,1) - \frac{2}{9}P_1(1,2) \\
& + \frac{4}{9}P_2(1,2) + \frac{8}{9}P_2(2,2) - \frac{1}{9}P_2(1,1) - \frac{2}{9}P_2(2,1)
\end{aligned}$$ 按照报告的类型组合 $(\tilde{v}_1, \tilde{v}_2)$
对各项进行重新组合，这与笔记中的方式一致：

-   类型组合 $(\tilde{v}_1, \tilde{v}_2) = (2,2)$: 来自 $P_1(2,2)$ 和
    $P_2(2,2)$ 的贡献是
    $\frac{8}{9}P_1(2,2) + \frac{8}{9}P_2(2,2) = \frac{8}{9}(P_1(2,2)+P_2(2,2))$

-   类型组合 $(\tilde{v}_1, \tilde{v}_2) = (2,1)$: 来自 $P_1(2,1)$ 和
    $P_2(2,1)$ 的贡献是 $\frac{4}{9}P_1(2,1) - \frac{2}{9}P_2(2,1)$

-   类型组合 $(\tilde{v}_1, \tilde{v}_2) = (1,2)$: 来自 $P_1(1,2)$ 和
    $P_2(1,2)$ 的贡献是 $-\frac{2}{9}P_1(1,2) + \frac{4}{9}P_2(1,2)$

-   类型组合 $(\tilde{v}_1, \tilde{v}_2) = (1,1)$: 来自 $P_1(1,1)$ 和
    $P_2(1,1)$ 的贡献是
    $-\frac{1}{9}P_1(1,1) - \frac{1}{9}P_2(1,1) = -\frac{1}{9}(P_1(1,1)+P_2(1,1))$

所以，总期望收益可以写为： $$\begin{aligned}
R = & \frac{8}{9}(P_1(2,2)+P_2(2,2)) \\
& + \left(\frac{4}{9}P_1(2,1) - \frac{2}{9}P_2(2,1)\right) \\
& + \left(-\frac{2}{9}P_1(1,2) + \frac{4}{9}P_2(1,2)\right) \\
& - \frac{1}{9}(P_1(1,1)+P_2(1,1))
\end{aligned}$$ 这个表达式中的系数（如
$8/9, 4/9, -2/9, -1/9$）可以看作是与每种类型组合下分配物品的"虚拟估价"或"影子价格"相关的权重。

## 最优分配规则的确定

为了最大化期望收益 $R$，我们需要选择分配概率
$P_i(\tilde{v}_1, \tilde{v}_2)$。这些概率必须满足
$0 \le P_i(\tilde{v}_1, \tilde{v}_2) \le 1$ 以及
$P_1(\tilde{v}_1, \tilde{v}_2) + P_2(\tilde{v}_1, \tilde{v}_2) \le 1$。
观察 $R$ 的表达式中各项的系数：

1.  **当报告为 $(\tilde{v}_1, \tilde{v}_2) = (2,2)$ 时**: 项为
    $\frac{8}{9}(P_1(2,2)+P_2(2,2))$。由于系数
    $\frac{8}{9} > 0$，为最大化此项，应使 $P_1(2,2)+P_2(2,2)$ 最大，即
    $P_1(2,2)+P_2(2,2)=1$。这意味着当两个竞标者都报告高估价时，物品应该被分配出去。

2.  **当报告为 $(\tilde{v}_1, \tilde{v}_2) = (2,1)$ 时**: 项为
    $\frac{4}{9}P_1(2,1) - \frac{2}{9}P_2(2,1)$。为最大化此项，应使
    $P_1(2,1)=1$ 且
    $P_2(2,1)=0$。这意味着当竞标者1报告高估价、竞标者2报告低估价时，物品应该分配给竞标者1。

3.  **当报告为 $(\tilde{v}_1, \tilde{v}_2) = (1,2)$ 时**: 项为
    $-\frac{2}{9}P_1(1,2) + \frac{4}{9}P_2(1,2)$。为最大化此项，应使
    $P_1(1,2)=0$ 且
    $P_2(1,2)=1$。这意味着当竞标者1报告低估价、竞标者2报告高估价时，物品应该分配给竞标者2。

4.  **当报告为 $(\tilde{v}_1, \tilde{v}_2) = (1,1)$ 时**: 项为
    $-\frac{1}{9}(P_1(1,1)+P_2(1,1))$。由于系数
    $-\frac{1}{9} < 0$，为最大化此项（即最小化损失），应使
    $P_1(1,1)+P_2(1,1)$ 最小，即
    $P_1(1,1)+P_2(1,1)=0$。这意味着当两个竞标者都报告低估价时，物品不应被分配出去（卖方保留）。

## 最优机制下的结果

根据上述最优分配规则：

-   $P_1(1,1)=0, P_2(1,1)=0$

-   $P_1(1,2)=0, P_2(1,2)=1$

-   $P_1(2,1)=1, P_2(2,1)=0$

-   $P_1(2,2)+P_2(2,2)=1$。为确定起见，可以设定多种分配方式，例如将物品确定地分配给竞标者1（$P_1(2,2)=1, P_2(2,2)=0$），或分配给竞标者2，或以某种概率分配。总收益中只关心其和。

现在计算在此最优分配规则下，竞标者1的期中获胜概率和期中支付：

-   $P_1(1) = \frac{1}{3}P_1(1,1) + \frac{2}{3}P_1(1,2) = \frac{1}{3}(0) + \frac{2}{3}(0) = 0$。

-   根据 $T_1(1) = P_1(1)$, 可得 $T_1(1) = 0$。

-   $P_1(2) = \frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2)$。 如果采用
    $P_1(2,2)=1, P_2(2,2)=0$
    的分配方式（当两人都报高价时，竞标者1获得物品），则：
    $P_1(2) = \frac{1}{3}(1) + \frac{2}{3}(1) = 1$。

-   根据 $T_1(2) = 2P_1(2) - P_1(1)$, 可得 $T_1(2) = 2(1) - 0 = 2$。

笔记中最后的 \"$T_1(2) = 2(\frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2))$\"
实际上就是将 $P_1(2)$ 的定义代入 $T_1(2)=2P_1(2)$ (因为 $P_1(1)=0$)。

如果当 $(\tilde{v}_1, \tilde{v}_2) = (2,2)$ 时，物品以各 $1/2$
的概率分配给竞标者1和2，即 $P_1(2,2)=1/2, P_2(2,2)=1/2$，那么：
$P_1(2) = \frac{1}{3}P_1(2,1) + \frac{2}{3}P_1(2,2) = \frac{1}{3}(1) + \frac{2}{3}(\frac{1}{2}) = \frac{1}{3} + \frac{1}{3} = \frac{2}{3}$。
$T_1(2) = 2P_1(2) - P_1(1) = 2(\frac{2}{3}) - 0 = \frac{4}{3}$。
对称地，$P_2(2) = \frac{2}{3}$，$T_2(2) = \frac{4}{3}$。
这种情况下，卖方在 $(\tilde{v}_1, \tilde{v}_2) = (2,2)$ 时收到的总支付为
$T_1(2,2)+T_2(2,2)$。而期中支付 $T_1(2)$ 是针对所有 $v_2$ 的平均。

# 默森 (Myerson) 拍卖设计理论

本文档详细解释了罗杰·迈尔森 (Roger Myerson)
关于最优拍卖设计的理论。核心目标是设计一个拍卖机制，使得卖方的期望收益最大化。理论的关键在于引入"虚拟估值"(Virtual
Valuation) 的概念，并通过显示原理 (Revelation Principle)
将问题简化为设计一个激励相容的直接机制。

## 拍卖设定 (Auction design by Myerson)

-   **竞标者集合 (Set of bidders):** $N = \{1, 2, \dots, n\}$，代表有
    $n$ 个竞标者。

-   **估值/类型 (Types):** 每个竞标者 $i$ 对拍卖品有一个私人估值 $t_i$
    (也称作其类型)。该估值从区间 $[a_i, b_i]$ 中独立抽取，其概率密度函数
    (PDF) 为 $f_i: [a_i, b_i] \to \mathbb{R}^+$。我们用 $F_i(t_i)$
    表示其累积分布函数 (CDF)。

-   $t = (t_1, \dots, t_n)$ 是所有竞标者类型组成的向量。

-   $t_{-i} = (t_1, \dots, t_{i-1}, t_{i+1}, \dots, t_n)$ 表示除竞标者
    $i$ 以外其他所有竞标者的类型向量。

-   所有可能的类型组合的空间为
    $T = \underset{i=1}{\overset{n}{\times}} [a_i, b_i]$。

-   由于独立性，类型向量 $t$ 的联合概率密度函数为
    $f(t) = \prod_{j=1}^n f_j(t_j)$。

-   除 $i$ 以外其他人类型向量的联合概率密度函数为
    $f_{-i}(t_{-i}) = \prod_{j \neq i} f_j(t_j)$。

## 拍卖机制 (Auction Mechanism)

一个拍卖机制由一对函数 $(p, x)$ 定义：

-   $p = (p_1(t), \dots, p_n(t))$：其中 $p_i(t)$
    表示当所有竞标者的类型组合为 $t$ 时，竞标者 $i$
    赢得拍卖品的概率。必须满足
    $\sum_{i=1}^n p_i(t) \le 1$（物品最多被一个竞标者赢得，或者不被任何人赢得）。

-   $x = (x_1(t), \dots, x_n(t))$：其中 $x_i(t)$
    表示当所有竞标者的类型组合为 $t$ 时，竞标者 $i$ 需要支付的金额。

## 显示原理 (Revelation Principle)

显示原理是拍卖理论中的一个基石。它指出：对于任何（可能是间接的、复杂的）可行拍卖机制，都存在一个等价的、可行的**直接机制
(direct mechanism)**，在该直接机制中：

1.  竞标者被要求直接向拍卖方汇报他们的真实类型。

2.  如实汇报其真实类型是每个竞标者的最优策略（即该机制是激励相容的，Incentive
    Compatible）。

3.  这个直接机制能给卖方和所有竞标者带来与原机制相同的期望效用。

因此，在设计最优拍卖时，我们无需考虑所有复杂的间接机制，只需在所有激励相容的直接机制中寻找最优者。

## 竞标者 $i$ 的效用 (Utility for bidder $i$)

如果竞标者 $i$ 的真实类型是
$t_i$，在直接机制中，他汇报类型（我们关注他如实汇报的情况），那么他的期望效用
$U_i(p, x, t_i)$ 是：
$$U_i(p, x, t_i) = \int_{T_{-i}} (V_i(t) p_i(t) - x_i(t)) f_{-i}(t_{-i}) dt_{-i}$$
其中 $V_i(t)$ 是竞标者 $i$ 在类型向量为 $t$ 时对物品的估值。

-   笔记中首先提到了一种可能带有外部性的估值形式：$V_i(t) = t_i + \sum_{j \neq i} e_j(t_j)$，这意味着
    $i$ 的估值不仅取决于自己的类型 $t_i$，还受到其他人类型 $t_j$
    的影响（$e_j(t_j)$ 是外部性项）。

-   然而，一个重要的修正或常见假设是**私人估值 (private
    value)**模型：$V_i(t) = t_i$。这意味着物品对竞标者 $i$
    的价值仅由其自身类型 $t_i$ 决定。在后续推导中，我们主要采用此设定。

## 卖方效用 (Utility for seller)

卖方的期望效用 $U_0(p, x)$（通常是期望收益）是：
$$U_0(p, x) = \int_T \left( V_0(t)\left(1 - \sum_{k=1}^n p_k(t)\right) + \sum_{k=1}^n x_k(t) \right) f(t) dt$$
这里：

-   $V_0(t)$
    是物品在未被卖出时，卖方对物品的估值（保留估值）。笔记中提及
    $V_0(t) = s_0 + \sum_{j=1}^n e_j(t_j)$，其中 $s_0$
    是卖方的基础保留估值。

-   第一项 $V_0(t) (1 - \sum_{k=1}^n p_k(t))$ 表示物品未卖出时（其概率为
    $1 - \sum_{k=1}^n p_k(t)$），卖方保留物品获得的期望估值。

-   第二项 $\sum_{k=1}^n x_k(t)$
    是卖方从所有竞标者那里收到的期望总支付。

为了最大化卖方收益，通常假设卖方试图最大化 $U_0(p, x)$。

## 激励相容约束 (IC - Incentive Compatibility)

在直接机制中，激励相容意味着每个竞标者如实汇报其类型 $t_i$
是其最优策略。也就是说，汇报 $t_i$
所得的期望效用，必须大于或等于谎报为任何其他类型 $s_i$
所得到的期望效用。 对于私人估值 $V_i(t)=t_i$，当真实类型为 $t_i$
时，若谎报为 $s_i$，其期望效用为
$\int_{T_{-i}} (t_i p_i(s_i, t_{-i}) - x_i(s_i, t_{-i})) f_{-i}(t_{-i}) dt_{-i}$。
则IC约束为：
$$U_i(p, x, t_i) \ge \int_{T_{-i}} (t_i p_i(s_i, t_{-i}) - x_i(s_i, t_{-i})) f_{-i}(t_{-i}) dt_{-i} \quad \forall s_i \in [a_i, b_i]$$
定义竞标者 $i$ 汇报类型 $s_i$ 时的**期望获胜概率** (interim expected
probability of winning) 为：
$$Q_i(p, s_i) = \int_{T_{-i}} p_i(s_i, t_{-i}) f_{-i}(t_{-i}) dt_{-i}$$
通过一系列推导（基于效用函数的包络定理），可以从IC约束得到：
$$\label{eq:envelope}
\frac{dU_i(p, x, t_i)}{dt_i} = Q_i(p, t_i)$$
这个公式表明：竞标者期望效用对其类型的边际变化率，等于其在该类型下的期望获胜概率。
对 [\[eq:envelope\]](#eq:envelope){reference-type="eqref"
reference="eq:envelope"} 式从类型的下限 $a_i$ 积分到 $t_i$：
$$U_i(p, x, t_i) - U_i(p, x, a_i) = \int_{a_i}^{t_i} Q_i(p, s_i) ds_i$$
因此，竞标者的期望效用可以表示为：
$$U_i(p, x, t_i) = U_i(p, x, a_i) + \int_{a_i}^{t_i} Q_i(p, s_i) ds_i$$
$U_i(p, x, a_i)$ 是类型为最低可能值 $a_i$ 的竞标者的期望效用。个体理性
(Individual Rationality, IR) 约束要求 $U_i(p, x, t_i) \ge 0$ 对所有
$t_i$ 成立。由于 $Q_i(p, s_i) \ge 0$（获胜概率非负），$U_i(p, x, t_i)$
是关于 $t_i$ 非减的。因此，要满足所有类型的IR约束，只需满足最低类型
$a_i$ 的IR约束，即 $U_i(p, x, a_i) \ge 0$。为了最大化卖方收益，通常将
$U_i(p, x, a_i)$ 设定为0。

## 卖方期望收益 $R$ 的改写

卖方的期望收益
$R = U_0(p, x)$。为简化讨论，我们先假设卖方对物品的保留估值 $V_0(t) = 0$
(即不保留物品则价值为0)，此时卖方收益完全来自竞标者的支付：
$$R = \sum_{i=1}^n \int_T x_i(t) f(t) dt = \sum_{i=1}^n \int_{a_i}^{b_i} \left( \int_{T_{-i}} x_i(t_i, t_{-i}) f_{-i}(t_{-i}) dt_{-i} \right) f_i(t_i) dt_i$$
从竞标者效用定义
$U_i(p, x, t_i) = \int_{T_{-i}} (t_i p_i(t) - x_i(t)) f_{-i}(t_{-i}) dt_{-i}$，我们可以得到竞标者
$i$ 的期望支付：
$$\int_{T_{-i}} x_i(t) f_{-i}(t_{-i}) dt_{-i} = t_i Q_i(p, t_i) - U_i(p, x, t_i)$$
令 $A_i$ 代表竞标者 $i$ 的总期望支付对卖方收益的贡献： $$\begin{aligned}
A_i &= \int_{a_i}^{b_i} \left( t_i Q_i(p, t_i) - U_i(p, x, t_i) \right) f_i(t_i) dt_i \\
&= \int_{a_i}^{b_i} \left( t_i Q_i(p, t_i) - \left( U_i(p, x, a_i) + \int_{a_i}^{t_i} Q_i(p, s_i) ds_i \right) \right) f_i(t_i) dt_i \\
&= \int_{a_i}^{b_i} t_i Q_i(p, t_i) f_i(t_i) dt_i - U_i(p, x, a_i)\int_{a_i}^{b_i}f_i(t_i)dt_i - \int_{a_i}^{b_i} \left( \int_{a_i}^{t_i} Q_i(p, s_i) ds_i \right) f_i(t_i) dt_i
\end{aligned}$$ 注意到
$\int_{a_i}^{b_i}f_i(t_i)dt_i = 1$。对于最后一项，可以使用分部积分法或以下常用的积分恒等式（通过改变积分次序得到）：
$$\int_{a_i}^{b_i} \left( \int_{a_i}^{t_i} Q_i(p, s_i) ds_i \right) f_i(t_i) dt_i = \int_{a_i}^{b_i} Q_i(p, s_i) (1 - F_i(s_i)) ds_i$$
将此代回 $A_i$ 的表达式： $$\begin{aligned}
A_i &= \int_{a_i}^{b_i} t_i Q_i(p, t_i) f_i(t_i) dt_i - \int_{a_i}^{b_i} Q_i(p, t_i) (1 - F_i(t_i)) dt_i - U_i(p, x, a_i) \\
&= \int_{a_i}^{b_i} Q_i(p, t_i) \left( t_i f_i(t_i) - (1 - F_i(t_i)) \right) dt_i - U_i(p, x, a_i) \\
&= \int_{a_i}^{b_i} Q_i(p, t_i) \left( t_i - \frac{1 - F_i(t_i)}{f_i(t_i)} \right) f_i(t_i) dt_i - U_i(p, x, a_i)
\end{aligned}$$ 将
$Q_i(p, t_i) = \int_{T_{-i}} p_i(t_i, t_{-i}) f_{-i}(t_{-i}) dt_{-i}$
代回，并将 $f_i(t_i)f_{-i}(t_{-i})=f(t)$ 合并：
$$A_i = \int_{T} p_i(t) \left( t_i - \frac{1 - F_i(t_i)}{f_i(t_i)} \right) f(t) dt - U_i(p, x, a_i)$$

## 虚拟估值 (Virtual Valuation)

我们定义竞标者 $i$ 的**虚拟估值函数** (或者称为虚拟类型) $\phi_i(t_i)$
为： $$\phi_i(t_i) = t_i - \frac{1 - F_i(t_i)}{f_i(t_i)}$$
这个函数是迈尔森理论的核心。$\frac{1 - F_i(t_i)}{f_i(t_i)}$ 是逆风险率
(inverse hazard rate)。 那么 $A_i$ 可以写成：
$$A_i = \int_{T} p_i(t) \phi_i(t_i) f(t) dt - U_i(p, x, a_i)$$
卖方的总期望收益（假设 $U_i(p, x, a_i) = 0$ 对所有 $i$
成立，并且考虑卖方保留物品的估值 $V_0(t)$）为： $$\begin{aligned}
R &= \sum_{i=1}^n A_i + \int_T V_0(t)\left(1-\sum_{k=1}^n p_k(t)\right)f(t)dt \\
&= \sum_{i=1}^n \int_{T} p_i(t) \phi_i(t_i) f(t) dt + \int_T V_0(t)f(t)dt - \int_T V_0(t)\sum_{k=1}^n p_k(t)f(t)dt \\
&= \int_T \left( \sum_{k=1}^n p_k(t) (\phi_k(t_k) - V_0(t)) + V_0(t) \right) f(t) dt
\end{aligned}$$

## 最优拍卖规则 (Optimal Auction)

为了最大化卖方期望收益 $R$，我们需要逐点（即对于每一个可能的类型组合
$t$）选择分配概率 $p_k(t)$ 来最大化被积函数
$\sum_{k=1}^n p_k(t) (\phi_k(t_k) - V_0(t))$。 最优的分配规则如下：

1.  对每个竞标者 $k$，计算其有效虚拟估值 $\phi_k(t_k) - V_0(t)$。

2.  将物品分配给使得这个有效虚拟估值最大的竞标者 $i^*$ (即
    $p_{i^*}^*(t)=1$)，前提条件是这个最大值必须为非负，即
    $\phi_{i^*} (t_{i^*}) - V_0(t) \ge 0$。

3.  如果所有竞标者的 $\phi_k(t_k) - V_0(t)$ 都小于0，则物品不卖出 (即
    $p_k^*(t)=0$ 对所有 $k$)，卖方保留物品。

简单来说，若定义卖方的虚拟估值为
$\phi_0(t) = V_0(t)$（在笔记中，这可能对应 $e_i(t_i)$ 或一个保留价
$r_0$），则最优规则是： 找出
$i^* = \text{argmax}_{i \in N \cup \{0\}} \{ \phi_i(t_i) \}$ (这里
$\phi_0(t_0)$ 代表卖方的虚拟估值，例如一个固定的 $r_0$ 或者更复杂的
$V_0(t)$)。如果 $i^* \in N$
(即某个竞标者的虚拟估值最高且不低于卖方的)，则物品分配给 $i^*$。如果
$i^*=0$ (卖方自己的虚拟估值最高)，则物品不卖出。 笔记中的分配规则
$i^* = \text{argmax}_i \{ t_i - e_i(t_i) - \frac{1 - F_i(t_i)}{f_i(t_i)} \}$，然后要求这个最大值非负。这等价于将
$e_i(t_i)$ 视为与卖方保留估值相关的一个项。

通常要求虚拟估值函数 $\phi_i(t_i)$ 是关于 $t_i$ 严格递增的，即
$\phi_i'(t_i) > 0$。这个条件被称为**正则性条件 (regularity
condition)**。如果分布不是正则的（例如，$\phi_i(t_i)$
可能有下降的部分），则需要进行所谓的"熨平
(ironing)"处理，这使得分析更为复杂。

## 例子和收益等价定理

### 例子：均匀分布 (Uniform Distribution)

假设竞标者 $i$ 的类型 $t_i$ 服从 $[0,1]$ 上的均匀分布
($t_i \sim U[0,1]$)。 则：

-   $F_i(t_i) = t_i$ (CDF)

-   $f_i(t_i) = 1$ (PDF)

其虚拟估值为：
$$\phi_i(t_i) = t_i - \frac{1 - t_i}{1} = t_i - 1 + t_i = 2t_i - 1$$

### 例子：独立同分布私人估值 (i.i.d. Private Values)

如果所有竞标者的估值都是从同一个分布 $F(\cdot)$ (密度为 $f(\cdot)$)
中独立抽取的，并且满足正则性条件 ($\phi'(t) > 0$)。
此时，最优拍卖将物品分配给虚拟估值 $\phi(t_i)$
最高的竞标者，前提是其虚拟估值不低于卖方的虚拟保留估值 (例如0，或 $r_0$
使得 $\phi(r_0)=0$)。
在这种对称i.i.d.正则环境下，常见的具体最优拍卖形式包括：

-   **带有保留价 $r^*$ 的第一价格密封拍卖 (First-price sealed-bid
    auction with reserve price $r^*$)**。

-   **带有保留价 $r^*$ 的第二价格密封拍卖 (Second-price sealed-bid
    auction with reserve price $r^*$)**。

这里的保留价 $r^*$ 通常设定为使得 $\phi(r^*) = \phi_0$
(卖方的虚拟估值，若卖方保留物品价值为0，则 $\phi(r^*)=0$)。

### 收益等价定理 (Revenue Equivalence Theorem)

该定理指出：在某些条件下，不同的拍卖机制可以产生相同的卖方期望收益。标准条件如下：
假设竞标者是风险中性的，他们的私人估值是独立抽取的（可以来自不同的分布）。那么任何满足以下条件的拍卖机制，其卖方期望收益都相同：

1.  **相同的分配结果：**
    对于任何类型组合，具有合格最高估值的竞标者赢得物品（即，物品总是分配给某个特定规则选出的赢家，例如，在对称情况下，估值最高者赢得，只要其估值超过某个保留价）。

2.  **最低类型的期望效用相同：** 所有竞标者中，类型为最低可能值 ($a_i$)
    的个体，其期望效用为零 (或者某个相同的常数)。

笔记中的表述为：如果所有竞标者的 $U_i(p,x,a_i)$
相同，并且对于所有类型组合 $t$，分配概率 $P_i(t)$
都相同，那么这些机制会产生相同的期望收益 $R$。

### 例子：非对称竞标者 (Asymmetric Bidders)

考虑两个竞标者 ($n=2$)，具有私人估值：

-   竞标者1:
    $t_1 \sim U[0,1] \implies F_1(t_1) = t_1, f_1(t_1) = 1 \implies \phi_1(t_1) = 2t_1 - 1$.

-   竞标者2:
    $t_2 \sim U[0,2] \implies F_2(t_2) = t_2/2, f_2(t_2) = 1/2 \implies \phi_2(t_2) = t_2 - \frac{1 - t_2/2}{1/2} = t_2 - (2 - t_2) = 2t_2 - 2$.

假设一个具体的类型实现：$t_1 = 1, t_2 = 1.1$。 计算他们的虚拟估值：

-   $\phi_1(1) = 2(1) - 1 = 1$.

-   $\phi_2(1.1) = 2(1.1) - 2 = 2.2 - 2 = 0.2$.

根据最优拍卖规则（分配给虚拟估值最高者，假设卖方保留估值为0，且两个虚拟估值都非负），由于
$\phi_1(1) = 1 > \phi_2(1.1) = 0.2$，竞标者1赢得物品。
然而，我们注意到竞标者2的实际估值 $t_2 = 1.1$ 高于竞标者1的实际估值
$t_1 = 1$。这意味着，在这个非对称竞标者的例子中，最优拍卖（旨在最大化卖方收益的拍卖）并**不一定是有效率的**
(allocatively efficient)。效率通常指将物品分配给对其估值最高的竞标者。
这也说明，标准的第二价格拍卖（总是将物品给予估值最高者）在这种非对称情况下，**不是最优的**
(not optimal for revenue
maximization)，因为它不一定能最大化卖方收益。为了达到收益最大化，需要根据虚拟估值来设计分配规则和支付规则。

# 动态非完全信息博弈 (Dynamic Incomplete Information Game)

动态非完全信息博弈是博弈论中的一个重要分支，它研究在信息不对称且行动有先后顺序的情况下，参与者的策略选择问题。

-   **动态 (Dynamic):**
    参与者按一定的顺序行动，后行动者在行动时可能已经观察到先行动者的部分或全部行动。

-   **非完全信息 (Incomplete Information):**
    至少有一位参与者不完全了解博弈的某些关键特征，最常见的是不清楚其他参与者的"类型"(Type)。类型可以指参与者的偏好、成本结构、拥有的资源或私人信息等。

## 图示博弈示例：简化的求婚博弈

笔记中的图示为一个典型的动态非完全信息博弈，可以理解为一个简化的求婚博弈场景：

1.  **自然 (N, Nature):** 首先决定参与人1（求婚者）的类型。

    -   以概率 $p$ 成为 R 型 (Rich, 富有)。

    -   以概率 $1-p$ 成为 P 型 (Poor, 贫穷)。

2.  **参与人1 (求婚者，信号发送方):**

    -   知道自己的真实类型（R 或 P），此为私人信息。

    -   根据自己的类型选择一个行动：送 E (Expensive gift, 贵重礼物) 或 C
        (Cheap gift, 便宜礼物)。

3.  **参与人2 (被求婚者，信号接收方):**

    -   观察到参与人1选择的礼物 (E 或
        C)，但*不能直接观察到*参与人1的真实类型 (R 或 P)。

    -   在观察到礼物后，参与人2会对其类型形成一个**信念
        (Belief)**。例如，观察到贵重礼物
        E，可能会使得参与人2认为求婚者是 R 型的概率增加。

    -   根据观察到的礼物和形成的信念，选择一个行动：M (Marry, 结婚) 或 N
        (Not Marry, 不结婚)。

在这个博弈中，核心的信息不对称在于：参与人1知道自己的类型，而参与人2不知道，只能通过参与人1的行动（送礼）来推断其类型。参与人2的决策节点（例如，观察到E后决定是否结婚）构成一个**信息集
(Information
Set)**，因为她不确定这个E是由R型还是P型发出的（除非某种类型从不选择E）。

## 精炼贝叶斯均衡 (Perfect Bayesian Equilibrium - PBE)

精炼贝叶斯均衡是分析动态非完全信息博弈的主要解概念。它由参与者的策略组合和信念系统构成，并且这些策略和信念需要满足一定的理性要求。

PBE 要求满足以下两个核心条件：

1.  **信念一致性 (Belief Consistency):**
    在均衡路径上，当一个参与者处于某个信息集时，他对该信息集内不同节点的信念（即到达各个节点的概率）必须通过贝叶斯法则，并结合其他参与者的均衡策略以及类型的先验概率分布来计算得出。对于**非均衡路径
    (Out-of-Equilibrium
    Path)**上的信息集（即在均衡中不会被到达的信息集），贝叶斯法则可能无法直接应用，PBE对此处的信念约束较弱，但通常要求信念是"合理的"。

2.  **序贯理性 (Sequential Rationality):**
    在给定信念和其他参与者策略的情况下，每一位参与者在博弈的每一个可能的信息集上，其选择的行动都必须是最大化其自身期望效用的最优选择（从该信息集开始直至博弈结束）。

    -   **发送方理性:**
        给定接收方的预期反应策略，发送方的每个类型都选择能最大化自身期望效用的行动。

    -   **接收方理性:**
        给定观察到的行动以及据此形成的信念，接收方选择能最大化自身期望效用的行动。

根据发送方在均衡中行为模式的不同，PBE可以分为：

-   **分离均衡 (Separating Equilibrium):**
    不同类型的发送方选择不同的行动，使得接收方可以通过观察行动完美地推断出发送方的类型。

-   **混同均衡 (Pooling Equilibrium):**
    所有类型的发送方都选择相同的行动，因此接收方在观察到该行动后无法通过行动本身更新关于发送方类型的先验信念。

-   **半分离/部分混同均衡 (Semi-Separating/Semi-Pooling Equilibrium):**
    某些类型的发送方选择相同的行动，而另一些类型选择不同的行动；或者某一类型的发送方以一定的概率混合其行动选择。

# 教育信号传递模型 (Signaling by Education)

这是Michael Spence (1973)
提出的经典模型，用于解释教育如何作为一种信号机制在劳动力市场中发挥作用。即使教育本身不直接提高生产力，它也可以帮助企业区分高生产力工人和低生产力工人。

## 模型设定

-   **参与者:**

    -   **工人 (Workers, 信号发送方):** 拥有关于自身生产力的私人信息。

    -   **企业 (Firms, 信号接收方):** 无法直接观察到工人的生产力。

-   **工人的生产力类型 ($\theta$):**

    -   高生产力 ($\theta_H$): 出现概率为 $\lambda$。

    -   低生产力 ($\theta_L$): 出现概率为 $1-\lambda$。

    -   假设 $\theta_H > \theta_L$。

-   **工人的行动:** 选择教育水平 $e \ge 0$。

-   **企业的行动:** 观察到工人的教育水平 $e$
    后，形成对工人类型的信念，并提供工资
    $w(e)$。在完全竞争的市场中，企业会支付等于工人预期生产力的工资。

-   **教育成本 ($C(e, \theta)$):** 工人获得教育水平 $e$
    所需的成本，该成本也取决于工人的类型 $\theta$。 关键的成本函数假设：

    -   $C(0, \theta) = 0$: 不受教育则成本为零。

    -   $C_e(e, \theta) = \frac{\partial C(e, \theta)}{\partial e} > 0$:
        教育的边际成本为正，即获得更高教育水平需要付出更多成本。

    -   $C_{ee}(e, \theta) = \frac{\partial^2 C(e, \theta)}{\partial e^2} > 0$:
        教育的边际成本递增（可选假设，但图中曲线常如此表示）。

    -   $C_\theta(e, \theta) = \frac{\partial C(e, \theta)}{\partial \theta} < 0$:
        对于相同的教育水平 $e$，高生产力工人 ($\theta_H$)
        付出的总成本更低（或至少不更高）。

    -   $C_{e\theta}(e, \theta) = \frac{\partial^2 C(e, \theta)}{\partial e \partial \theta} < 0$:
        这是**关键的单调信号条件 (Spence-Mirrlees Condition 或
        Single-Crossing
        Property)**。它意味着高生产力工人获得教育的*边际成本*更低。也就是说，$\theta_H$
        类型的人增加一个单位教育水平所付出的额外成本，要比 $\theta_L$
        类型的人少。这个性质是分离均衡能够存在的数学基础，因为它使得高生产力者通过选择较高的教育水平来"廉价地"将自己与低生产力者区分开来成为可能。

-   **工人的净效用:** 工资减去教育成本，即
    $U(w, e, \theta) = w - C(e, \theta)$。

## 博弈顺序 (Timing)

1.  自然选择工人的类型
    $\theta \in \{\theta_L, \theta_H\}$。工人知道自己的类型，企业不知道。

2.  工人选择教育水平 $e \ge 0$。

3.  企业观察到工人选择的教育水平 $e$，但不能直接观察到其类型
    $\theta$。企业根据 $e$ 更新对工人类型的信念 $\mu(\theta|e)$。

4.  企业向工人提供工资
    $w(e)$。在竞争性市场中，企业会支付等于工人预期生产力的工资：$w(e) = \mu(\theta_H|e)\theta_H + \mu(\theta_L|e)\theta_L$。

5.  工人获得净效用 $w(e) - C(e, \theta)$。

## 分离均衡 (Separating Equilibrium)

在分离均衡中，不同类型的工人选择不同的教育水平，从而使得企业能够通过观察到的教育水平完美地识别出工人的真实生产力类型。

### 定义与信念

假设存在一个分离均衡，其中：

-   高生产力工人选择教育水平 $e_H$: $e(\theta_H) = e_H$。

-   低生产力工人选择教育水平 $e_L$: $e(\theta_L) = e_L$。

-   且
    $e_H \neq e_L$。通常为了简化分析，假设低生产力工人选择最低的教育水平，即
    $e_L = 0$。

在这种情况下，企业的信念是：

-   如果观察到教育水平 $e_H$，企业相信该工人是高生产力类型
    ($\theta_H$)，因此支付工资 $w(e_H) = \theta_H$。

-   如果观察到教育水平 $e_L$，企业相信该工人是低生产力类型
    ($\theta_L$)，因此支付工资 $w(e_L) = \theta_L$。

-   **非均衡路径信念 (Out-of-Equilibrium Beliefs):**
    如果企业观察到一个既不是 $e_H$ 也不是 $e_L$ 的教育水平 $e'$ (即
    $e' \notin \{e_H, e_L\}$)，企业会如何判断？一个常见的支持分离均衡的信念是"悲观信念"：企业认为任何偏离均衡路径选择
    $e'$ 的工人都是低生产力类型 $\theta_L$，因此支付工资
    $w(e') = \theta_L$。

### 激励相容约束 (Incentive Compatibility Constraints - IC)

为了使上述策略和信念构成一个PBE，每种类型的工人选择其均衡教育水平所获得的净效用，必须不低于其选择其他任何教育水平（包括模仿另一种类型）所能获得的净效用。
假设 $e_L = 0$ 且 $C(0, \theta)=0$。

1.  **高生产力工人 $(\theta_H)$ 的激励相容约束 (IC-H):** $\theta_H$
    型工人选择 $e_H$ 的净效用，不应低于他选择 $e_L=0$ (从而被误认为
    $\theta_L$ 类型并获得工资 $\theta_L$) 的净效用。
    $$\theta_H - C(e_H, \theta_H) \ge \theta_L - C(0, \theta_H)$$ 由于
    $C(0, \theta_H)=0$，上式简化为：
    $$\theta_H - C(e_H, \theta_H) \ge \theta_L \implies C(e_H, \theta_H) \le \theta_H - \theta_L$$
    这意味着高生产力工人愿意为获得工资差额 $(\theta_H - \theta_L)$
    而支付的教育成本 $C(e_H, \theta_H)$ 不能超过这个差额。这条约束定义了
    $e_H$ 的一个上限（对应笔记图中分离均衡部分的 $e_2$）。

2.  **低生产力工人 $(\theta_L)$ 的激励相容约束 (IC-L):** $\theta_L$
    型工人选择 $e_L=0$ 的净效用，不应低于他模仿 $\theta_H$ 型工人选择
    $e_H$ (从而被误认为是 $\theta_H$ 类型并获得工资 $\theta_H$)
    的净效用。
    $$\theta_L - C(0, \theta_L) \ge \theta_H - C(e_H, \theta_L)$$ 由于
    $C(0, \theta_L)=0$，上式简化为：
    $$\theta_L \ge \theta_H - C(e_H, \theta_L) \implies C(e_H, \theta_L) \ge \theta_H - \theta_L$$
    这意味着低生产力工人模仿高生产力工人选择 $e_H$ 所需的成本
    $C(e_H, \theta_L)$ 必须足够高，使得他不值得去模仿以获取工资差额
    $(\theta_H - \theta_L)$。这条约束定义了 $e_H$
    的一个下限（对应笔记图中分离均衡部分的 $e^*$，有时也记为 $e_1$）。

因此，任何满足 $C(e_H, \theta_L) \ge \theta_H - \theta_L$ 且
$C(e_H, \theta_H) \le \theta_H - \theta_L$ 的教育水平 $e_H$（以及
$e_L=0$）都可以构成一个分离均衡。 由于 $C_{e\theta} < 0$（即
$C(e, \theta_L)$ 曲线比 $C(e, \theta_H)$
曲线更陡峭），总能找到满足这两个条件的 $e_H$ 区间。

### 成本最低的分离均衡 (Least-Cost Separating Equilibrium)

在所有可能的分离均衡中，对社会福利（或至少对高能力工人）而言最优的是成本最低的分离均衡。这发生在
$e_H$ 取其可能的最小值时，即刚好满足低能力者不愿模仿的条件：
$$e_H^* \text{ 使得 } C(e_H^*, \theta_L) = \theta_H - \theta_L$$
同时，这个 $e_H^*$
也必须满足高能力者的IC约束：$C(e_H^*, \theta_H) \le \theta_H - \theta_L$。由于
$C(e_H^*, \theta_H) < C(e_H^*, \theta_L)$ （因为 $\theta_H > \theta_L$
且 $C_\theta < 0$ 或者更直观地从 $C_{e\theta}<0$
导致两条成本曲线分离），这个条件通常是满足的。 笔记图中的 $e^*$
(对应分离均衡图中的 $e^*$) 就是这样一个点。

## 混同均衡 (Pooling Equilibrium)

在混同均衡中，所有类型的工人都选择相同的教育水平
$e^*$。因此，企业在观察到 $e^*$ 后，无法区分工人的真实类型。

### 定义与信念

假设存在一个混同均衡，其中：

-   所有类型的工人（$\theta_H$ 和 $\theta_L$）都选择相同的教育水平
    $e^*$: $e(\theta_H) = e(\theta_L) = e^*$。

在这种情况下，企业的信念是：

-   如果观察到教育水平 $e^*$：企业知道工人是 $\theta_H$ 类型的先验概率为
    $\lambda$，是 $\theta_L$ 类型的先验概率为
    $1-\lambda$。由于无法区分，企业支付的工资是工人的期望生产力：
    $$w(e^*) = \lambda\theta_H + (1-\lambda)\theta_L$$

-   **非均衡路径信念:** 如果企业观察到任何不同于 $e^*$ 的教育水平
    $e' \neq e^*$，一个常见的支持混同均衡的信念（同样是"悲观信念"）是：企业认为选择
    $e'$ 的工人是低生产力类型 $\theta_L$，因此支付工资
    $w(e') = \theta_L$。

### 激励相容约束 (IC)

为了使这种策略和信念构成PBE，每种类型的工人都必须觉得选择 $e^*$
并获得平均工资 $w(e^*)$，比偏离到其他教育水平（例如选择 $e=0$ 并被认为是
$\theta_L$ 类型）要好。我们考虑偏离到 $e=0$ 的情况，此时 $C(0,\theta)=0$
且获得的工资为 $\theta_L$（根据非均衡路径信念）。

1.  **高生产力工人 $(\theta_H)$ 的激励相容约束 (IC-H):** $\theta_H$
    型工人在混同均衡中选择 $e^*$ 的净效用，不应低于他偏离选择 $e=0$
    的净效用。
    $$\lambda\theta_H + (1-\lambda)\theta_L - C(e^*, \theta_H) \ge \theta_L - C(0, \theta_H)$$
    由于 $C(0, \theta_H)=0$，上式简化为：
    $$\lambda\theta_H + (1-\lambda)\theta_L - C(e^*, \theta_H) \ge \theta_L$$
    $$\implies \lambda(\theta_H - \theta_L) \ge C(e^*, \theta_H)$$
    这意味着高生产力工人在混同均衡中获得的相对于被单独识别为低生产力者的工资溢价
    $\lambda(\theta_H - \theta_L)$，必须足以补偿其为达到混同教育水平
    $e^*$ 所付出的成本 $C(e^*, \theta_H)$。这定义了 $e^*$
    的一个上限（对应笔记图中混同均衡部分的 $e_4$）。

2.  **低生产力工人 $(\theta_L)$ 的激励相容约束 (IC-L):** $\theta_L$
    型工人在混同均衡中选择 $e^*$ 的净效用，不应低于他偏离选择 $e=0$
    的净效用。
    $$\lambda\theta_H + (1-\lambda)\theta_L - C(e^*, \theta_L) \ge \theta_L - C(0, \theta_L)$$
    由于 $C(0, \theta_L)=0$，上式简化为：
    $$\lambda\theta_H + (1-\lambda)\theta_L - C(e^*, \theta_L) \ge \theta_L$$
    $$\implies \lambda(\theta_H - \theta_L) \ge C(e^*, \theta_L)$$
    这意味着低生产力工人在混同均衡中获得的相对于其基础工资 $\theta_L$
    的工资溢价
    $\lambda(\theta_H - \theta_L)$，也必须足以补偿其为达到混同教育水平
    $e^*$ 所付出的成本 $C(e^*, \theta_L)$。这定义了 $e^*$
    的一个上限（对应笔记图中混同均衡部分的 $e_3$）。

由于 $C(e^*, \theta_L) > C(e^*, \theta_H)$ (对于
$e^*>0$)，所以低生产力工人的IC约束通常更为严格 (即 $e_3 < e_4$
如果两者都为正)。因此，混同均衡能够存在的条件主要是由低生产力工人的IC约束决定的：$e^*$
必须满足
$\lambda(\theta_H - \theta_L) \ge C(e^*, \theta_L)$。如果混同的教育成本
$e^*$ 对于低生产力者来说太高，他宁愿选择 $e=0$ 并接受 $\theta_L$
的工资。

一个常见的混同均衡是
$e^*=0$。此时，所有工人都不接受教育，企业支付平均工资
$\lambda\theta_H + (1-\lambda)\theta_L$。这个均衡总是存在的，只要非均衡路径信念设定为：任何
$e>0$ 的教育水平都被认为是 $\theta_L$
类型发出的（这是一种极度悲观但允许的信念，足以阻止任何人偏离 $e^*=0$）。

# 逆向选择 (Adverse Selection)

逆向选择指的是在交易中，一方拥有另一方所不具备的信息，从而可能导致市场出现低效率甚至失灵的情况。这里主要讨论的是劳动市场。

## 基本设定

-   **生产力 (productivity) $\theta$**: 代表工人的生产能力，分布在区间
    $[\underline{\theta}, \bar{\theta}]$，其分布函数为
    $F(\theta)$，密度函数为 $f(\theta)$。

-   **保留工资 (reservation wage) $r(\theta)$**:
    $\theta$类型的工人愿意接受的最低工资。通常假设
    $r'(\theta) > 0$，即生产力越高的工人，其保留工资也越高。

-   **企业提供的工资 $W$**: 企业提供一个统一的工资 $W$，所有满足
    $r(\theta) \le W$ 的工人都会被吸引并前来应聘。

## 图示解释 (根据图片描述)

图片的左上角图示了生产力 $\theta$ (横轴)与工资 $W$
(纵轴)之间的关系。$r(\theta)$ 是一条向上倾斜的曲线/直线。当企业提供工资
$W$ 时，所有保留工资低于或等于 $W$ 的工人（即图中 $r(\theta)$ 曲线上 $W$
水平线以下的对应 $\theta$ 区间，记为
$\{\theta | r(\theta) \le W\}$）会接受这份工作。企业无法直接观察到每个应聘者的真实生产力
$\theta$。

## 竞争性均衡 (Competitive Equilibrium)

在竞争性市场中，企业的期望利润为零。这意味着企业支付的工资 $W$
应该等于它雇佣到的工人的平均生产力。均衡条件为：
$$W = E[\theta | r(\theta) \le W]$$
即企业支付的工资，等于所有接受该工资的工人的期望生产力。

## 例子1 (市场失灵 - Market Breakdown)

假设 $r(\theta) = \frac{2}{3}\theta$，且 $\theta \sim U(0,1)$
(在0到1之间均匀分布)。 那么，工人接受工资 $W$ 的条件是
$\frac{2}{3}\theta \le W \Rightarrow \theta \le \frac{3}{2}W$。
因此，企业雇佣的工人的期望生产力为：
$$E[\theta | r(\theta) \le W] = E\left[\theta \middle| \theta \le \frac{3}{2}W\right]$$
假设 $\frac{3}{2}W \le 1$ (即
$W \le \frac{2}{3}$，这样工人的生产力上限仍在1以内)。由于
$\theta \sim U(0,1)$，那么：
$$E\left[\theta \middle| \theta \le \frac{3}{2}W\right] = \frac{0 + \frac{3}{2}W}{2} = \frac{3}{4}W$$
在均衡时，$W = E[\theta | r(\theta) \le W]$，所以： $$W = \frac{3}{4}W$$
这只有在 $W=0$
时成立。这意味着市场上唯一可能的均衡工资是0，此时没有工人愿意工作，市场崩溃。

## 例子2 (多重均衡)

假设 $r(\theta) = \frac{1}{3}\theta$，且 $\theta \sim U(0,1)$。
工人接受工资 $W$ 的条件是
$\frac{1}{3}\theta \le W \Rightarrow \theta \le 3W$。

-   **情况一：若 $3W \le 1$ (即 $W \le \frac{1}{3}$)**
    $$E[\theta | \theta \le 3W] = \frac{3W}{2}$$ 均衡时
    $W = \frac{3W}{2} \Rightarrow W=0$。

-   **情况二：若 $3W > 1$ (即 $W > \frac{1}{3}$)**
    这意味着所有类型的工人 ($\theta \in [0,1]$) 都会被吸引。此时：
    $$E[\theta | \theta \in [0,1]] = \frac{0+1}{2} = \frac{1}{2}$$
    均衡时 $W = \frac{1}{2}$。这个均衡是成立的，因为 $W=\frac{1}{2}$
    满足 $W > \frac{1}{3}$ 的条件。

因此，这个例子中存在两个均衡：$W=0$ 和 $W=\frac{1}{2}$。

## 图示解释 (根据图片左下角描述 - 多重均衡)

该图的横轴是 $\theta$ (或代表平均生产力的某种指标)，纵轴是 $W$。曲线
$E[\theta | r(\theta) \le W]$ 表示给定工资 $W$
时，企业能吸引到的工人的平均生产力。$45^\circ$ 线表示
$W = E[\theta | r(\theta) \le W]$，即均衡点。图中可能标出多个交点
$E_1, E_2, E_3, E_4$，这些都是潜在的均衡。

### 稳定性 (Stability)

-   如果 $E[\theta | \dots]$ 曲线从上方穿过 $45^\circ$ 线（如
    $E_2, E_4$），则该均衡是局部稳定的。

-   如果 $E[\theta | \dots]$ 曲线从下方穿过 $45^\circ$ 线（如
    $E_3$），则该均衡是不稳定的。

图片注释提到 \"$E_4$ is the only equ for dynamic game where firms first
make wage offers then workers choose the best
offer\"，指出在特定动态博弈中 $E_4$ 可能是最终结果。

# 逆向选择 ($r'(\theta) < 0$) 和 伯特兰竞争

## 逆向选择 ($r'(\theta) < 0$)

这是一个特殊情况，即生产力越高的工人，其保留工资反而越低。图中
$r(\theta)$ 曲线向下倾斜。企业提供工资 $W$ 时，会吸引 $r(\theta) \le W$
的工人，这意味着会吸引生产力 $\theta$ 高于某个阈值的工人。

## 寡头垄断 (Oligopoly)

指市场上只有少数几家公司（寡头）进行竞争。这些公司的决策会相互影响。

## 价格竞争 (Bertrand Competition - 伯特兰竞争)

### 核心思想

企业通过制定价格来进行竞争。

### 基本假设

-   至少有两家企业。

-   产品同质 (homogeneous product)。

-   企业同时选择价格 $p_1, p_2$。

-   消费者从价格较低的企业购买。若价格相同，则市场需求平均分配。

-   边际生产成本 (marginal cost) 为常数 $c$。即 $MC = c$。

### 逻辑

如果一家企业的价格 $p_i > c$，竞争对手可以通过定一个略低于 $p_i$
但仍高于 $c$
的价格来抢占所有市场份额并获得正利润。这种削价竞争会一直持续下去，直到价格被压低到边际成本的水平。

### 均衡结果

$$p_1 = p_2 = \dots = p_N = MC = c$$
在伯特兰均衡中，所有企业都将价格定在边际成本水平，经济利润为零。这个结果与完全竞争市场相同。
图片中图示了水平的边际成本线 $c$，伯特兰均衡价格 $P=c$，而垄断价格 $P^m$
通常高于 $c$。

### 例子与注释

图片中提到两家企业 $mc_1=3, mc_2=3$，则均衡价格 $p_1=p_2=3$。 关于
\"$p1=5, p2=5-\epsilon$ 加价获利 (若 price 连续) No Nash equ. unless
$P_1^m < 5 \Rightarrow P_2 = P_1^m, P_1=5$\"
的注释，是在特定条件下对标准伯特兰模型的讨论或扩展，可能涉及非对称成本或垄断势力。

# 古诺竞争、斯塔克尔伯格模型、线性城市模型

## 数量竞争 (Cournot Competition - 古诺竞争)

### 核心思想

企业通过选择产量来进行竞争。

### 基本假设

-   至少有两家企业 (设为企业1和企业2)。

-   企业同时决定各自的产量 $q_1, q_2$。

-   市场价格 $P$ 由市场总产量 $Q = q_1 + q_2$
    决定。线性需求函数为：$P(Q) = a - bQ = a - b(q_1+q_2)$。

-   边际成本相同且为常数，$mc_1 = mc_2 = c$。

### 企业1的利润最大化问题

企业1的利润函数为：
$$\pi_1(q_1, q_2) = P(q_1+q_2) \cdot q_1 - c q_1 = [a - b(q_1+q_2)]q_1 - c q_1$$

### 一阶条件 (FOC)

企业1选择 $q_1$ 使其利润最大化，对 $q_1$ 求导并令其为0:
$$\frac{\partial \pi_1}{\partial q_1} = a - 2bq_1 - bq_2 - c = 0$$

### 反应函数 (Reaction Function) $R_1(q_2)$

从一阶条件中解出 $q_1$: $$q_1 = R_1(q_2) = \frac{a-c-bq_2}{2b}$$
这是企业1在给定 $q_2$ 时的最优产量。

### 对称性与古诺均衡

由于企业对称，企业2的反应函数为 $q_2 = R_2(q_1) = \frac{a-c-bq_1}{2b}$。
古诺均衡是 $(q_1^*, q_2^*)$ 满足 $q_1^* = R_1(q_2^*)$ 和
$q_2^* = R_2(q_1^*)$。 在对称情况下，$q_1^* = q_2^* = q^*$，解得：
$$q^* = \frac{a-c}{3b}$$ 总产量 $Q^* = 2q^* = \frac{2(a-c)}{3b}$。
均衡价格 $P^* = a - bQ^* = \frac{a+2c}{3}$。

### 图示

图片显示两条向下倾斜的反应函数，其交点即为古诺-纳什均衡
$(q_1^*, q_2^*)$。

## 斯塔克尔伯格模型 (Stackelberg Model)

### 核心思想

数量竞争，但企业决策有先后顺序：领导者 (leader) 和 跟随者 (follower)。

### 基本假设

-   两家企业，边际成本 $mc_1=mc_2=c$。需求函数 $P = a-bQ$。

-   企业1 (领导者) 首先选择其产量 $q_1$。

-   企业2 (跟随者) 观察到 $q_1$ 后，再选择其产量 $q_2$。

### 求解方法：逆向归纳法 (Backward Induction)

1.  **第二阶段 (Stage 2 - 跟随者的决策)**: 企业2在观察到 $q_1$ 后，选择
    $q_2$ 最大化其利润。其反应函数与古诺模型中相同：
    $$q_2(q_1) = R_2(q_1) = \frac{a-c-bq_1}{2b}$$ (图片中显示
    $q_2 = \frac{a-bq_1-C}{2b}$，若 $C$ 代表边际成本，则一致)。

2.  **第一阶段 (Stage 1 - 领导者的决策)**: 企业1预期到企业2的反应，选择
    $q_1$ 以最大化其利润，将 $R_2(q_1)$ 代入 $\pi_1$:
    $$\pi_1(q_1) = \left[a-b\left(q_1 + \frac{a-c-bq_1}{2b}\right)\right]q_1 - c q_1$$
    根据图片中的简化形式（扣除成本后的利润表达式）：
    $\pi_1(q_1) = \left(\frac{a-c}{2} - \frac{b}{2}q_1\right)q_1 = \frac{1}{2}(a-c)q_1 - \frac{b}{2}q_1^2$
    (更完整的推导是
    $\pi_1 = (P(q_1+R_2(q_1)) - c)q_1 = (\frac{a-c-bq_1}{2})q_1$)

    对 $\pi_1$ 关于 $q_1$ 求一阶条件并令其为0:
    $$\frac{d\pi_1}{dq_1} = \frac{1}{2}(a-c) - bq_1 = 0$$
    解得领导者产量： $$q_1^S = \frac{a-c}{2b}$$ 将 $q_1^S$
    代入企业2的反应函数，得到跟随者的产量：
    $$q_2^S = \frac{a-c-b\left(\frac{a-c}{2b}\right)}{2b} = \frac{a-c-\frac{a-c}{2}}{2b} = \frac{a-c}{4b}$$

### 结果比较

-   领导者产量 $q_1^S = \frac{a-c}{2b}$ (高于古诺产量)。

-   跟随者产量 $q_2^S = \frac{a-c}{4b}$ (低于古诺产量)。

-   总产量 $Q^S = q_1^S + q_2^S = \frac{3(a-c)}{4b}$ (高于古诺总产量)。

-   领导者有先发优势，利润更高。

图片中注释古诺均衡产量 $q_1^* = q_2^* = \frac{a-c}{3b}$。

## 线性城市模型 (Linear City Model - Hotelling Model)

### 核心思想

考虑空间差异和运输成本的企业竞争模型。

### 基本设定

-   消费者均匀分布在一条长度为1的线段上 (例如从0到1)。

-   两家企业，企业1位于0点 (Firm 1)，企业2位于1点 (Firm 2)。

-   企业边际成本 $mc_1=mc_2=c$。

-   消费者购买产品需要支付运输成本，每单位距离的运输成本为 $t$。位于 $x$
    处的消费者到企业1的运输成本为 $tx$，到企业2的运输成本为 $t(1-x)$。

-   每个消费者购买一单位产品。$V$ 是产品的价值。

### 消费者的决策

位于 $\hat{x}$ 的消费者在企业1和企业2之间无差异
(indifferent)，如果他从两家企业购买的总成本（价格+运输成本）相同：
$$P_1 + t\hat{x} = P_2 + t(1-\hat{x})$$ 解出市场分界点 $\hat{x}$
(即企业1的需求，假设总消费者数量为1)：
$$P_1 + t\hat{x} = P_2 + t - t\hat{x}$$ $$2t\hat{x} = P_2 - P_1 + t$$
$$\hat{x} = \frac{P_2 - P_1 + t}{2t} = \frac{P_2 - P_1}{2t} + \frac{1}{2}$$
企业1的需求 $D_1 = \hat{x}$。企业2的需求 $D_2 = 1-\hat{x}$。

# 线性城市模型 (续)

## 假设

$V$ 足够高，以至于所有消费者都会购买产品 ($U(x) > 0$ for all
$x \in [0,1]$)。企业同时定价 (simultaneous price setting)，且
$mc_1=mc_2=c$。

## 企业1的利润最大化

$$\pi_1(P_1, P_2) = \hat{x}(P_1 - c) = \left(\frac{P_2 - P_1}{2t} + \frac{1}{2}\right)(P_1 - c)$$
对 $P_1$ 求一阶导数并令其为0 (FOC):
$$\frac{\partial \pi_1}{\partial P_1} = -\frac{1}{2t}(P_1-c) + \left(\frac{P_2 - P_1}{2t} + \frac{1}{2}\right) = 0$$
图片中整理后得到 (将 $\frac{1}{2}$ 写作 $\frac{t}{2t}$ 并合并同类项)：
$$\frac{-P_1+c}{2t} + \frac{P_2-P_1+t}{2t} = 0 \quad (\text{图片中为 } -\frac{1}{2t}(P_1-c) + \frac{P_2-P_1}{2t} + \frac{1}{2}=0)$$
$$-P_1+c + P_2-P_1+t = 0$$ $$2P_1 = P_2 + c + t$$
这是企业1对企业2价格的反应函数 $P_1(P_2)$。

## 企业2的利润最大化

企业2的需求是
$D_2 = 1-\hat{x} = 1 - \left(\frac{P_2 - P_1}{2t} + \frac{1}{2}\right) = \frac{P_1 - P_2 + t}{2t}$。
$$\pi_2(P_1, P_2) = (1-\hat{x})(P_2 - c) = \left(\frac{P_1 - P_2 + t}{2t}\right)(P_2 - c)$$
对 $P_2$ 求一阶导数并令其为0 (FOC):
$$\frac{\partial \pi_2}{\partial P_2} = -\frac{1}{2t}(P_2-c) + \frac{P_1 - P_2 + t}{2t} = 0$$
$$-P_2+c + P_1-P_2+t = 0$$ $$2P_2 = P_1 + c + t$$
这是企业2对企业1价格的反应函数 $P_2(P_1)$。

## 对称均衡 (Symmetric Equilibrium)

由于两家企业是对称的，在一个纳什均衡中它们会定相同的价格，即
$P_1^* = P_2^* = P^*$。 将 $P_1 = P_2 = P$
代入任何一个反应函数，例如企业1的反应函数 $2P_1 = P_2 + c + t$:
$$2P^* = P^* + c + t$$ $$P^* = c + t$$ 所以，均衡时两家企业的价格都是
$P_1^* = P_2^* = c+t$。

在这个模型中，企业会收取高于边际成本的价格。价格加成 (markup)
等于运输成本参数 $t$。运输成本 $t$
越大，产品因地理位置带来的差异化程度越高，企业就拥有更大的定价权。如果
$t \to 0$，则 $P^* \to c$，结果趋向于伯特兰竞争的结果。


=== File: ./Lessons/lesson9.qmd ===
---
title: "2: 大语言模型形式化与非形式化推理"
---

TBD

=== File: ./Lessons/test.py ===
import requests
from bs4 import BeautifulSoup, NavigableString, Tag

url = 'https://lyubh.cn'

try:
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Name
    name = soup.find('h1').text.strip() if soup.find('h1') else 'Name not found'

    # Introduction
    intro_section = soup.find('p')
    intro_text = intro_section.text.strip() if intro_section else ''

    # News
    news_items = []
    news_ul = soup.find('h2', id='News')
    if news_ul:
        news_ul = news_ul.find_next('ul')
        if news_ul:
            for li in news_ul.find_all('li'):
                news_items.append(li.text.strip())

    # Publications with full authors and venue
    publications = []
    for tr in soup.find_all('tr', class_='paper-row'):
        h3 = tr.find('h3')
        if h3:
            title = h3.text.strip()

            # 下面拼接所有作者节点（包括<b>、普通文本等），直到遇到<em>或者<p>为止
            authors = []
            node = h3.next_sibling
            while node:
                if isinstance(node, Tag) and node.name in ['em', 'p']:
                    break
                # 跳过换行等
                if isinstance(node, NavigableString):
                    t = node.strip()
                    if t:
                        authors.append(t)
                elif isinstance(node, Tag):
                    # 例如<b>Bohan Lyu</b>或其它标签
                    t = node.get_text(strip=True)
                    if t:
                        authors.append(t)
                node = node.next_sibling

            authors_str = ' '.join(authors).replace(' ,', ',').replace('  ', ' ').replace('\n', '').strip()
            # venue
            venue_elem = tr.find('em')
            venue = venue_elem.text.strip() if venue_elem else ''
            publications.append({
                'title': title,
                'authors': authors_str,
                'venue': venue
            })

    # 输出
    print(f"Name: {name}")
    print(f"\nIntroduction: {intro_text}")

    print("\nRecent News:")
    for item in news_items[:3]:
        print(f"- {item}")

    print("\nSelected Publications:")
    for i, pub in enumerate(publications[:3], 1):
        print(f"{i}. {pub['title']}")
        print(f"    Authors: {pub['authors']}")
        print(f"    Venue: {pub['venue']}\n")

except requests.exceptions.RequestException as e:
    print(f"Error: Could not connect to website {url}. Reason: {e}")

=== File: ./Lessons/lesson8.qmd ===
---
title: "1: 量化金融"
---

TBD

=== File: ./Lessons/lesson5.qmd ===
---
title: "2: Python爬虫原理与实践"
---

## 第一章：爬虫基础与核心原理再探

在我们开始写代码之前，必须先稳固地掌握爬虫的内在逻辑。

### 什么是爬虫？

网络爬虫，本质上是一个**自动化**的程序。它的核心任务是**模拟人类访问网页的行为**，但速度和规模远超人类。

  * **人类访问网页**：打开浏览器 -\> 输入网址 (URL) -\> 回车 -\> 浏览器发送请求 -\> 服务器返回HTML代码 -\> 浏览器将代码**渲染**成我们看到的图文并茂的页面。
  * **爬虫访问网页**：执行程序 -\> 指定URL -\> 程序发送请求 -\> 服务器返回HTML代码 -\> 程序**不渲染**，而是直接**解析**这份纯文本的HTML代码，寻找并提取数据。

关键区别在于**“渲染”**和**“解析”**。浏览器负责美观地展示，而爬虫负责高效地提取。

### HTTP：爬虫与网站沟通的语言

你的爬虫程序通过HTTP协议与网站服务器对话。最常见的两种对话方式是 `GET` 和 `POST`。

  * **`GET` 请求**：就像在浏览器地址栏输入网址后回车。这是最常见的请求，用于**获取**（GET）网页数据。我们的绝大多数抓取任务都从`GET`请求开始。
  * **`POST` 请求**：通常用于向服务器**提交**（POST）数据，例如填写登录表单、提交搜索关键词等。有些需要登录或搜索后才能看到内容的页面，就需要模拟`POST`请求。

### 网页的骨架：HTML简介

爬虫获取到的就是一堆HTML（超文本标记语言）代码。你需要能看懂它的基本结构，才能知道去哪里找数据。

```html
<!DOCTYPE html>
<html>
<head>
    <title>网页标题</title>
</head>
<body>
    <div class="content">
        <h1>这是一个主标题</h1>
        <p id="intro">这是一个段落。</p>
        <a href="/about.html">关于我们</a>
    </div>
</body>
</html>
```

  * **标签 (Tag)**：由尖括号包围，如`<html>`, `<h1>`, `<p>`, `<a>`。它们定义了内容的类型和结构。
  * **属性 (Attribute)**：在标签内部，提供额外信息，如`class="content"`，`id="intro"`，`href="/about.html"`。**属性是我们定位数据的关键线索**。

## 第二章：环境准备与第一个静态爬虫

现在，让我们卷起袖子，开始动手！

### 2.1 搭建你的爬虫工作室

1.  **安装 Python**: 确保你的电脑上安装了Python 3。
2.  **安装必备库**: 打开你的终端或命令行工具，安装我们即将使用的两个核心库。

    ```bash
    pip install requests
    pip install beautifulsoup4
    ```

      * `requests`: 负责发送HTTP请求，从网站获取HTML。
      * `beautifulsoup4`: 负责解析HTML，帮我们轻松提取数据。

### 2.2 实战：抓取学者主页上的信息

**目标**：自动抓取学者主页（如 [https://lyubh.cn](https://lyubh.cn)）上的姓名、简介、新闻动态，以及每一篇论文的标题、作者和会议。

#### 第一步：分析目标网页（侦察工作）

在浏览器中打开目标网页，右键点击你想要爬取的内容，选择“检查”或“审查元素”。你会看到类似这样的 HTML 结构：

```html
<!-- 姓名 -->
<h1>Bohan Lyu</h1>

<!-- 简介 -->
<p>
  I am an undergruduate at Tsinghua University. I'm interested in ML and NLP topics. My works are published in ICML and ACL.
</p>

<!-- 新闻 -->
<h2 id="News">News</h2>
<ul>
  <li>2025-07 Goedel-Prover is accepted to COLM 2025!</li>
  ...
</ul>

<!-- 论文条目，title、作者、会议分布在同一个<tr>里 -->
<tr class="paper-row">
  <td>...</td>
  <td>
    <div>
      <h3>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving</h3>
      <br>
      Yong Lin*, Shange Tang*, <b>Bohan Lyu</b>, Jiayun Wu, ...
      <br>
      <em>COLM 2025</em>
      ...
    </div>
  </td>
</tr>
```

**侦察结论**：

- 姓名在 `<h1>` 标签里。
- 简介在第一个 `<p>` 标签里。
- 新闻在 `<h2 id="News">` 后的 `<ul>` 里。
- 论文信息在 `<tr class="paper-row">` 里，标题是 `<h3>`，作者是 `<h3>` 下方的所有文本，会议在 `<em>` 内。

**第二步：编写Python代码**

```{python}
import requests
from bs4 import BeautifulSoup, NavigableString, Tag

url = 'https://lyubh.cn'

try:
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Name
    name = soup.find('h1').text.strip() if soup.find('h1') else 'Name not found'

    # Introduction
    intro_section = soup.find('p')
    intro_text = intro_section.text.strip() if intro_section else ''

    # News
    news_items = []
    news_ul = soup.find('h2', id='News')
    if news_ul:
        news_ul = news_ul.find_next('ul')
        if news_ul:
            for li in news_ul.find_all('li'):
                news_items.append(li.text.strip())

    # Publications with full authors and venue
    publications = []
    for tr in soup.find_all('tr', class_='paper-row'):
        h3 = tr.find('h3')
        if h3:
            title = h3.text.strip()

            # 下面拼接所有作者节点（包括<b>、普通文本等），直到遇到<em>或者<p>为止
            authors = []
            node = h3.next_sibling
            while node:
                if isinstance(node, Tag) and node.name in ['em', 'p']:
                    break
                # 跳过换行等
                if isinstance(node, NavigableString):
                    t = node.strip()
                    if t:
                        authors.append(t)
                elif isinstance(node, Tag):
                    t = node.get_text(strip=True)
                    if t:
                        authors.append(t)
                node = node.next_sibling

            authors_str = ' '.join(authors).replace(' ,', ',').replace('  ', ' ').replace('\n', '').strip()
            # venue
            venue_elem = tr.find('em')
            venue = venue_elem.text.strip() if venue_elem else ''
            publications.append({
                'title': title,
                'authors': authors_str,
                'venue': venue
            })

    # 输出
    print(f"Name: {name}")
    print(f"\nIntroduction: {intro_text}")

    print("\nRecent News:")
    for item in news_items[:3]:
        print(f"- {item}")

    print("\nSelected Publications:")
    for i, pub in enumerate(publications[:3], 1):
        print(f"{i}. {pub['title']}")
        print(f"    Authors: {pub['authors']}")
        print(f"    Venue: {pub['venue']}\n")

except requests.exceptions.RequestException as e:
    print(f"Error: Could not connect to website {url}. Reason: {e}")
```

## 第三章：进阶爬取技术

抓取单个页面只是开始，真正的威力在于处理列表和多个页面。

### 3.1 爬取列表页数据（“爬列表”）

**目标**：抓取一个虚构的图书商店（`http://books.toscrape.com` - 这是一个真实的、为爬虫练习而生的网站）第一页所有书的标题和价格。

**侦察工作**：
打开网站，用开发者工具检查一本书。你会发现，每一本书的信息都在一个`<article class="product_pod">`标签里。书名在`<h3>`标签的`<a>`标签里，价格在`<p class="price_color">`标签里。

**代码实现**：

```{python}
import requests
from bs4 import BeautifulSoup
import csv # 引入csv库，用于保存数据

# 目标URL
url = 'http://books.toscrape.com/'

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# 1. 定位所有包含书籍信息的article标签
# find_all()会返回一个包含所有匹配项的列表
books = soup.find_all('article', class_='product_pod')

book_data = [] # 创建一个列表来存储所有书籍信息

# 2. 循环处理每一本书
for book in books:
    # 在每个book标签内继续查找标题和价格
    # 注意这里的路径查找方式
    title = book.h3.a['title']
    price = book.find('p', class_='price_color').text
    
    # 打印出来看看
    print(f"书名: {title}, 价格: {price}")
    
    # 将提取的数据存入字典，再添加到列表中
    book_data.append({'title': title, 'price': price})

print(book_data)
```

### 3.2 循环爬取多个页面（“循环爬”/翻页）

**目标**：抓取`books.toscrape.com`前5页所有书的信息。

**侦察工作**：
在第一页底部，找到“next”按钮。检查它的HTML，你会看到一个`<a>`标签，它的`href`属性指向下一页的地址（`catalogue/page-2.html`）。这给了我们构建下一页URL的规律。

**代码实现**：

```{python}
import requests
from bs4 import BeautifulSoup
import time # 引入时间库，用于设置延时

base_url = 'http://books.toscrape.com/catalogue/'
all_book_data = []

# 循环爬取前3页
for i in range(1, 4): # 从第1页到第3页
    # 构造每一页的完整URL
    url = f"{base_url}page-{i}.html"
    print(f"正在抓取页面: {url}")
    
    response = requests.get(url)
    
    # 如果页面不存在，则跳出循环
    if response.status_code != 200:
        print(f"页面 {url} 不存在，停止抓取。")
        break
        
    soup = BeautifulSoup(response.text, 'html.parser')
    books = soup.find_all('article', class_='product_pod')

    for book in books:
        title = book.h3.a['title']
        price = book.find('p', class_='price_color').text
        all_book_data.append({'title': title, 'price': price})
    
    # 做一个有礼貌的爬虫，每次请求后暂停一下
    print(f"页面 {i} 抓取完毕，暂停1秒...")
    time.sleep(1) 

print(f"\n全部 {len(all_book_data)} 本书的信息抓取完毕！")

print(all_book_data)
```

## 第四章：攻克动态网站（“动态爬”）

**挑战**：很多现代网站使用JavaScript在页面加载后才动态地载入数据。你用`requests`直接请求，只能拿到一个空壳HTML，数据根本不在里面。

**例子**：一个股价实时更新的页面，或者无限滚动的社交媒体。

**解决方案**：使用**浏览器自动化工具**，如`Selenium`。

`Selenium`可以驱动一个真实的浏览器（如Chrome或Firefox）去加载网页。它会等待所有JavaScript执行完毕，渲染出最终的页面，然后我们再从这个完整的页面中提取数据。

### 4.1 安装与配置 Selenium

1.  **安装 Selenium 库**:

    ```bash
    pip install selenium
    ```

2.  **下载 WebDriver**: `Selenium`需要一个叫做`WebDriver`的驱动程序来控制浏览器。你需要下载与你的**浏览器版本完全对应**的`WebDriver`。
      * **Chrome用户**: 搜索 "ChromeDriver" 下载。
      * **Firefox用户**: 搜索 "GeckoDriver" 下载。
      * 下载后，将`chromedriver.exe`（或`geckodriver.exe`）放到你的Python脚本所在的文件夹，或者一个系统路径下。

### 4.2 实战：抓取动态加载的数据

**目标**：抓取一个虚构的、由JS加载名言的网站 (`http://quotes.toscrape.com/js/`) 的第一页所有名言。

**侦察工作**：
如果你直接用`requests`请求这个URL，会发现返回的HTML里根本没有名言数据。但用浏览器打开，名言却清晰可见。这就是JS动态加载的证据。

**代码实现**：

```{python}
#| eval: false
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

# --- Selenium 设置 ---
# 指定WebDriver的路径（如果不在系统路径下）
# driver_path = 'path/to/your/chromedriver.exe'
# driver = webdriver.Chrome(executable_path=driver_path)
driver = webdriver.Chrome() # 假设chromedriver在PATH中

url = 'http://quotes.toscrape.com/js/'
driver.get(url)

try:
    # --- 等待动态内容加载 ---
    # 设置一个最长等待时间（10秒）
    # 等待直到class为'quote'的元素出现
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "quote"))
    )

    # 此刻，浏览器中的页面已经完全加载好了
    # 获取渲染后的页面源代码
    html_content = driver.page_source
    
    # --- 使用BeautifulSoup解析 ---
    soup = BeautifulSoup(html_content, 'html.parser')
    
    quotes = soup.find_all('div', class_='quote')
    
    for quote in quotes:
        text = quote.find('span', class_='text').text
        author = quote.find('small', class_='author').text
        print(f"'{text}' - {author}")
        
finally:
    # 完成后务必关闭浏览器，释放资源
    time.sleep(2) # 稍等片刻，方便观察
    driver.quit()

```

**动态爬取小结**：

  * **优点**：能抓取几乎所有“所见即所得”的网页内容，是终极解决方案。
  * **缺点**：速度慢（因为要真实加载整个浏览器），消耗资源多。

**更高效的动态抓取思路（高级）**：
在开发者工具的“Network”标签页下，筛选`XHR`或`Fetch`请求。你往往能找到JS用来获取数据的那个后台API接口。如果能找到这个接口，你就可以直接用`requests`去请求这个API，获取返回的JSON数据，这比启动整个Selenium快得多！

=== File: ./Lessons/lesson4.qmd ===
---
title: "1: Git与GitHub的应用"
---

## Git 与 GitHub 入门指南 

欢迎来到版本控制的世界！本指南将带你深入了解 Git 和 GitHub，从基本概念到日常协作流程，并使用图表让复杂的概念变得一目了然。无论你是编程新手还是想系统学习的开发者，这篇教程都将是你的得力助手。

### **为什么需要版本控制？**

想象一下你正在进行一个复杂的项目，无论是写代码、写论文还是设计作品。你一定会遇到以下问题：

  * **混乱的文件管理**：`项目_v1.zip`, `项目_v2_final.zip`, `项目_final_final_我发誓是最终版.zip`... 这种方式既原始又低效。
  * **无法追溯的错误**：你今天写的代码导致整个项目无法运行，但你不记得昨天做了哪些修改，想恢复到之前的版本变得异常困难。
  * **协作冲突**：你和你的同伴修改了同一个文件，当你们试图合并工作时，一个人的修改覆盖了另一个人的，造成了“代码冲突”的噩梦。

**版本控制系统 (Version Control System, VCS)** 正是为解决这些问题而生的。

**Git** 是当今最流行、最强大的**分布式版本控制系统**。它就像一个精密的时光机和协作中心，能够：

1.  **记录每一次变更**：对文件的每一次增、删、改，Git 都会拍下一张“快照”(snapshot)。
2.  **随时回溯历史**：你可以轻松地将整个项目恢复到过去的任意一个“快照”状态。
3.  **支持并行开发**：通过“分支”功能，团队成员可以在不互相干扰的独立空间里开发新功能。
4.  **高效解决冲突**：当不同人的修改发生冲突时，Git 提供了工具来帮助你清晰地合并它们。

**GitHub** 又是什么呢？
它是一个基于云的**代码托管平台**。如果说 Git 是你本地的“时光机”，那 GitHub 就是这个“时光机”的云端枢纽和社交网络。它提供了：

  * **远程仓库**：一个安全的地方来存储你的代码和所有版本历史。
  * **协作工具**：强大的 Pull Request、Code Review 和 Issue Tracking 功能，让团队协作变得前所未有的高效。
  * **个人名片**：一个展示你技术实力的平台，你的 GitHub 主页就是程序员最好的简历。

**总结：Git 是工具，负责版本控制；GitHub 是平台，负责代码托管与协作。**

### **安装与初次配置**

#### 1\. 安装 Git

  * **Windows**: 访问 [git-scm.com/download/win](https://git-scm.com/download/win) 下载安装包。安装时，建议一路使用默认选项，它会自动安装 `Git Bash`，一个在 Windows 上模拟 Linux 命令行的强大工具。
  * **macOS**:
      * 最简单的方式是安装 Xcode Command Line Tools。打开“终端”(Terminal) 并输入 `xcode-select --install`。
      * 或者使用 [Homebrew](https://brew.sh/) (推荐) 安装：`brew install git`。
  * **Linux (Debian/Ubuntu)**: 打开终端并输入 `sudo apt-get install git`。

安装后，打开终端 (或 Git Bash) 输入 `git --version`，如果看到版本号，说明安装成功。

#### 2\. 配置你的身份

安装 Git 后，第一件事就是设置你的用户名和邮箱。这个身份信息会附加到你的每一次“提交”(commit) 上，让别人知道是谁做的修改。

```bash
# 设置你的 GitHub 用户名
git config --global user.name "Your GitHub Username"

# 设置你的 GitHub 注册邮箱
git config --global user.email "your.email@example.com"
```

`--global` 标志表示这是全局配置，你电脑上所有的 Git 项目都会默认使用这个配置。

### **Git 核心工作流与本地操作**

Git 的核心在于其三个主要区域的流转。理解了这个模型，你就理解了 Git 的一半。

1.  **工作区 (Working Directory)**: 你在电脑上能直接看到和编辑的项目文件夹。
2.  **暂存区 (Staging Area/Index)**: 一个虚拟的区域，用于临时存放你**希望包含在下一次提交中**的变更。它像一个购物车的概念，你可以把修改好的东西一件件放进去，最后统一“结账”。
3.  **本地仓库 (Local Repository)**: `.git` 隐藏文件夹，存放了项目所有的版本历史“快照”。一旦变更被提交到这里，它就永久地记录下来了。

#### **工作流图解**

下面这张图清晰地展示了文件在三个区域之间的流转过程：

```{mermaid}
graph TD
    A["工作区<br>(Working Directory)"] -- "git add" --> B["暂存区<br>(Staging Area)"]
    B -- "git commit" --> C["本地仓库<br>(Local Repository)"]
    C -- "git checkout" --> A
    A -- "编辑文件<br>(Edit files)" --> A
```

#### **实战演练：你的第一个仓库**

1.  **初始化仓库 (`git init`)**

      * 在电脑上创建一个新文件夹，例如 `git-project`。
      * 通过终端进入该文件夹：`cd path/to/git-project`。
      * 执行初始化命令：
        ```bash
        git init
        ```
        这个命令会在当前目录下创建一个名为 `.git` 的子目录，你的本地仓库就此诞生。

2.  **检查状态 (`git status`)**
    `git status` 是你最应该频繁使用的命令。它会告诉你当前仓库的状态：哪些文件被修改了？哪些文件在暂存区？

3.  **创建文件并添加到暂存区 (`git add`)**

      * 在 `git-project` 文件夹中创建一个 `README.md` 文件，并写入 "Hello, Git\!"。
      * 现在运行 `git status`，你会看到 `README.md` 出现在 "Untracked files" (未跟踪文件) 列表中。
      * 使用 `git add` 命令来跟踪这个文件，并将其变更放入暂存区。
        ```bash
        # 添加指定文件到暂存区
        git add README.md
    
        # 如果想添加所有已修改或新增的文件，使用点号
        # git add .
        ```
      * 再次运行 `git status`，你会看到 `README.md` 现在处于 "Changes to be committed" (待提交的变更) 列表中。

4.  **提交变更到仓库 (`git commit`)**
    当暂存区里的内容准备就绪后，就可以使用 `git commit` 将它们“拍摄快照”并存入本地仓库。

    ```bash
    # -m 参数允许你直接在命令行提供提交信息
    git commit -m "Initial commit: Create README.md"
    ```

    **编写好的 Commit Message 至关重要！** 它应该简洁明了地描述本次提交的目的。一个好的习惯是使用 "动词+宾语" 的格式，例如 "Fix: user login bug" 或 "Feat: add user profile page"。

5.  **查看提交历史 (`git log`)**
    想回顾你走过的路吗？`git log` 会按时间倒序列出所有的提交记录，包括哈希值 (唯一ID)、作者、日期和提交信息。

### **连接 GitHub，走向世界**

现在，你的项目只存在于你的电脑上。让我们把它推送到 GitHub，实现云端备份和远程协作。

#### **本地与远程关系图解**

```{mermaid}
graph TD
    subgraph "Your Computer"
        A["本地仓库(Local Repo)"]
    end

    subgraph "GitHub Cloud"
        B["远程仓库(Remote Repo)"]
    end

    A -- "git push" --> B
    B -- "git pull / git clone" --> A
```

#### **操作步骤**

1.  **在 GitHub 创建远程仓库**

      * 登录 GitHub，点击右上角 “+” -\> “New repository”。
      * 填写仓库名称 (建议与本地文件夹同名，如 `git-project`)。
      * 保持 “Public” (公开) 或选择 “Private” (私有)。
      * **非常重要**：**不要**勾选任何 “Initialize this repository with...” 的选项，因为我们已经有了一个本地仓库。
      * 点击 “Create repository”。

2.  **关联本地与远程 (`git remote add`)**
    创建后，GitHub 会提供一个 URL。复制这个 HTTPS 或 SSH 格式的 URL。回到你的终端，运行：

    ```bash
    # 将 '你的仓库URL' 替换成你刚刚复制的地址
    git remote add origin 你的仓库URL
    ```

      * `git remote add`：添加一个远程仓库的引用。
      * `origin`：是这个远程仓库的默认别名，你也可以取别的名字，但 `origin` 是约定俗成的。

3.  **推送本地变更 (`git push`)**
    最后，将你本地 `main` 分支上的所有提交推送到远程仓库 `origin`。

    ```bash
    git push -u origin main
    ```

      * `push`：推送动作。
      * `-u` (或 `--set-upstream`)：建立本地 `main` 分支与远程 `origin/main` 分支的联系。这个参数**只需要在第一次推送时使用**。之后，你只需要简单地运行 `git push`。
      * `origin`：远程仓库的别名。
      * `main`：你要推送的本地分支名。

    现在，刷新你的 GitHub 仓库页面，代码已经成功上传！

### **分支——安全开发的“平行宇宙”**

分支 (Branch) 是 Git 最强大的功能之一。它允许你从主线 (通常是 `main` 分支) 创建一个独立的副本，在新功能开发、Bug 修复等工作中，你可以在这个副本上自由地实验，而不会影响到主线的稳定性。

#### **分支与合并图解**

```{mermaid}
gitGraph
   commit id: "Initial"
   branch feature-A
   checkout feature-A
   commit id: "Add button"
   commit id: "Style button"
   checkout main
   merge feature-A
   commit id: "Release v1.0"
```

上图展示了：

1.  从 `main` 分支创建了一个 `feature-A` 分支。
2.  在 `feature-A` 上进行了两次提交 (开发新功能)。
3.  开发完成后，切换回 `main` 分支。
4.  将 `feature-A` 分支上的所有工作合并 (merge) 回 `main` 分支。

#### **分支常用命令**

  * **查看所有分支**:

    ```bash
    git branch
    ```

    星号 `*` 标记的是你当前所在的分支。

  * **创建新分支**:

    ```bash
    git branch <分支名>
    # 例如: git branch new-feature
    ```

  * **切换分支**:

    ```bash
    git checkout <分支名>
    # 例如: git checkout new-feature
    ```

  * **创建并立即切换到新分支 (常用)**:

    ```bash
    git checkout -b <新分支名>
    # 例如: git checkout -b another-feature
    ```

  * **合并分支**:
    首先，切换到你希望接纳变更的分支 (目标分支)，通常是 `main`。

    ```bash
    git checkout main
    ```

    然后，执行合并命令，将其他分支的变更合并进来。

    ```bash
    git merge <要合并的分支名>
    # 例如: git merge new-feature
    ```

  * **删除分支**:
    当一个分支的工作已经合并完成，通常会将其删除以保持仓库整洁。

    ```bash
    git branch -d <分支名>
    ```

### **日常协作流程**

```{mermaid}
%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel': true, 'mainBranchName': 'main'}} }%%
gitGraph
    commit id: "初始化项目"
    branch develop
    checkout develop
    commit id: "开发环境配置"
    
    %% --- 功能A开发 ---
    branch feat/user-login
    checkout feat/user-login
    commit id: "实现UI界面"
    commit id: "完成后端API对接"
    
    %% --- 在功能A开发的同时，主干发现紧急bug ---
    checkout main
    branch hotfix/bug-payment-api
    checkout hotfix/bug-payment-api
    commit id: "紧急修复支付掉单Bug"
    checkout main
    merge hotfix/bug-payment-api tag: "v1.0.1"
    
    %% --- 紧急修复也需要合并到develop分支 ---
    checkout develop
    merge main id: "同步紧急修复"
    
    %% --- 功能A开发完成，合并回develop ---
    checkout develop
    merge feat/user-login id: "合并用户登录功能"
    
    %% --- 功能B开始开发 ---
    branch feat/data-report
    checkout feat/data-report
    commit id: "设计报表模型"
    commit id: "完成报表前端"
    checkout develop
    
    %% --- 功能C也开始开发 ---
    branch feat/performance-opt
    checkout feat/performance-opt
    commit id: "优化数据库查询"
    
    %% --- 功能B开发完成，合并回develop ---
    checkout develop
    merge feat/data-report id: "合并数据报表功能"
    
    %% --- 功能C开发完成，合并回develop ---
    checkout develop
    merge feat/performance-opt id: "合并性能优化"
    
    %% --- 准备发布新版本 ---
    branch release/v1.1.0
    checkout release/v1.1.0
    commit id: "版本文档和最终测试"
    
    %% --- 正式发布：合并到main并打上标签 ---
    checkout main
    merge release/v1.1.0 tag: "v1.1.0"
    
    %% --- 将发布分支的修改也合并回develop ---
    checkout develop
    merge release/v1.1.0 id: "同步发布版本v1.1.0的修改"

```

在真实的项目中，你通常会从一个已经存在的项目开始工作。

1.  **克隆远程仓库 (`git clone`)**
    要获取 GitHub 上的项目到你的本地，使用 `git clone`。

    ```bash
    # 替换为项目的URL
    git clone https://github.com/some-user/some-project.git
    ```

    这个命令会自动创建项目文件夹，初始化 `.git` 仓库，并自动设置好远程别名 `origin`，还会将项目的所有数据都拉取下来。

2.  **保持本地更新 (`git pull`)**
    在你开始一天的工作前，或者在准备开发新功能前，务必从远程仓库拉取最新的变更，确保你的本地版本是最新的。

    ```bash
    git pull origin main
    # 如果已经设置了上游分支，可以直接用 git pull
    ```

    `git pull` 实际上是 `git fetch` (从远程拉取最新数据) 和 `git merge` (将远程分支合并到本地) 的一个快捷命令。

3.  **GitHub Flow：一个标准的协作模型**
    这是一个被广泛采用的、简单高效的协作流程：

    a.  从 `main` 分支创建一个描述性命名的**新分支** (`git checkout -b fix-login-bug`)。
    b.  在新分支上进行**编码和提交**。
    c.  将你的新分支**推送**到 GitHub (`git push origin fix-login-bug`)。
    d.  在 GitHub 上，为你的分支创建一个 **Pull Request (PR)**。PR 是一个请求，请求项目维护者审查你的代码，并将其合并到 `main` 分支。
    e.  团队成员在 PR 上进行**讨论和代码审查 (Code Review)**。
    f.  一旦审查通过，项目维护者会在 GitHub 网站上点击**合并 (Merge Pull Request)** 按钮。
    g.  合并后，你可以安全地**删除**你的功能分支。

### **实战：用 GitHub Pages 搭建你的个人主页**

现在，你已经掌握了 Git 和 GitHub 的核心技能，是时候将它们付诸实践，创建一个所有开发者都梦寐以求的“技术名片”——你的个人网站。GitHub 提供了一项名为 **GitHub Pages** 的免费服务，可以让你轻松地将你的代码仓库变成一个公开的网站。

#### **前端开发简介**

网页是由三种核心技术构建的：

  * **HTML (HyperText Markup Language)**: 定义了网页的**结构**和**内容**。比如，标题、段落、图片、链接等。
  * **CSS (Cascading Style Sheets)**: 负责网页的**样式**和**外观**。比如，颜色、字体、布局、边距等。
  * **JavaScript**: 赋予网页**交互性**和**动态功能**。比如，响应用户点击、表单验证、动画效果等。

我们将创建一个只包含 HTML 和 CSS 的简单静态页面，并将其部署到 GitHub Pages。

#### **搭建步骤**

1.  **创建一个特殊的仓库**
    登录 GitHub。点击右上角的“+”号，选择 "New repository"。这一步至关重要：

      * **Repository name (仓库名称)** 必须是 `<你的GitHub用户名>.github.io`。例如，如果你的用户名是 `octocat`，那么仓库名必须是 `octocat.github.io`。
      * **必须设置为 Public (公开)**。
      * 可以勾选 "Add a README file" 来初始化仓库，这会使后续的克隆操作更简单。

2.  **克隆仓库到本地**
    进入你刚刚创建的仓库页面，点击绿色的 "\<\> Code" 按钮，复制 HTTPS URL。然后，在你的电脑终端中运行 `git clone`：

    ```bash
    # 将URL替换为你自己的仓库URL
    git clone https://github.com/YourUsername/yourusername.github.io.git
    ```

3.  **创建你的主页文件**
    使用 `cd` 命令进入刚刚克隆下来的文件夹：

    ```bash
    cd yourusername.github.io
    ```

    创建一个名为 `index.html` 的文件。这个文件名是特殊的，当别人访问你的网站时，GitHub Pages 会默认显示这个文件的内容。在 `index.html` 文件中写入以下基础 HTML 代码：

    ```html
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>我的个人主页</title>
        <style>
            body { font-family: sans-serif; line-height: 1.6; margin: 40px; background-color: #f4f4f4; color: #333; }
            .container { max-width: 800px; margin: auto; background: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
            h1 { color: #0366d6; }
            a { color: #0366d6; text-decoration: none; }
            a:hover { text-decoration: underline; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>欢迎来到我的世界！</h1>
            <p>你好，我是 [你的名字]。这是我用 GitHub Pages 搭建的个人网站。</p>
            <p>你可以在 <a href="https://github.com/YourUsername" target="_blank">这里</a> 找到我的 GitHub 主页，关注我的开源项目。</p>
        </div>
    </body>
    </html>
    ```

    > **提示**: 记得将 `[你的名字]` 和 `YourUsername` 替换成你自己的信息。

4.  **提交并推送到 GitHub**
    现在，按照我们学过的标准 Git 流程，将你的新主页上传到 GitHub。

    ```bash
    # 1. 将所有新文件和修改添加到暂存区
    git add .

    # 2. 提交变更，并附上有意义的信息
    git commit -m "Feat: Create initial homepage"

    # 3. 将本地提交推送到远程仓库
    git push origin main
    ```

5.  **在 GitHub Settings 中配置 Pages**
    默认情况下，名为 `<username>.github.io` 的仓库会自动启用 GitHub Pages。但了解设置过程对所有项目都至关重要。

    a.  在浏览器中打开你的 GitHub 仓库页面。
    b.  点击仓库导航栏右侧的 **"Settings"** 标签。
    c.  在左侧的菜单中，找到并点击 **"Pages"**。

    d.  在 "Build and deployment" (构建与部署) 部分，你会看到 "Source" (源) 的选项。请确保它的设置为 **"Deploy from a branch"** (从分支部署)。

    e.  在下方的 "Branch" (分支) 设置中：

      * 选择你刚刚推送代码的分支，通常是 **`main`** (或者 `master`)。
      * 文件夹选项保持默认的 **`/ (root)`**。
      * 点击 **"Save"**。

    f.  保存后，页面顶部会出现一个蓝色的提示框，告诉你 “Your site is live at `https://<你的用户名>.github.io`”。有时，GitHub 需要一两分钟来完成部署（这个过程被称为 Action）。你可以点击提示框中的链接查看你的网站。

6.  **见证奇迹！**
    访问 `https://<你的GitHub用户名>.github.io`，你应该就能看到你刚刚创建的、拥有简单样式的个人主页了！

现在，你拥有了一个可以向全世界展示的个人网站。你可以继续学习 HTML 和 CSS 来美化它，或者添加你的项目作品集，让它成为你真正的在线简历。

### **总结**

恭喜你完成了这份详细的指南！现在你应该对 Git 和 GitHub 有了扎实的理解。

  * **核心理念**：版本控制是为了追踪历史和促进协作。
  * **本地三区**：工作区 -\> `git add` -\> 暂存区 -\> `git commit` -\> 本地仓库。
  * **远程交互**：`git clone` 开始，`git pull` 更新，`git push` 分享。
  * **分支是关键**：始终在独立的分支上工作，通过 Pull Request 合并，以保证 `main` 分支的稳定。


=== File: ./Lessons/lesson6.qmd ===
---
title: "3: Python数据可视化"
---


数据可视化是数据科学中连接数据与决策的关键桥梁。它不仅仅是制作图表，更是一门将原始、复杂的数据集转化为富有洞察力的视觉故事的艺术和科学。通过图形化的方式，我们能够更直观地识别数据中的模式、发现隐藏的趋势、理解变量间的关系并快速定位异常值。在众多数据科学工具中，Python 凭借其简洁的语法和强大的库生态系统，已成为执行数据可视化任务的首选语言。本指南将系统地引导你，从掌握最基础的静态绘图开始，一步步进阶到能够构建复杂、动态的交互式可视化应用。

## 🎨 为什么选择 Python 进行数据可视化？

Python 在数据科学领域的统治地位并非偶然，其在可视化方面的优势尤为突出：

  * **易于学习与使用**：相较于 R 或 Java 等其他语言，Python 的语法更接近自然语言，使得初学者可以快速上手，将主要精力集中在数据分析本身，而非复杂的编程语法上。
  * **强大的库生态系统**：Python 拥有一个无与伦比的开源库集合。
      * **Matplotlib** 是这个生态的基石，提供底层的、全面的绘图控制。
      * **Seaborn** 在 Matplotlib 之上提供了更高级的统计绘图接口，让美观的图表唾手可得。
      * **Plotly** 和 **Bokeh** 则将可视化带入了现代 Web 时代，专注于交互性和动态展示。
  * **无缝的社区支持**：得益于其庞大的用户和开发者社区，几乎任何你能想到的可视化问题，都已经有了现成的教程、代码示例或解决方案。Stack Overflow、GitHub 和各类博客是学习路上的宝贵资源。
  * **卓越的集成性**：可视化通常是数据处理流程的最后一步。Python 的可视化库能与数据处理的“三驾马车”——**Pandas (数据处理)**、**NumPy (数值计算)** 和 **Scikit-learn (机器学习)**——无缝集成，形成一个从数据清洗、分析到可视化的流畅工作流。


## Matplotlib：可视化的基石

**Matplotlib** 是 Python 可视化世界的奠基者和核心。它诞生于 2003 年，其设计初衷是模仿 MATLAB 的绘图功能，因此它拥有一个非常成熟和稳定的 API。几乎所有 Python 中的静态图表，无论多么复杂，都可以用 Matplotlib 实现。学习 Matplotlib 不仅是学习一个库，更是理解图形如何被一层层构建起来的过程——从画布 (Figure) 到坐标系 (Axes)，再到线条、点、标签等每一个独立的元素。这种精细的控制力是它最大的优势，也是其略显复杂的根源。

### 安装 Matplotlib

通过 pip 包管理器可以轻松安装：

```bash
pip install matplotlib
```

### 基础绘图：解构一张图表

在 Matplotlib 中，推荐使用其面向对象的接口进行绘图，这能让你对图表的各个部分有更清晰的控制。一个基本的绘图流程如下：

1.  **创建画布 (Figure) 和坐标系 (Axes)**：`plt.subplots()` 是最常用的方法，它像是在画纸上为你准备好了一块准备作画的区域。
2.  **在坐标系上绘图**：调用 `ax` 对象的方法（如 `ax.plot()`, `ax.scatter()`）来添加数据。
3.  **定制图表元素**：通过 `ax` 的 `set_` 系列方法（如 `set_title()`, `set_xlabel()`）来添加标题、坐标轴标签等。
4.  **显示图形**：最后调用 `plt.show()` 将最终结果渲染出来。

#### 1\. 折线图 (Line Plot)

折线图是展示连续数据趋势最基础也最有效的工具，尤其适合于时间序列数据。

```{python}
import matplotlib.pyplot as plt
import numpy as np

# 准备一组平滑的、有代表性的数据
# np.linspace 在 0 到 10 之间生成 100 个等间距的点
x = np.linspace(0, 10, 100)
# 计算每个 x 点对应的正弦值
y = np.sin(x)

# 1. 创建画布(fig)和坐标系(ax)
fig, ax = plt.subplots(figsize=(8, 5)) # figsize可以控制图形的尺寸

# 2. 在坐标系(ax)上绘制折线图
# 'label' 参数用于后续图例的生成
ax.plot(x, y, label='sin(x)', color='blue', linewidth=2)

# 3. 定制图表的标题和坐标轴标签，增加可读性
ax.set_title('Simple Sine Wave')
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
# ax.grid(True, linestyle='--', alpha=0.6) # 添加网格线，使数值读取更容易
ax.legend() # 显示图例

# 4. 显示最终的图形
plt.show()
```

#### 2\. 散点图 (Scatter Plot)

散点图是探索两个数值变量之间是否存在关联的利器。每个点代表一个数据样本，其在 x-y 平面上的位置由两个变量的值决定。

```{python}
import matplotlib.pyplot as plt
import numpy as np

# 使用 NumPy 生成两组随机数据来模拟两个变量
np.random.seed(42) # 设置随机种子以保证结果可复现
x = np.random.rand(50) * 10
y = np.random.rand(50) * 10

# 直接使用 plt 接口进行快速绘图，这对于简单图表更便捷
plt.figure(figsize=(8, 5)) # 创建一个新画布
plt.scatter(x, y, c='red', alpha=0.6, edgecolors='black') # c是颜色, alpha是透明度, edgecolors是点的边缘颜色

# 添加标题和标签
plt.title('Basic Scatter Plot')
plt.xlabel('X Value')
plt.ylabel('Y Value')

# 显示图形
plt.show()
```

#### 3\. 条形图 (Bar Chart)

当需要比较不同类别之间的数值大小时，条形图是最佳选择。它的高度或长度直观地表示了数值的大小。

```{python}
import matplotlib.pyplot as plt

# 准备离散的类别和它们对应的数值
categories = ['Group A', 'Group B', 'Group C', 'Group D']
values = [23, 45, 58, 32]

plt.figure(figsize=(8, 5))
# plt.bar() 用于创建垂直条形图，plt.barh() 用于创建水平条形图
plt.bar(categories, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']) # 为每个条形指定不同颜色

# 添加标题和标签
plt.title('Categorical Bar Chart')
plt.xlabel('Category')
plt.ylabel('Value')
# 在每个条形图上方显示具体数值
for i, value in enumerate(values):
    plt.text(i, value + 1, str(value), ha='center')

# 显示图形
plt.show()
```

#### 4\. 直方图 (Histogram)

直方图是理解单个数值变量分布情况的核心工具。它将数值范围分割成若干个“箱子”(bins)，然后统计落入每个箱子的数据点数量。

```{python}
import matplotlib.pyplot as plt
import numpy as np

# 生成 1000 个服从标准正态分布的随机数
data = np.random.randn(1000)

plt.figure(figsize=(8, 5))
# bins 参数非常关键，它决定了分布的精细程度
# edgecolor='black' 让每个条柱的边界更清晰
plt.hist(data, bins=30, color='skyblue', edgecolor='black')

# 添加标题和标签
plt.title('Data Distribution Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 显示图形
plt.show()
```


## 📊 Seaborn：更美观的统计图表

**Seaborn** 是在 Matplotlib 基础上进行的一次华丽封装。它的出现解决了 Matplotlib 的两大痛点：复杂的参数设置和不够现代的默认样式。Seaborn 的核心理念是让统计绘图变得简单，它提供了大量专门为展示统计信息而设计的图表类型，并且能够直接识别和使用 Pandas DataFrame 的列名，极大地简化了数据准备的过程。你只需要一行代码，就能创建出在 Matplotlib 中可能需要十几行代码才能实现的复杂图表。

### 安装 Seaborn

```bash
pip install seaborn pandas
```

### 常用统计图

我们将使用 Seaborn 内置的 `tips` (餐厅小费) 数据集来展示其强大功能。这个数据集记录了顾客的账单总额、支付的小费、性别、是否吸烟等信息。

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Seaborn 加载数据集非常方便
tips = sns.load_dataset("tips")

# 设置 Seaborn 的美学风格
sns.set_theme(style="whitegrid", palette="muted")
```

#### 1\. 关系图 (Relational Plot)

`scatterplot` 是 Seaborn 的明星函数之一。除了基本的 x-y 关系，它还能通过 `hue`, `style`, `size` 等参数，将多达五个维度的信息压缩到一张二维图表中，极大地提升了信息密度。

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

tips = sns.load_dataset("tips")
plt.figure(figsize=(10, 6))

# hue: 按 'time' (晚餐/午餐) 对点进行着色
# style: 按 'smoker' (是否吸烟) 改变点的形状
# size: 按 'size' (用餐人数) 改变点的大小
sns.scatterplot(data=tips, x="total_bill", y="tip", hue="time", style="smoker", size="size")

# 添加一个更具信息量的标题
plt.title('Tip Amount vs. Total Bill by Time, Smoker Status, and Party Size')
plt.show()
```

#### 2\. 分类图 (Categorical Plot) - 小提琴图 (Violin Plot)

当箱形图（Box Plot）不足以展示数据分布的细节时，小提琴图是绝佳的替代品。它将箱形图与核密度估计（KDE）图相结合，不仅展示了数据的四分位数，还通过外部的形状展示了数据的完整分布密度，尤其适合于比较多组数据的分布形态。

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

tips = sns.load_dataset("tips")
plt.figure(figsize=(10, 6))

# x: 类别轴 (星期几)
# y: 数值轴 (账单总额)
# hue: 在每个类别内再按 'smoker' 进行二次分类
# split=True: 将 hue 分类的两个小提琴合并到一起，便于比较
sns.violinplot(data=tips, x="day", y="total_bill", hue="smoker", split=True, inner="quartile")

plt.title('Total Bill Distribution by Day and Smoker Status')
plt.show()
```

#### 3\. 热力图 (Heatmap)

热力图是一种将矩阵数据可视化的强大工具，它通过颜色的深浅来直观地表示数值的大小。它最经典的应用场景是展示变量间的相关性矩阵，让人一眼就能看出哪些变量是强相关的。

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# 计算 tips 数据集中数值列的相关性矩阵
numeric_cols = tips.select_dtypes(include=np.number)
correlation_matrix = numeric_cols.corr()

plt.figure(figsize=(8, 6))
# annot=True: 在每个单元格中显示数值
# fmt=".2f": 数值格式化为两位小数
# cmap='coolwarm': 使用一个冷暖色调的色彩映射，正相关为暖色，负相关为冷色
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')

plt.title('Correlation Matrix of Tips Dataset')
plt.show()
```

#### 4\. 配对图 (Pair Plot)

`pairplot` 是探索性数据分析（EDA）的终极武器。它能快速生成数据集中所有数值变量两两之间的关系图矩阵。在矩阵的对角线上，是每个变量自身的分布直方图或核密度图；在非对角线的位置，是两个变量之间的散点图。这让你能够在一个宏观的视角下审视整个数据集的结构。

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# 加载著名的鸢尾花数据集
iris = sns.load_dataset("iris")

# hue="species": 根据花的品种对所有图表进行着色，这使得不同类别间的差异一目了然
# markers: 为不同品种指定不同的标记形状，增强区分度
sns.pairplot(iris, hue="species", markers=["o", "s", "D"])

plt.suptitle('Pairwise Relationships in the Iris Dataset', y=1.02) # 添加总标题
plt.show()
```


## ✨ Plotly：交互式图表的未来

当静态图表无法满足你的探索需求时，**Plotly** 便登场了。Plotly 是一家科技公司，也是一个强大的 Python 库，其核心目标是创建具有丰富交互能力的、出版物级别的图表。用户可以缩放、平移、选择数据区域，以及通过鼠标悬停查看精确的数值。这极大地增强了数据探索的深度和广度。`Plotly Express` 作为其高级接口，秉承“用最少的代码做最多的事”的原则，让创建复杂的交互式图表变得前所未有的简单。

### 安装 Plotly

```bash
pip install plotly
```

### 创建交互式图表

Plotly 生成的图表本质上是 JSON 数据结构，由 Plotly.js（一个 JavaScript 库）在浏览器中进行渲染。这意味着你可以轻松地将它们嵌入到网页、Jupyter Notebook、或者导出为独立的 HTML 文件。

#### 1\. 交互式散点图

这个例子展示了 Plotly Express 如何仅用一行核心代码就创建一个信息丰富的交互式散点图。

```{python}
import plotly.express as px

# Plotly Express 内置了大量方便的数据集，包括著名的 Gapminder 数据
gapminder = px.data.gapminder().query("year==2007")

# x, y, color, size, hover_data 等参数都直接对应 DataFrame 的列名
fig = px.scatter(gapminder,
                 x="gdpPercap",       # x轴：人均GDP
                 y="lifeExp",         # y轴：预期寿命
                 color="continent",   # 点的颜色：按大洲划分
                 size='pop',          # 点的大小：按人口划分
                 hover_name="country",# 鼠标悬停时显示的名称
                 log_x=True,          # x轴使用对数尺度，以便更好地展示GDP差异
                 size_max=60)         # 控制点的最大尺寸

fig.update_layout(title_text='GDP per Capita vs. Life Expectancy (2007)')
fig.show()
```

#### 2\. 交互式地图（Choropleth Map）

地理空间可视化是 Plotly 的一大亮点。制作等值区域图（Choropleth Map）非常直观，你只需要提供地理位置编码（如国家代码）和对应的数值即可。

```{python}
import plotly.express as px

# 同样使用 Gapminder 数据，但这次是多年的数据
gapminder_all_years = px.data.gapminder()

# animation_frame: 指定按'year'列创建动画帧
# animation_group: 指定'country'作为动画中保持一致的对象
# color_continuous_scale: 指定连续颜色的主题
# projection: 选择地图的投影方式
fig = px.choropleth(gapminder_all_years,
                    locations="iso_alpha", # 使用 ISO Alpha-3 国家代码进行地理定位
                    color="lifeExp",       # 区域的颜色深浅由预期寿命决定
                    hover_name="country",  # 悬停名称
                    animation_frame="year",# 创建一个时间滑块动画
                    color_continuous_scale=px.colors.sequential.Plasma,
                    title="Global Life Expectancy Over Time")

fig.show()
```

#### 3\. 3D 散点图

Plotly 让 3D 可视化变得触手可及。你可以通过鼠标拖拽来自由地旋转、缩放和平移 3D 场景，从任何角度观察数据的空间分布。

```{python}
import plotly.express as px

# 加载鸢尾花数据集
iris = px.data.iris()

# 创建一个 3D 散点图，探索三个花瓣/花萼特征之间的关系
fig = px.scatter_3d(iris,
                    x='sepal_length',
                    y='sepal_width',
                    z='petal_width',
                    color='species',
                    symbol='species',  # 为不同种类使用不同形状的标记
                    title="3D View of Iris Dataset Features")

fig.update_layout(margin=dict(l=0, r=0, b=0, t=40)) # 调整边距以更好地展示
fig.show()
```

## 🤔 如何选择合适的工具？

面对如此多的选择，关键在于理解你的核心需求。下面是一个更详细的决策指南：

| 库 | 主要优点 | 最佳使用场景 | 不太适合的场景 |
| :--- | :--- | :--- | :--- |
| **Matplotlib** | **极致控制力**：可定制图表的每一个细节。稳定、成熟，是学术出版的首选。 | 1. 需要精确布局和标注的**学术论文**图表。\<br\>2. 创建全新的、非标准的图表类型。\<br\>3. 作为其他库（如 Seaborn）的底层进行微调。 | 1. 快速制作交互式图表。\<br\>2. 简单的探索性数据分析（代码相对繁琐）。 |
| **Seaborn** | **统计之美**：API 简洁优雅，专为统计分析设计。与 Pandas 结合极佳，默认样式美观。 | 1. **探索性数据分析 (EDA)**，快速洞察数据分布和关系。\<br\>2. 展示变量间的统计相关性（如相关性热图、回归图）。 | 1. 高度定制化的非统计类图表。\<br\>2. 构建复杂的交互式仪表盘。 |
| **Plotly** | **现代交互**：图表美观、交互体验流畅。支持 3D、动画和地理图，易于分享 (HTML)。 | 1. **商业智能 (BI) 报告**和在线演示。\<br\>2. 创建需要用户交互探索的 Web 图表。\<br\>3. 制作引人注目的动画和 3D 可视化。 | 1. 对性能要求极高的超大规模数据集实时渲染。\<br\>2. 简单的静态图表（可能有些大材小用）。 |
| **Bokeh** | **应用构建**：专为 Web 设计，可构建复杂的**数据应用**。支持流数据，交互逻辑强大。 | 1. 构建**复杂的仪表盘**，包含图表联动、下拉菜单、滑块等控件。\<br\>2. 需要实时更新数据的**流数据监控**应用。 | 1. 静态图表的制作（不如 Matplotlib/Seaborn 直接）。\<br\>2. 对非 Web 输出（如 PDF, PNG）有高质量要求时。 |

**经验法则**：

1.  **日常探索与分析**：你的起点应该是 **Seaborn**。它能用最少的代码满足你 80% 的统计绘图需求。当需要交互时，无缝切换到 **Plotly Express**。
2.  **发表论文或书籍**：使用 **Seaborn** 或 **Plotly** 进行初步绘图，然后用 **Matplotlib** 对字体、分辨率、布局等细节进行最后的精修，以满足出版要求。
3.  **构建交互式 Web 应用**：如果你的目标是一个独立的、可交互的网页或仪表盘，**Plotly** 和 **Bokeh** 是你的不二之选。Plotly 在单图表的华丽度和易用性上略有优势，而 Bokeh 在构建多组件联动的复杂应用时提供了更强的灵活性。

=== File: ./Lessons/lesson7.qmd ===
---
title: "4: 基于Python的科学计算"
---

Python凭借其简洁的语法和强大的库生态系统，已成为科学计算和数据分析领域首选的编程语言之一。本文档将引导你从基础的数值计算开始，逐步深入到更复杂的科学计算领域，并辅含有可视化的代码示例来帮助理解。


## 核心库：科学计算的基石

Python的科学计算能力主要构建在几个核心库之上。掌握它们是进行高效科学计算的前提。

  * **NumPy (Numerical Python)**: 提供了多维数组对象（`ndarray`），以及对这些数组进行操作的各种函数。是几乎所有科学计算库的基础。
  * **SciPy (Scientific Python)**: 基于NumPy，提供了大量用于科学和工程计算的算法，如优化、积分、插值、信号处理、线性代数等。
  * **Pandas**: 提供了高性能、易于使用的数据结构（如`DataFrame`）和数据分析工具，特别适合处理结构化数据。
  * **Matplotlib**: 一个功能强大、灵活的绘图库，可以创建各种静态、动态、交互式的图表。


## 一、NumPy: 高效的数值计算

NumPy的核心是`ndarray`对象，它是一个n维数组，可以存储同类型的数据。与Python原生的列表相比，`ndarray`在数值计算上效率更高，内存占用也更少。

### 1.1 创建数组

你可以从Python列表或元组创建NumPy数组。

```{python}
import numpy as np

# 从列表创建一维数组
a = np.array([1, 2, 3, 4, 5])
print(f"1D Array: {a}")
print(f"Shape: {a.shape}")

# 创建一个3x3的二维数组
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print("2D Array:\n", b)

# 创建特定类型的数组
c = np.zeros((2, 4))  # 2x4的全零数组
d = np.ones((3, 3), dtype=np.int16) # 3x3的全一整数数组
e = np.arange(0, 10, 2) # 从0到10，步长为2的数组
f = np.linspace(0, 1, 5) # 从0到1，生成5个等间距的数
```

### 1.2 数组运算：向量化

NumPy的强大之处在于其“向量化”运算。你可以在整个数组上执行操作，而无需编写显式的循环，这使得代码更简洁、执行速度更快。

```{python}
import numpy as np

a = np.array([10, 20, 30, 40])
b = np.array([1, 2, 3, 4])

# 元素级运算
c = a - b  # [ 9 18 27 36]
d = a * b  # [ 10  40  90 160]
e = a**2   # [100 400 900 1600]
f = np.sin(a) # 对每个元素计算正弦值

print(f"a - b = {c}")
print(f"a * b = {d}")

# 矩阵运算
A = np.array([[1, 1], [0, 1]])
B = np.array([[2, 0], [3, 4]])

print("Element-wise product:\n", A * B)
print("Matrix product (dot product):\n", A @ B) # 或者 np.dot(A, B)
```

### 1.3 索引和切片

NumPy的索引和切片机制非常灵活，与Python列表类似，但功能更强大，支持多维操作。

```{python}
import numpy as np

arr = np.arange(10) # [0 1 2 3 4 5 6 7 8 9]

# 切片
print(arr[2:5]) # [2 3 4]

# 多维数组索引
matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(f"Element at row 1, col 2: {matrix[1, 2]}") # 6

# 切片多维数组
print("First two rows:\n", matrix[:2, :])
print("First column:\n", matrix[:, 0])
```


## 二、SciPy: 科学计算算法库

如果说NumPy是基础的数据结构，那么SciPy就是建立在这个基础之上的算法工具箱。它包含了许多预先构建好、经过优化的函数来解决常见的科学计算问题。

### 2.1 数值积分 (Numerical Integration)

SciPy的`integrate`模块可以处理数值积分问题。例如，计算函数 $f(x) = e^{-x^2}$ 在 $[0, \infty)$ 上的积分。

$$\int_{0}^{\infty} e^{-x^2} dx$$

这个积分的解析解是 $\frac{\sqrt{\pi}}{2}$。

```{python}
import numpy as np
from scipy.integrate import quad

# 定义被积函数
def integrand(x):
    return np.exp(-x**2)

# quad 函数返回两个值：积分结果和估计的误差
result, error = quad(integrand, 0, np.inf)

print(f"Numerical result: {result}")
print(f"Analytical result: {np.sqrt(np.pi) / 2}")
print(f"Estimated error: {error}")
```

### 2.2 优化 (Optimization)

`scipy.optimize` 模块提供了一系列函数来寻找函数的最小值或根。例如，我们来寻找函数 $f(x) = (x-2)^2 + 3$ 的最小值。

```{python}
from scipy.optimize import minimize

# 定义目标函数
def objective_function(x):
    return (x - 2)**2 + 3

# 使用minimize函数寻找最小值，需要提供一个初始猜测值
# 'BFGS' 是一种常用的优化算法
result = minimize(objective_function, x0=0, method='BFGS')

print(result)
print(f"\nMinimum found at x = {result.x[0]}")
```

### 2.3 信号处理 (Signal Processing)

SciPy的`signal`模块是处理信号的利器。例如，我们可以对一个含有噪声的信号进行滤波。

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal

# 生成信号
t = np.linspace(0, 1, 1000, endpoint=False)
# 一个干净的5Hz正弦波
clean_signal = np.sin(2 * np.pi * 5 * t)
# 添加高斯白噪声
noise = np.random.randn(len(t)) * 0.5
noisy_signal = clean_signal + noise

# 设计一个巴特沃斯低通滤波器
b, a = signal.butter(4, 0.03, 'low') # 4阶，截止频率为0.03

# 应用滤波器
filtered_signal = signal.filtfilt(b, a, noisy_signal)

# 可视化
plt.figure(figsize=(12, 6))
plt.plot(t, noisy_signal, label='Noisy Signal', alpha=0.7)
plt.plot(t, filtered_signal, label='Filtered Signal', linewidth=2)
plt.plot(t, clean_signal, label='Clean Signal', linestyle='--', color='black')
plt.title('Signal Filtering Example')
plt.xlabel('Time [s]')
plt.ylabel('Amplitude')
plt.legend()
plt.grid(True)
plt.show()
```


## 三、Pandas: 强大的数据处理

Pandas是处理和分析结构化数据的标准库。它的核心数据结构是`Series`（一维）和`DataFrame`（二维），可以让你用直观的方式对数据进行切片、筛选、分组、聚合等操作。

```{python}
import pandas as pd

# 创建一个DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],
    'Age': [25, 30, 35, 28, 22],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],
    'Salary': [70000, 80000, 90000, 75000, 65000]
}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# 数据筛选
print("\nPeople older than 30:")
print(df[df['Age'] > 30])

# 分组和聚合
# 按城市计算平均薪水
average_salary_by_city = df.groupby('City')['Salary'].mean()
print("\nAverage salary by city:")
print(average_salary_by_city)
```


## 四、Matplotlib: 数据可视化

“一图胜千言”。Matplotlib是Python中最基础也最强大的绘图库，能够创建出版质量的图表。

### 实例：布朗运动与正态分布

**布朗运动**（或称维纳过程）是模拟粒子随机游走的数学模型。它与正态分布有着深刻的联系：**大量独立粒子经过一段时间的布朗运动后，它们最终位置的分布会趋向于一个正态分布**。

这个现象是中心极限定理的一个直观体现。我们可以通过模拟成千上万条独立的布朗运动轨迹来验证这一点。

1.  **模拟过程**:
      * 我们模拟 `N` 个粒子，每个粒子都从原点 (0) 出发。
      * 每个粒子都运动 `M` 个时间步。
      * 在每个时间步 `dt` 内，粒子的位移是一个服从正态分布的随机数（均值为0，方差为 `dt`）。
      * 一个粒子的总位移是所有步位移的累加。
2.  **理论**:
      * 根据理论，在总时间 `T = M * dt` 后，所有粒子最终位置 `X(T)` 的分布应该是一个均值为 0，方差为 `T` 的正态分布，即 $X(T) \sim \mathcal{N}(0, T)$。

下面的代码将模拟这个过程并进行可视化。

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# --- 1. 定义模拟参数 ---
num_paths = 5000  # 模拟的粒子（路径）数量
num_steps = 1000  # 每条路径的时间步数
dt = 0.01         # 每个时间步的长度
T = num_steps * dt # 总时间

# --- 2. 生成布朗运动路径 ---
# 为每条路径的每一步生成一个随机位移
# 位移服从均值为0, 标准差为sqrt(dt)的正态分布
random_steps = np.random.normal(0, np.sqrt(dt), (num_paths, num_steps))

# 计算每条路径在每个时间点的位置
# 使用cumsum(axis=1)对时间步进行累加
# 在开头插入0，表示所有路径都从0开始
initial_positions = np.zeros((num_paths, 1))
paths = np.concatenate((initial_positions, random_steps), axis=1)
paths = np.cumsum(paths, axis=1)

# 创建时间轴
time_axis = np.linspace(0, T, num_steps + 1)

# --- 3. 可视化部分 ---

# 图1：展示几条样本路径
plt.figure(figsize=(10, 6))
# 只画前50条路径，否则会很乱
for i in range(50):
    plt.plot(time_axis, paths[i, :])

plt.title('Sample Brownian Motion Paths')
plt.xlabel('Time')
plt.ylabel('Position')
plt.grid(True)
plt.show()

final_positions = paths[:, -1]

# 计算理论上的正态分布参数
mu = 0
sigma = np.sqrt(T)

# 创建用于绘制理论曲线的x轴
x_theory = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)
# 计算理论正态分布的概率密度函数 (PDF)
pdf_theory = norm.pdf(x_theory, loc=mu, scale=sigma)

plt.figure(figsize=(10, 6))

# 绘制最终位置的直方图
# density=True 表示将直方图面积归一化，以便与PDF比较
plt.hist(final_positions, bins=50, density=True, alpha=0.7, label=f'Final Positions of {num_paths} Paths')

# 叠加理论上的正态分布曲线
plt.plot(x_theory, pdf_theory, 'r-', lw=2, label=f'Normal Distribution (mu=0, sigma={sigma:.2f})')

plt.title('Distribution of Final Positions after Time T')
plt.xlabel('Final Position')
plt.ylabel('Probability Density')
plt.legend()
plt.grid(True)
plt.show()
```

**结果分析**:
第一张图展示了粒子随机游走的轨迹。第二张图是核心，它清晰地显示了5000个粒子在运动10秒后，其最终位置的分布（蓝色直方图）与理论上的正态分布曲线（红色）完美吻合。这直观地证明了布朗运动与正态分布之间的深刻联系。

## 五、进阶话题：符号计算与机器学习

### 5.1 SymPy: 符号计算

与NumPy和SciPy进行数值计算不同，SymPy可以进行符号计算（代数运算）。这意味着你可以处理数学表达式，而不是数值。

```{python}
import sympy as sp

# 定义符号
x, y = sp.symbols('x y')

# 创建表达式
expr = (x + y)**2
print(f"Original expression: {expr}")

# 展开表达式
expanded_expr = sp.expand(expr)
print(f"Expanded expression: {expanded_expr}") # x**2 + 2*x*y + y**2

# 对表达式求导
derivative_expr = sp.diff(expanded_expr, x)
print(f"Derivative with respect to x: {derivative_expr}") # 2*x + 2*y

# 解方程 x**2 - 4 = 0
solutions = sp.solve(x**2 - 4, x)
print(f"Solutions for x**2 - 4 = 0 are: {solutions}") # [-2, 2]
```

### 5.2 Scikit-learn: 机器学习

Scikit-learn是建立在NumPy, SciPy和Matplotlib之上的机器学习库，提供了大量易于使用的监督学习和无监督学习算法。

这里是一个简单的线性回归示例。

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# 创建一些样本数据
# X 必须是二维数组
X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([1.5, 3.8, 6.5, 8.2, 11.5, 13.8])

# 创建并训练模型
model = LinearRegression()
model.fit(X, y)

# 预测
X_new = np.array([[0], [7]])
y_pred = model.predict(X_new)

# 打印模型参数
print(f"Intercept: {model.intercept_}") # 截距
print(f"Coefficient: {model.coef_[0]}") # 斜率

# 可视化结果
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X_new, y_pred, color='red', linewidth=2, label='Linear Fit')
plt.title('Simple Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()
```

## 结论

Python通过其强大的科学计算库生态系统，为研究人员、工程师和数据科学家提供了一套完整、高效且易于上手的工具。从基础的数组操作到复杂的机器学习模型，你都可以在Python中找到解决方案。希望本文档能为你开启Python科学计算的大门。继续探索，不断实践，你将能利用Python解决更多有趣的实际问题。

=== File: ./Lessons/lesson3.qmd ===
---
title: "3: Python实用进阶"
---

## 进阶心法：拥抱命令行

当你熟练使用 VS Code 或 PyCharm 这样的 IDE 时，你会发现它们侧边栏上有一个绿色的“运行”按钮，以及强大的图形化调试（Debug）工具。这些功能非常高效，但我强烈建议你先“封印”它们。

### 为什么要坚持使用命令行？

- **理解程序的真实运行环境**  
  你的 Python 代码最终是在操作系统中的一个进程里执行的，而不是在 IDE 的“魔法盒子”里。命令行就是这个真实环境最直接的交互窗口。理解 `python your_script.py` 这行命令的背后意义，比点击一个按钮要重要得多。它告诉你：“我正在调用 Python 解释器，让它来执行这个脚本文件。”

- **掌握处理参数的能力**  
  真实世界的程序很少是“一键运行”就完事的。它们通常需要接收外部传入的参数来改变行为。例如，一个处理文件的脚本需要知道要处理哪个文件；一个数据转换工具需要知道输出格式是什么。这些参数都是通过命令行传入的。如果你只用 IDE 的运行按钮，你将失去练习这项核心技能的机会。

- **学习更本质的调试方法**  
  图形化的 Debugger 非常强大，但它也隐藏了许多底层细节。`pdb` 是 Python 内置的命令行调试工具。学会使用它，意味着你可以在任何地方进行调试，哪怕是在一个没有图形界面的服务器上。这种能力是专业开发者必备的。它能让你更深刻地理解代码的执行流程、作用域和调用栈。

当你真正理解了命令行的工作方式后，再回头去使用 IDE 的便捷功能，你就会明白那些按钮背后到底发生了什么，从而更好地驾驭这些工具，而不是成为它们的“囚徒”。

## 在命令行中处理参数

让我们的脚本变得更实用！一个能接收参数的程序才算得上是一个真正的“工具”。

### 方式一：使用 sys.argv（入门）

Python 的 `sys` 模块提供了一个名为 `argv` 的列表，它可以捕获所有从命令行传入的参数。

- `sys.argv` 是一个列表（list）。
- 列表的第一个元素 `sys.argv[0]` 永远是脚本文件自己的名字。
- 从 `sys.argv[1]` 开始，依次是用户传入的参数。

#### 示例：greet_user.py

```{python}
# greet_user.py
import sys

# 检查用户是否传入了足够多的参数
if len(sys.argv) > 1:
    # sys.argv[0] 是脚本名 'greet_user.py'
    # sys.argv[1] 是我们期望的第一个参数
    name = sys.argv[1]
    print(f"你好，{name}！欢迎来到命令行世界。")
else:
    print("错误：请输入你的名字作为参数！")
    print("用法: python greet_user.py [你的名字]")

# 让我们看看 sys.argv 到底是什么
print(f"\ Debug Info")
print(f"sys.argv 的内容是: {sys.argv}")
print(f"总共有 {len(sys.argv)} 个元素。")
```

#### 在命令行中运行它：

**不带参数运行：**

```{bash}
python greet_user.py
```

输出：

```
错误：请输入你的名字作为参数！
用法: python greet_user.py [你的名字]
 Debug Info
sys.argv 的内容是: ['greet_user.py']
总共有 1 个元素。
```

**带一个参数运行：**

```{bash}
python greet_user.py 张三
```

输出：

```
你好，张三！欢迎来到命令行世界。
 Debug Info
sys.argv 的内容是: ['greet_user.py', '张三']
总共有 2 个元素。
```

**带多个参数运行：**

```{bash}
python greet_user.py "张三" "来自北京"
```

如果你传入的参数包含空格，请用引号把它包起来。`sys.argv` 会把每个用空格隔开的单元（或引号包起来的整体）当作一个独立的参数。

> 对于简单的脚本，`sys.argv` 足够好用。但当参数变得复杂时（例如需要处理 `-f`, `--file` 这样的选项），我们通常会使用更强大的 `argparse` 模块，你可以在学完基础后自行探索。

### 方式二：使用 argparse（进阶推荐）

有些脚本需要处理更多参数：选项、类型、默认值、帮助信息……这时候，推荐用 Python 标准库中的 [`argparse`](https://docs.python.org/zh-cn/3/library/argparse.html)。

#### 什么是 argparse？

`argparse` 能帮你自动解析命令行参数，自动生成帮助说明，还能支持多种参数类型、可选项、布尔开关等，非常适合写稍微复杂一点的命令行工具。

#### 示例：greet_user_argparse.py

```{python}
#| eval: false
import argparse

parser = argparse.ArgumentParser(description="欢迎使用命令行问候脚本")
parser.add_argument('name', help='你的名字')
parser.add_argument('--city', '-c', help='你来自的城市', default='未知')

args = parser.parse_args()

print(f"你好，{args.name}！来自 {args.city}。欢迎来到命令行世界。")
```

#### 运行效果：

**查看帮助：**

```{bash}
python greet_user_argparse.py --help
```

输出：

```
usage: greet_user_argparse.py [-h] [--city CITY] name

欢迎使用命令行问候脚本

positional arguments:
  name           你的名字

options:
  -h, --help     show this help message and exit
  --city CITY, -c CITY
                 你来自的城市
```

**带参数运行：**

```{bash}
python greet_user_argparse.py 张三
```
输出：
```
你好，张三！来自 未知。欢迎来到命令行世界。
```

**带全部参数运行：**

```{bash}
python greet_user_argparse.py 张三 --city 北京
```
输出：
```
你好，张三！来自 北京。欢迎来到命令行世界。
```

#### 为什么推荐 argparse？

- 自动生成友好的帮助信息（`-h`/`--help`）
- 支持位置参数、可选参数、类型检查、默认值等
- 错误提示更友好
- 写复杂命令行工具几乎必备

> 建议：写任何对外开放的小工具，都推荐用 argparse 提升专业度和可维护性！

## 使用 pdb 进行命令行调试

当你的代码出错了，或者你想观察代码的执行过程，`print()` 是最简单的调试方法，但它不够灵活。这时，`pdb` 就该登场了。

`pdb` 允许你在程序的任意位置设置一个“断点”，然后逐行执行代码，检查变量，让你像侦探一样剖析程序的每一步。

### 如何启动 pdb？

你只需要在你想要暂停的地方，加入下面这两行代码：

```{python}
#| eval: false
import pdb
pdb.set_trace()
```

这就像在你的代码里设置了一个检查站。当程序运行到这里时，它会自动停下来，并给你一个 `(Pdb)` 的交互式提示符。

### pdb 实战演练

让我们来调试一个有问题的函数。创建一个文件 `buggy_calculator.py`：

```{python}
#| eval: false
import pdb

def calculate_sum(data_list):
    total = 0
    for item in data_list:
        # 我们怀疑这里可能出问题，所以设置一个断点
        pdb.set_trace() 
        total += item
    return total

numbers = [1, 2, "3", 4, 5] # 列表中混入了一个字符串
result = calculate_sum(numbers)
print(f"计算结果是: {result}")
```

#### 在命令行运行这个脚本：

```{bash}
python buggy_calculator.py
```

当程序执行到 `pdb.set_trace()` 时，它会暂停，你的终端会显示类似这样的信息：

```
> /path/to/your/buggy_calculator.py(8)calculate_sum()
-> total += item
(Pdb)
```

你进入了 pdb 的调试环境！你可以输入不同的命令来控制和检查程序。


## pdb 常用核心命令

记住下面几个命令，你就能解决 80% 的问题：

- `l` (list): 查看当前代码及上下文。它会显示你正停在哪一行，以及周围的代码。
- `n` (next): 执行下一行代码。如果下一行是函数调用，它会直接执行完整个函数，不会进入函数内部。
- `s` (step): 执行下一行代码。如果下一行是函数调用，它会进入该函数内部，让你逐行调试函数里的代码。
- `c` (continue): 继续执行代码，直到遇到下一个断点或者程序结束。
- `p <变量名>` (print): 打印一个变量的当前值。例如，输入 `p total` 或 `p item`。
- `q` (quit): 退出调试并终止程序。

### 用这些命令找出 `buggy_calculator.py` 的问题

程序停在了 `total += item` 这一行。我们先用 `l` 看一下代码：

```
(Pdb) l
  3     def calculate_sum(data_list):
  4         total = 0
  5         for item in data_list:
  6             # 我们怀疑这里可能出问题，所以设置一个断点
  7             import pdb; pdb.set_trace()
  8  ->         total += item
  9         return total
 10
 11     numbers = [1, 2, "3", 4, 5] # 列表中混入了一个字符串
 12     result = calculate_sum(numbers)
```

我们想知道 `total` 和 `item` 当前的值。使用 `p` 命令：

```
(Pdb) p total
0
(Pdb) p item
1
```

第一次循环没问题。我们让程序继续执行，进入下一次循环。输入 `c`：

```
(Pdb) c
```

程序会再次在断点处停下。再次检查变量：

```
(Pdb) p total
1
(Pdb) p item
2
```

第二次循环也没问题。再次输入 `c`，进入第三次循环：

```
(Pdb) c
```

再次检查变量：

```
(Pdb) p total
3
(Pdb) p item
'3'
```

啊哈！问题找到了！`total` 是整数 3，但 `item` 是字符串 `'3'`。整数和字符串不能直接用 `+` 相加，这会导致 `TypeError`。

我们已经找到了 bug 的根源。现在输入 `q` 退出调试。

```
(Pdb) q
```

现在你知道了，问题在于 `numbers` 列表中的数据类型不统一。`pdb` 让你像用显微镜一样精确地定位了问题所在。


## pip & conda：命令行下的包管理

除了运行和调试脚本，命令行还有一项超级重要的用途——**管理 Python 包和环境**。

### pip：Python 的官方包管理工具

- **安装第三方库：**

  ```{bash}
  pip install requests
  ```

- **查看已安装的包：**

  ```{bash}
  pip list
  ```

- **升级包：**

  ```{bash}
  pip install --upgrade numpy
  ```

- **卸载包：**

  ```{bash}
  pip uninstall requests
  ```

- **指定安装版本：**

  ```{bash}
  pip install pandas==2.0.3
  ```

- **导出依赖列表：**

  ```{bash}
  pip freeze > requirements.txt
  ```

- **根据依赖列表安装：**

  ```{bash}
  pip install -r requirements.txt
  ```

> pip 是最常用的 Python 包管理工具，适合绝大部分纯 Python 包的安装。


### conda：强大的多语言环境和包管理器

如果你用的是 Anaconda/Miniconda，或者需要管理依赖复杂、含有大量 C/C++ 库的包（如数据科学、机器学习领域常用的 numpy、pandas、scipy、pytorch 等），建议用 conda！

- **创建新环境：**

  ```{bash}
  conda create -n myenv python=3.10
  ```

- **激活环境：**

  ```{bash}
  conda activate myenv
  ```

- **安装包：**

  ```{bash}
  conda install numpy pandas matplotlib
  ```

- **列出所有环境：**

  ```{bash}
  conda env list
  ```

- **导出环境配置：**

  ```{bash}
  conda env export > environment.yml
  ```

- **根据配置文件还原环境：**

  ```{bash}
  conda env create -f environment.yml
  ```

- **删除环境：**

  ```{bash}
  conda remove -n myenv --all
  ```

> conda 不仅能管理 Python 包，还能管理 R、Java、C/C++ 等语言的依赖，适合需要跨语言和复杂依赖的项目。


## 总结

掌握命令行运行、参数传递、调试（pdb）、包管理（pip/conda），是从“会写 Python 代码”到“会用 Python 解决问题”的关键一步。这个过程也许比点击按钮更“麻烦”，但它会赋予你对程序更深刻的理解和更强的控制力。

现在，去尝试修改 `buggy_calculator.py`，修复那个 bug，然后再次用命令行运行它吧！并试着用 `pip` 或 `conda` 安装你需要的包，用命令行一步步成长为真正的开发者！

=== File: ./Lessons/lesson2.qmd ===
---
title: "2: Python入门"
---

## Python 入门完全指南

欢迎来到 Python 的世界！Python 是一种功能强大、用途广泛且极其易于学习的编程语言。无论你是想进入数据科学、人工智能、网站开发还是只想写一些自动化脚本，Python 都是绝佳的起点。

本教程将从零开始，带你一步步掌握 Python 的核心知识。

### 准备工作：安装 Python

在开始编程之前，你需要在你的电脑上安装 Python 环境。

**检查是否已安装：**

打开你的终端（在 Windows 上是 `命令提示符` 或 `PowerShell`，在 macOS 或 Linux 上是 `终端`），然后输入：

```{bash}
python --version
```

或者（对于某些系统）：

```{bash}
python3 --version
```

如果你看到了类似 `Python 3.x.x` 的输出，那么恭喜你，Python 已经安装好了！如果提示命令未找到，或者版本是 2.x.x，我们强烈建议你安装最新的 Python 3 版本。

**下载和安装：**

1.  访问 Python 官网：[https://www.python.org/downloads/](https://www.python.org/downloads/)
2.  网站会自动检测你的操作系统并推荐合适的下载版本。点击 "Download Python" 按钮即可。
3.  **Windows 用户请注意：** 在安装过程中，请务必勾选 **"Add Python to PATH"** 这个选项，这会让后续操作方便很多。
4.  按照安装向导的提示完成安装即可。

**Windows安装教程**

<iframe width="514" height="289" src="https://player.bilibili.com/player.html?bvid=BV1YJ411j7fm&page=1" title="Windows10 下 Python 安装运行" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


**Mac自带Python**

<iframe width="514" height="289" src="https://player.bilibili.com/player.html?bvid=BV1jr4y1y7hc&page=1" title="Python编程环境安装——MacOS版本" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**代码编辑器推荐：**

虽然你可以用记事本写代码，但一个好的代码编辑器会让你的编程体验大大提升。对于初学者，我们推荐：

  * **Visual Studio Code (VS Code):** 免费、功能强大，拥有庞大的插件生态系统。安装 Python 插件后会变得非常好用。
  * **PyCharm Community Edition:** 由 JetBrains 公司开发的专业 Python IDE，社区版免费。


### 你的第一个 Python 程序：`Hello, World!`

这是编程界的传统，让我们用 Python 向世界问好。

1.  打开你的代码编辑器，新建一个文件，并将其命名为 `hello.py`（`.py` 是 Python 文件的标准扩展名）。
2.  在文件中输入以下代码：

```{python}
print("Hello, World!")
```

3.  保存文件。
4.  打开终端，使用 `cd` 命令切换到你保存 `hello.py` 文件的目录。例如，如果文件在桌面上，你可以输入 `cd Desktop`。
5.  运行你的程序！在终端中输入：


```{bash}
python hello.py
```

或者（如果你的系统同时有 Python 2 和 3）：

```{bash}
python3 hello.py
```

你应该会在终端看到输出：

```
Hello, World!
```

恭喜你！你已经成功运行了你的第一个 Python 程序。`print()` 是 Python 的一个内置函数，用于在屏幕上输出信息。


### Python 基础语法

#### 注释

注释是代码中不会被执行的部分，用于解释代码的功能，方便自己和他人阅读。

  * **单行注释：** 使用 `#` 号。`#` 号之后的所有内容都会被忽略。



```{python}
# 这是一行注释，用来解释下面的代码
print("Hello, Python!") # 这也是一行注释
```

* **多行注释：** 使用三个单引号 `'''` 或三个双引号 `"""` 将注释内容包裹起来。


```{python}
'''
这是一个多行注释。
可以写很多行的说明文字。
这部分内容不会被执行。
'''
print("欢迎学习 Python！")
```

#### 变量与数据类型

**变量 (Variable)** 就像一个带标签的盒子，你可以把数据放进去。在 Python 中，你不需要提前声明变量的类型，直接赋值即可。


```{python}
# 变量赋值
message = "你好，世界！" # 字符串 (String)
age = 20                # 整数 (Integer)
price = 99.99           # 浮点数 (Float)
is_learning = True      # 布尔值 (Boolean)
```

**常见数据类型：**

  * **字符串 (String / `str`):** 文本信息，用单引号 `' '` 或双引号 `" "` 包裹。
  * **整数 (Integer / `int`):** 没有小数部分的数字。
  * **浮点数 (Float / `float`):** 带有小数部分的数字。
  * **布尔值 (Boolean / `bool`):** 只有两个值 `True` (真) 和 `False` (假)，注意首字母大写。

你可以使用 `type()` 函数来查看一个变量的数据类型：


```{python}
name = "小明"
age = 18
print(type(name))  # 输出: <class 'str'>
print(type(age))   # 输出: <class 'int'>
```

#### 字符串操作

字符串是 Python 中最常用的数据类型之一。

  * **拼接 (Concatenation):** 使用 `+` 号。



```{python}
first_name = "张"
last_name = "三"
full_name = first_name + last_name
print(full_name) # 输出: 张三
```

* **格式化字符串 (f-string):** 这是推荐的、更现代的字符串格式化方法。在字符串引号前加上 `f`，然后用 `{}` 包裹变量。



```{python}
name = "小红"
age = 19
intro = f"我的名字是 {name}，我今年 {age} 岁了。"
print(intro) # 输出: 我的名字是 小红，我今年 19 岁了。
```

* **常用方法:**



```{python}
text = "  Python is Fun!  "
print(text.lower())      # 全部转为小写: "  python is fun!  "
print(text.upper())      # 全部转为大写: "  PYTHON IS FUN!  "
print(text.strip())      # 去除首尾空格: "Python is Fun!"
print(text.replace("Fun", "Awesome")) # 替换: "  Python is Awesome!  "
```

### 数据结构：列表、元组和字典

当我们需要存储一组数据时，就需要用到更复杂的数据结构。

#### 列表 (List)

列表是一个有序的、可变的集合。你可以添加、删除或修改列表中的元素。用方括号 `[]` 定义。


```{python}
# 创建一个列表
fruits = ["苹果", "香蕉", "橙子"]
print(fruits) # 输出: ['苹果', '香蕉', '橙子']

# 访问元素（索引从 0 开始）
print(fruits[0]) # 输出: 苹果
print(fruits[1]) # 输出: 香蕉

# 修改元素
fruits[1] = "西瓜"
print(fruits) # 输出: ['苹果', '西瓜', '橙子']

# 添加元素
fruits.append("葡萄") # 在末尾添加
print(fruits) # 输出: ['苹果', '西瓜', '橙子', '葡萄']

# 删除元素
fruits.remove("西瓜")
print(fruits) # 输出: ['苹果', '橙子', '葡萄']

# 获取列表长度
print(len(fruits)) # 输出: 3
```

#### 元组 (Tuple)

元组是一个有序的、**不可变**的集合。一旦创建，就不能修改。用圆括号 `()` 定义。


```{python}
# 创建一个元组
point = (10, 20)
print(point) # 输出: (10, 20)

# 访问元素
print(point[0]) # 输出: 10

# 尝试修改元组会报错
# point[0] = 15 # 这行代码会引发 TypeError
```

> **何时使用元组？** 当你希望数据是只读的、不被意外修改时，例如坐标点、配置信息等。

#### 字典 (Dictionary)

字典是一个无序的、键值对 (key-value) 的集合。每个元素都由一个唯一的 **键 (key)** 和对应的 **值 (value)** 组成。用花括号 `{}` 定义。


```{python}
# 创建一个字典
student = {
    "name": "李四",
    "age": 22,
    "city": "北京"
}
print(student) # 输出: {'name': '李四', 'age': 22, 'city': '北京'}

# 访问值 (通过键)
print(student["name"]) # 输出: 李四
print(student["age"])  # 输出: 22

# 添加或修改键值对
student["email"] = "lisi@example.com" # 添加新键值对
student["age"] = 23                   # 修改已有的值
print(student) # 输出: {'name': '李四', 'age': 23, 'city': '北京', 'email': 'lisi@example.com'}

# 删除键值对
del student["city"]
print(student) # 输出: {'name': '李四', 'age': 23, 'email': 'lisi@example.com'}
```

### 流程控制

流程控制让你的代码可以根据不同的条件执行不同的路径，或者重复执行某些任务。

#### 条件判断 (`if`, `elif`, `else`)


```{python}
age = 18

if age < 18:
    print("你是未成年人。")
elif age == 18:
    print("你刚满 18 岁。")
else:
    print("你是成年人。")

# 注意：
# 1. 条件后面要加冒号 :
# 2. 下一级的代码块需要缩进（通常是 4 个空格）
# 3. `elif` 是 "else if" 的缩写，可以有多个。
# 4. `else` 是可选的。
```

#### 循环 (`for` 和 `while`)

##### `for` 循环

`for` 循环用于遍历一个序列（如列表、元组、字符串或范围）。


```{python}
# 遍历列表
fruits = ["苹果", "香蕉", "橙子"]
for fruit in fruits:
    print(f"我喜欢吃 {fruit}")

# 使用 range() 函数进行固定次数的循环
# range(5) 会生成从 0 到 4 的数字序列
for i in range(5):
    print(f"这是第 {i+1} 次循环")
```

##### `while` 循环

`while` 循环在给定条件为 `True` 时会一直执行。


```{python}
count = 0
while count < 5:
    print(f"当前的数字是: {count}")
    count = count + 1 # 必须更新循环变量，否则会造成无限循环！

print("循环结束！")
```

### 函数

函数是一段可重复使用的代码块，用于执行特定的任务。使用函数可以使你的代码更有条理、更易于维护。

#### 定义和调用函数


```{python}
# 定义一个简单的函数
def greet():
    print("你好！欢迎使用本程序。")

# 调用函数
greet()
greet()
```

#### 参数和返回值

函数可以接受输入（**参数**）并产生输出（**返回值**）。


```{python}
# 带参数的函数
def greet_user(name):
    """这是一个文档字符串，用于解释函数的功能。"""
    print(f"你好，{name}！")

greet_user("小王") # 输出: 你好，小王！
greet_user("小李") # 输出: 你好，小李！


# 带参数和返回值的函数
def add(a, b):
    """这个函数计算两个数的和并返回结果。"""
    result = a + b
    return result

sum_result = add(5, 3)
print(f"5 + 3 = {sum_result}") # 输出: 5 + 3 = 8
```


### 模块和库：扩展你的 Python 能力

Python 的强大之处在于其庞大的标准库和第三方库生态系统。

  * **模块 (Module):** 一个 `.py` 文件就是一个模块。你可以使用 `import` 语句来使用其他模块中定义的函数或变量。
  * **库 (Library):** 库是相关模块的集合。

**示例：使用 `random` 模块生成随机数**

`random` 是 Python 的一个标准库模块，无需额外安装。


```{python}
import random # 导入 random 模块

# 生成一个 1 到 10 之间的随机整数
random_number = random.randint(1, 10)
print(f"生成的随机数是: {random_number}")

# 从列表中随机选择一个元素
fruits = ["苹果", "香蕉", "橙子", "西瓜"]
random_fruit = random.choice(fruits)
print(f"今天的水果是: {random_fruit}")
```

**安装第三方库：**

如果想使用非标准库（例如用于数据分析的 `pandas`，或网络请求的 `requests`），你需要使用 `pip`（Python 的包管理器）来安装。

打开终端并输入：

```{bash}
pip install requests
pip install pandas
```

### 面向对象编程（OOP）

Python 支持面向对象编程（OOP, Object-Oriented Programming），这是一种模拟现实世界思考问题的方法。通过**类和对象**，我们可以组织更复杂的代码，构建出具有良好结构和可扩展性的程序。

#### 为什么要用面向对象？

- 让代码**更贴近现实世界**，更容易理解和维护
- **数据和功能绑定**在一起，便于复用
- 可以写出**更大的项目**，比如游戏、网站、软件等

#### 一、类（Class）和对象（Object）

- **类**是一个模板，用来描述一类事物的共同属性和行为。
- **对象**是根据类创建的具体实例。类好比蓝图，对象好比房子。

**举例：用类描述学生，每个学生就是一个对象。**

```{python}
class Student:
    pass  # 空语句，暂时不定义内容

s1 = Student()  # 创建一个学生对象
s2 = Student()  # 再创建一个学生对象
```

#### 二、属性和方法

属性和方法是类的“成员”。  
- **属性**（变量）：对象的状态或特征  
- **方法**（函数）：对象的行为或动作

##### 2.1 初始化方法 `__init__`

- 每次创建对象时，都会自动调用 `__init__`
- 用于给新对象**赋初值**

```{python}
class Student:
    def __init__(self, name, age):
        self.name = name  # 实例属性
        self.age = age

s = Student("小明", 18)
print(s.name) # 输出: 小明
print(s.age)  # 输出: 18
```

**注意：`self` 总是指向当前对象本身。**

##### 2.2 实例方法

- 普通方法都必须有 `self` 参数，表示操作的是哪个对象

```{python}
class Student:
    def __init__(self, name):
        self.name = name

    def say_hello(self):
        print(f"大家好，我是{self.name}")

s = Student("小明")
s.say_hello()  # 输出: 大家好，我是小明
```

##### 2.3 类属性和实例属性

- **实例属性**：每个对象自己的数据，用 `self.xxx` 定义
- **类属性**：所有对象共享的数据，直接在类内定义

```{python}
class Dog:
    species = "犬科"  # 类属性

    def __init__(self, name):
        self.name = name  # 实例属性

dog1 = Dog("旺财")
dog2 = Dog("小黑")
print(dog1.species)  # 输出: 犬科
print(dog2.species)  # 输出: 犬科
```

#### 三、继承与多态

##### 3.1 继承

- **继承**允许一个类（子类）获得另一个类（父类）的所有成员。
- 方便**代码复用**，比如所有动物都会吃东西。

```{python}
class Animal:
    def eat(self):
        print("吃东西")

class Dog(Animal):
    def bark(self):
        print("汪汪！")

d = Dog()
d.eat()  # 输出: 吃东西
d.bark() # 输出: 汪汪！
```

##### 3.2 方法重写（Override）

- 子类可以重写父类的方法，实现不同的行为。

```{python}
class Animal:
    def speak(self):
        print("动物叫")

class Cat(Animal):
    def speak(self):
        print("喵喵！")

c = Cat()
c.speak()  # 输出: 喵喵！
```

##### 3.3 多态

- **多态**：同一个方法名，不同子类有不同实现，调用时自动选择对应的方法。

```{python}
class Animal:
    def speak(self):
        print("动物叫")

class Dog(Animal):
    def speak(self):
        self.bark()

    def bark(self):
        print("汪汪！")

class Cat(Animal):
    def speak(self):
        print("喵喵！")

animals = [Dog(), Cat()]
for animal in animals:
    animal.speak()
# 输出:
# 汪汪！
# 喵喵！
```

#### 四、封装

- **封装**就是隐藏对象的内部细节，只暴露必要的接口。
- Python 通过**下划线**约定变量/方法的访问权限。

| 变量名          | 说明                  |
| --------------- | --------------------- |
| name            | 公有，外部可访问      |
| _name           | 受保护，不建议外部访问|
| __name          | 私有，类外无法直接访问 |

```{python}
class Person:
    def __init__(self, name):
        self.name = name      # 公有属性
        self._age = 18        # 受保护属性
        self.__salary = 5000  # 私有属性

    def get_salary(self):
        return self.__salary

p = Person("小李")
print(p.name)       # 正常访问
print(p._age)       # 可以访问，但不建议
# print(p.__salary)  # 会报错
print(p.get_salary()) # 推荐通过方法访问私有属性
```

#### 五、特殊方法和魔法方法

Python 有很多以 **双下划线**开头和结尾的“魔法方法”，让对象参与各种内置操作。

- `__init__`：初始化对象
- `__str__`：定义 print(对象) 时的输出
- `__len__`：定义 len(对象) 的行为

```{python}
class Book:
    def __init__(self, title):
        self.title = title

    def __str__(self):
        return f"《{self.title}》"

b = Book("Python入门")
print(b)  # 输出: 《Python入门》
```

#### 六、实战：自定义一个简单的学生管理系统（面向对象版）

```{python}
class Student:
    def __init__(self, name, score):
        self.name = name
        self.score = score

    def show(self):
        print(f"{self.name} 的成绩是 {self.score}")

# 管理多个学生
students = [
    Student("小明", 90),
    Student("小红", 95),
    Student("小刚", 88)
]

# 遍历所有学生
for stu in students:
    stu.show()
```

#### 七、面向对象思维举例

- **对象是什么？** 现实世界的事物，比如“小明”、一辆汽车、一本书
- **类是什么？** 模型或模板，比如“学生类”、“汽车类”
- **属性是什么？** 事物的特征，比如“姓名”、“颜色”
- **方法是什么？** 事物的行为，比如“学习”、“启动”

### 清华大学计算机系相关资料

**课程回放：**

<iframe width="514" height="289" src="https://player.bilibili.com/player.html?bvid=BV1Ny411B7Ex&page=1" title="【清华大学计算机系科协算协联合暑培 2024】Python - 基础 Track" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**讲义：**

[讲义](https://summer24.net9.org/basic/python/handout/)

### 下一步学习建议

恭喜你完成了 Python 的基础入门！你现在已经掌握了编写简单 Python 程序所需的核心知识。

接下来，你可以根据你的兴趣选择深入学习的方向：

  * **网站开发:** 学习 Django 或 Flask 框架。
  * **数据科学/机器学习:** 学习 NumPy, Pandas, Matplotlib, Scikit-learn 等库。
  * **自动化脚本:** 深入学习文件操作、网络请求等。
  * **游戏开发:** 探索 Pygame 库。

**最重要的一点是：多写代码，多练习！** 尝试解决一些小程序问题，比如：

  * 写一个简单的计算器。
  * 写一个猜数字游戏。
  * 写一个程序来整理你电脑上的文件。

如果想系统学习Python，可阅读[《Python编程从入门到实践》](https://file.notion.so/f/f/18d1828d-75b8-416c-9256-652a1a81baa2/4e393a7a-06a6-420a-8f80-7a99811878e4/Untitled.pdf?table=block&id=07ded07f-c832-40a8-a89f-af36d1a589fe&spaceId=18d1828d-75b8-416c-9256-652a1a81baa2&expirationTimestamp=1751932800000&signature=ruXk2_vOTCZvnHktme70fqXNtkzAi02iQD-b7ejeODY&downloadName=Python+编程从入门到实践.pdf).

=== File: ./Lessons/lesson1.qmd ===
---
title: "1: 计算机科学基础与程序语言"
---

## 计算机程序设计语言

对于很多初学者而言，“编程”、“代码”这些词汇可能听起来像是另一个世界的语言，充满了神秘感和距离感。大家可能会觉得，我们又不当程序员，为什么要了解这些？

其实，在今天这个高度数字化的时代，了解一些计算机科学的基本逻辑，能帮助我们更好地理解我们所生活的世界，提升解决问题的能力，甚至在未来的职业生涯中带来意想不到的优势。

这篇介绍的目的，不是让大家成为专业的程序员，而是揭开编程的神秘面纱，让大家明白它到底是什么，以及它如何驱动我们每天使用的软件和应用。

### 什么是程序设计语言？

想象一下，你想让一位只会说英语的朋友帮你去图书馆借一本书。你必须用英语，按照清晰的逻辑（比如：先走到图书馆 -\> 然后找到特定区域 -\> 根据书名找到书架 -\> 最后办理借阅手续），一步步地告诉他该怎么做。

**程序设计语言（Programming Language）**，就是我们与计算机沟通的“语言”。

计算机本质上只懂由 0 和 1 组成的“机器语言”，这对人类来说太复杂了。于是，科学家们发明了各种程序设计语言，让我们能用更接近人类自然语言的方式，给计算机下达一系列清晰、无歧义的指令，告诉它“做什么”以及“怎么做”。

当我们写好的指令（我们称之为**源代码**）被计算机执行后，就能实现各种各样的功能——比如你正在使用的微信、你浏览的淘宝、你观看的视频，背后都是由千百万行代码构成的复杂程序。

### C 和 C++：严谨高效的工业标准

在众多编程语言中，C 和 C++ 是两座无法绕过的高峰。

#### C 语言：现代语言的基石

C 语言诞生于上世纪 70 年代，是一位真正的“元老”。它的设计哲学是**简洁、高效、贴近底层硬件**。你可以把它想象成手动挡的汽车，虽然操作起来比自动挡复杂，需要你精确控制离合和油门，但它能让你最大程度地掌控汽车的性能，开得飞快。

  * **特点**：
      * **执行效率极高**：代码被转换成机器指令后，几乎没有多余的动作，运行速度非常快。
      * **控制力强**：可以直接操作内存（计算机存储数据的地方），像一个精密的外科医生，能精确控制计算机的每一个细节。
      * **过程导向**：编程的思维方式是“一步一步做什么”，像一个流程图。

#### C++ 语言：C 语言的超集与进化

随着软件越来越复杂，人们发现单纯用 C 语言来构建大型项目（比如一个操作系统或一个大型游戏）太困难了。于是，在 C 语言的基础上，C++ 诞生了。

C++ 完全兼容 C 语言，并在此之上增加了一个强大的新特性——**面向对象编程（Object-Oriented Programming, OOP）**。

这是什么意思呢？想象一下，你要盖一栋房子。

  * **C 语言的方式（过程导向）**：你会思考步骤，“先打地基 -\> 再砌墙 -\> 然后封顶 -\> 最后装修……”
  * **C++ 的方式（面向对象）**：你会先思考这栋房子由哪些“对象”组成，比如“柱子”、“墙壁”、“窗户”、“门”。你会先设计好每个“对象”应该长什么样、有什么功能（比如窗户可以打开和关闭），然后再把这些预制好的对象“组装”成一栋房子。

这种“面向对象”的思想，让 C++ 更适合开发规模宏大、逻辑复杂的软件系统。

  * **特点**：
      * 继承了 C 语言的高效和底层控制力。
      * 通过“面向对象”思想，让代码的组织、复用和维护变得更加容易。
      * 功能极其强大，但也因此变得非常复杂，学习曲线陡峭。

#### 工作原理：从代码到程序的“翻译”之旅

我们用 C/C++ 写下的代码（例如一个 `.cpp` 文件），计算机是看不懂的。它需要经历一个“翻译”和“组装”的过程，才能变成一个可以执行的程序（比如 Windows 上的 `.exe` 文件）。这个过程主要分为两步：

1.  **编译（Compilation）**

      * **编译器（Compiler）** 就像一个语言翻译官。它会检查你的 C++ 代码有没有语法错误（比如拼写错了、标点不对等）。
      * 如果没问题，编译器会把你的代码“翻译”成一种更低级的语言——**汇编语言（Assembly Language）**，并最终生成**目标文件（Object File）**，里面是接近机器码的指令。

2.  **链接（Linking）**

      * 一个大型软件可能由成百上千个代码文件组成，每个文件都会被编译成一个目标文件。同时，我们的代码可能还会用到一些系统自带的功能库（比如处理屏幕显示、文件读写的功能）。
      * **链接器（Linker）** 的工作就像一个项目经理，它会把所有这些零散的目标文件和你用到的系统功能库“链接”在一起，组装成一个完整的、可执行的程序。

#### 汇编语言与 CPU：指令的最终执行者

编译过程的中间产物——**汇编语言**，是机器语言的助记符形式。它和机器指令是一一对应的，但比纯粹的 0101 串更容易让人理解。例如，一条让 CPU 做加法运算的机器码可能是 `10000011 11000011`，对应的汇编指令可能是 `ADD AX, BX`。

最终，程序运行时，这些指令会被加载到内存中。**中央处理器（CPU）**，作为计算机的大脑，会一条一条地读取这些指令，并在其内部的运算单元和寄存器中执行它们（比如进行加减乘除、数据移动、逻辑判断等），从而完成程序指定的功能。

**总结一下 C/C++ 的世界：**

> **你写的 C++ 代码 -\> [编译器] -\> 汇编语言/目标文件 -\> [链接器] -\> 可执行程序 -\> [CPU] -\> 完成功能**


### **Python：为“人类”设计的语言**

讲完了复杂的 C/C++，我们再来看 Python，你会瞬间感觉轻松很多。

Python 的设计哲学是**优雅、明确、简单**。它不像 C++ 那样需要你手动管理内存，也不需要复杂的编译、链接过程。它是一种**解释型语言 (Interpreted Language)**。

- **解释型 vs. 编译型**：如果说 C++ 的编译器是把整本书一次性翻译完再出版，那么 Python 的**解释器 (Interpreter)** 就是一个同声传译。你说一句（写一行代码），它就翻译一句并立刻执行一句。这使得开发和调试过程变得非常快速和直观。

#### **澄清常见的困惑：Python, PyCharm, Pip, Conda 都是什么？**


这是初学者最容易混淆的地方。很多人以为下载一个 PyCharm 就是装了 Python。让我们用一个清晰的类比来区分它们：

- **Python (语言本身)**
  - **是什么**：它就是“英语”这门语言本身，包含所有的词汇、语法规则。它是核心，是一切的基础。
  - **如何体现**：你从 Python 官网下载的那个安装包，就是**Python 解释器**，是让你的电脑能够听懂 Python 这门语言的“翻译官”。
- **PyCharm (IDE - 集成开发环境)**
  - **是什么**：它是一个高级的“Word 文档处理器”，专门用来写“英语作文”（Python 代码）。它提供了语法高亮（帮你区分动词名词）、自动补全（输入 `pri` 就提示 `print`）、错误检查、一键运行等便利功能。
  - **类比**：你完全可以在记事本（最简单的文本编辑器）里写 Python 代码，但用 PyCharm 会让你的写作体验和效率大大提升。**PyCharm 是写代码的工具，而不是语言本身。**
- **Pip (包管理器)**
  - **是什么**：它是 Python 官方自带的“应用商店”。Python 的强大之处在于有海量的第三方**库 (Library/Package)**，这些库封装了各种强大的功能（比如数据分析、人工智能）。
  - **如何工作**：当你想用一个叫 `pandas` 的库来分析数据时，你不需要自己去网站下载，只需在命令行里输入 `pip install pandas`，pip 就会自动帮你找到、下载并安装好这个库。
- **Conda (环境与包管理器)**
  - **是什么**：Conda 是一个更强大的“项目总管家”。它不仅有 Pip 的功能（也能安装库），还有一个核心功能——**环境管理**。
  - **为什么需要它**：想象一下，你的项目A需要1.0版本的 `pandas`，而项目B需要2.0版本的 `pandas`，如果都装在系统里就会冲突。Conda 可以为你创建两个完全隔离的“房间”（环境），A房间里装1.0版，B房间里装2.0版，互不干扰。这对于管理复杂项目至关重要。

**总结一下：**

> 你决定用 **Python (语言)** 来完成一个任务。于是你打开 **PyCharm (写作工具)** 开始写代码。写到一半，你发现需要一个特殊工具，就用 **Pip** 或 **Conda (应用商店)** 下载了一个叫 `pandas` 的**库 (功能包)**来帮你处理数据。


### 应用场景：我们用它们来做什么？

| 特性比较 | C / C++ | Python |
| :--- | :--- | :--- |
| **性能** | 极致，像风一样快 | 足够用，但比 C++ 慢（因为是解释执行） |
| **开发效率** | 较低，代码长，编译慢 | 极高，代码简洁，即写即运行 |
| **学习难度** | 陡峭，需要理解底层 | 平缓，语法更接近自然语言 |
| **核心比喻** | 手动挡赛车 | 自动挡智能汽车 |

#### C/C++ 的主要应用场景（追求极致性能和底层控制）

  * **操作系统**：Windows, macOS, Linux, Android, iOS 的核心部分都是用 C/C++ 写的。
  * **游戏开发**：大型 3D 游戏引擎，如 Unreal Engine（虚幻引擎）、Unity（部分核心），需要榨干硬件的每一分性能来实现逼真的画面和物理效果。
  * **高性能计算/嵌入式系统**：驱动你家智能微波炉、洗衣机的微型芯片，或者用于科学模拟和金融交易的服务器，都要求极致的效率和低延迟。
  * **浏览器/数据库**：Google Chrome 浏览器、MySQL 数据库等，这些需要处理海量数据和高并发请求的底层软件。

**例子**：当你玩一款次世代 3D 大作时，那流畅的画面、复杂的物理碰撞，背后就是 C++ 强大的性能在支撑。

#### Python 的主要应用场景（追求开发效率和生态系统）

  * **数据科学与人工智能（AI）**：这是 Python **最**闪耀的领域。无论是数据清洗、分析、可视化，还是构建复杂的机器学习、深度学习模型（例如人脸识别、推荐系统），Python 凭借 `Pandas`, `NumPy`, `TensorFlow`, `PyTorch` 等强大的库，成为了事实上的标准。
  * **Web 开发**：许多网站的后端服务是用 Python 搭建的，例如 Instagram, YouTube, 知乎。使用 `Django` 或 `Flask` 这样的框架，可以快速开发出功能完善的网站。
  * **自动化办公/网络爬虫**：写个小程序自动处理 Excel 表格、批量下载文件、抓取网页上的信息，Python 是完成这些任务的绝佳工具，能把文科生从繁琐的重复劳动中解放出来。
  * **科学计算与教育**：在科研和高校教育中，Python 因其简单易学和丰富的科学计算库而备受青睐。

**例子**：今日头条、抖音为你推荐的短视频，背后就是基于 Python 的推荐算法在不知疲倦地学习你的偏好。你用 Python 写几十行代码，就能自动抓取所有同学的论文题目并整理成一个 Excel 表。

### Python 的魅力：用几行代码体验强大功能



Python 的强大不在于语言本身，而在于其背后无穷无尽的“功能库”。你不需要重新发明轮子，只需要学会如何使用这些强大的轮子。



#### **示例1：用 5 行代码完成数据分析**



假设你有一个 Excel 文件 `sales.csv`，记录了不同产品的销售额。你想快速计算每种产品的平均销售额。


```{python}
#| eval: false
# 导入强大的数据分析库 pandas
import pandas as pd

# 1. 读取 Excel/CSV 文件，只需要一行代码
data = pd.read_csv('sales.csv')

# 2. 按照 '产品名称' 分组，并计算 '销售额' 的平均值
average_sales = data.groupby('产品名称')['销售额'].mean()

# 3. 打印结果
print(average_sales)
```

解释：

你不需要关心如何打开文件、如何一行行读取数据、如何创建字典来存储求和、如何计算平均值。pandas 这个库已经把所有复杂操作都封装好了。你只需要调用 read_csv 和 groupby().mean() 这两个简单的指令，就能完成传统方法可能需要几十上百行代码才能完成的工作。



#### **示例2：用 3 行代码调用顶尖的 AI 大语言模型**



想不想体验一下 AI 的威力？比如，让 AI 判断一句话是积极的还是消极的？

```{python}
#| eval: false
# 导入 Hugging Face 的 transformers 库，这是AI领域的利器
from transformers import pipeline

# 1. 创建一个情感分析的流水线，模型会自动下载
sentiment_pipeline = pipeline("sentiment-analysis")

# 2. 分析一句话的情感
result = sentiment_pipeline("I love studying programming, it's so empowering!")
print(result)
# 输出可能类似: [{'label': 'POSITIVE', 'score': 0.999...}]
```

解释：

你不需要理解复杂的神经网络和 Transformer 模型。transformers 库让你只需要三行代码，就能调用一个训练好的、非常强大的 AI 模型来完成任务。这在几年前是不可想象的。对于文科同学，可以用它来做文本分析、舆情监控；对于商科同学，可以用它来分析用户评论。

### 为什么建议大家掌握 Python？

对于非计算机专业的同学来说，投入大量时间去学习 C++ 这样复杂的语言，性价比可能不高。而 **Python**，则是一项“**低投入、高回报**”的技能。

1.  **易于上手**：语法简单清晰，更符合人类的思维习惯，让你能快速体验到编程的乐趣和成就感。
2.  **应用广泛，解决实际问题**：你可以在你的专业领域找到它的用武之地。
      * **商科同学**：可以用 Python 做量化交易、分析用户消费数据、预测销售趋势。
      * **文科同学**：可以做社会调查数据分析、进行数字人文研究（如文本分析）、或者自动化处理文献资料。
3.  **提升数据素养**：在未来，无论从事什么行业，处理和分析数据的能力都将是核心竞争力。Python 是开启这扇大门的钥匙。
4.  **思维的体操**：学习编程能训练你的逻辑思维、抽象思维和解决问题的能力。这种思维方式的提升，会让你在任何领域都受益匪นาน。

**总结**：学习 C++ 是在学习如何制造一辆高性能的汽车，而学习 Python 是在学习如何驾驶汽车去到任何你想去的地方，并利用汽车的便利性来完成你的工作。对于大多数人来说，学会“驾驶”远比学会“制造”更加实用和迫切。

希望这份介绍能帮助大家对计算机编程有一个初步的、清晰的认识。编程不是一门高不可攀的技术，它只是一种强大的工具，等待着你去学习和使用它，来创造属于你的价值。
